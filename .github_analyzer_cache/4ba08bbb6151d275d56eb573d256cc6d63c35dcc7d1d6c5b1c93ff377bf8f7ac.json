{"metadata":{"name":"github_analyzer","full_name":"cruxhan/github_analyzer","description":"Github-Analyzer for Ai Model Context","is_private":false,"default_branch":"main","language":"Dart","languages":["JSON","Markdown","YAML","Dart"],"stars":1,"forks":0,"file_count":45,"commit_sha":"dbabc178a3a793eee5a683393260196560be02b3","directory_tree":"â”œâ”€â”€ .github_analyzer_cache\nâ”‚   â””â”€â”€ 738dd5299fb793950df50c862fb9dba0912b980b89670d36351de6e93ca16d32.json\nâ”œâ”€â”€ .gitignore\nâ”œâ”€â”€ .metadata\nâ”œâ”€â”€ CHANGELOG.md\nâ”œâ”€â”€ LICENSE\nâ”œâ”€â”€ README.md\nâ”œâ”€â”€ analysis_options.yaml\nâ”œâ”€â”€ lib\nâ”‚   â”œâ”€â”€ github_analyzer.dart\nâ”‚   â””â”€â”€ src\nâ”‚       â”œâ”€â”€ common\nâ”‚       â”‚   â”œâ”€â”€ config.dart\nâ”‚       â”‚   â”œâ”€â”€ constants.dart\nâ”‚       â”‚   â”œâ”€â”€ env_loader.dart\nâ”‚       â”‚   â”œâ”€â”€ errors\nâ”‚       â”‚   â”‚   â””â”€â”€ analyzer_exception.dart\nâ”‚       â”‚   â”œâ”€â”€ language_info.dart\nâ”‚       â”‚   â”œâ”€â”€ logger.dart\nâ”‚       â”‚   â””â”€â”€ utils\nâ”‚       â”‚       â”œâ”€â”€ context_generator.dart\nâ”‚       â”‚       â”œâ”€â”€ directory_tree_generator.dart\nâ”‚       â”‚       â”œâ”€â”€ file_utils.dart\nâ”‚       â”‚       â”œâ”€â”€ github_utils.dart\nâ”‚       â”‚       â”œâ”€â”€ markdown_generator.dart\nâ”‚       â”‚       â”œâ”€â”€ metadata_generator.dart\nâ”‚       â”‚       â””â”€â”€ pattern_matcher.dart\nâ”‚       â”œâ”€â”€ core\nâ”‚       â”‚   â”œâ”€â”€ cache_service.dart\nâ”‚       â”‚   â”œâ”€â”€ incremental_analyzer.dart\nâ”‚       â”‚   â”œâ”€â”€ local_analyzer_service.dart\nâ”‚       â”‚   â”œâ”€â”€ remote_analyzer_service.dart\nâ”‚       â”‚   â””â”€â”€ repository_analyzer.dart\nâ”‚       â”œâ”€â”€ data\nâ”‚       â”‚   â””â”€â”€ providers\nâ”‚       â”‚       â”œâ”€â”€ github_api_provider.dart\nâ”‚       â”‚       â””â”€â”€ zip_downloader.dart\nâ”‚       â”œâ”€â”€ github_analyzer.dart\nâ”‚       â”œâ”€â”€ infrastructure\nâ”‚       â”‚   â”œâ”€â”€ file_system\nâ”‚       â”‚   â”‚   â”œâ”€â”€ file_system.dart\nâ”‚       â”‚   â”‚   â”œâ”€â”€ file_system_interface.dart\nâ”‚       â”‚   â”‚   â”œâ”€â”€ file_system_io.dart\nâ”‚       â”‚   â”‚   â”œâ”€â”€ file_system_stub.dart\nâ”‚       â”‚   â”‚   â””â”€â”€ file_system_web.dart\nâ”‚       â”‚   â”œâ”€â”€ http_client_manager.dart\nâ”‚       â”‚   â”œâ”€â”€ interfaces\nâ”‚       â”‚   â”‚   â”œâ”€â”€ i_github_api_provider.dart\nâ”‚       â”‚   â”‚   â””â”€â”€ i_http_client_manager.dart\nâ”‚       â”‚   â””â”€â”€ isolate_pool.dart\nâ”‚       â””â”€â”€ models\nâ”‚           â”œâ”€â”€ analysis_error.dart\nâ”‚           â”œâ”€â”€ analysis_progress.dart\nâ”‚           â”œâ”€â”€ analysis_result.dart\nâ”‚           â”œâ”€â”€ analysis_statistics.dart\nâ”‚           â”œâ”€â”€ repository_metadata.dart\nâ”‚           â””â”€â”€ source_file.dart\nâ””â”€â”€ pubspec.yaml\n"},"files":[{"path":".github_analyzer_cache/738dd5299fb793950df50c862fb9dba0912b980b89670d36351de6e93ca16d32.json","content":"{\"metadata\":{\"name\":\"github_analyzer\",\"full_name\":\"cruxhan/github_analyzer\",\"description\":\"Github-Analyzer for Ai Model Context\",\"is_private\":false,\"default_branch\":\"main\",\"language\":\"Dart\",\"languages\":[\"Markdown\",\"YAML\",\"Dart\"],\"stars\":1,\"forks\":0,\"file_count\":43,\"commit_sha\":\"852277cd6d7516b810684befbe26b9ee769c3e2f\",\"directory_tree\":\"â”œâ”€â”€ .gitignore\\nâ”œâ”€â”€ .metadata\\nâ”œâ”€â”€ CHANGELOG.md\\nâ”œâ”€â”€ LICENSE\\nâ”œâ”€â”€ README.md\\nâ”œâ”€â”€ analysis_options.yaml\\nâ”œâ”€â”€ lib\\nâ”‚   â”œâ”€â”€ github_analyzer.dart\\nâ”‚   â””â”€â”€ src\\nâ”‚       â”œâ”€â”€ common\\nâ”‚       â”‚   â”œâ”€â”€ config.dart\\nâ”‚       â”‚   â”œâ”€â”€ constants.dart\\nâ”‚       â”‚   â”œâ”€â”€ errors\\nâ”‚       â”‚   â”‚   â””â”€â”€ analyzer_exception.dart\\nâ”‚       â”‚   â”œâ”€â”€ language_info.dart\\nâ”‚       â”‚   â”œâ”€â”€ logger.dart\\nâ”‚       â”‚   â””â”€â”€ utils\\nâ”‚       â”‚       â”œâ”€â”€ context_generator.dart\\nâ”‚       â”‚       â”œâ”€â”€ directory_tree_generator.dart\\nâ”‚       â”‚       â”œâ”€â”€ file_utils.dart\\nâ”‚       â”‚       â”œâ”€â”€ github_utils.dart\\nâ”‚       â”‚       â”œâ”€â”€ markdown_generator.dart\\nâ”‚       â”‚       â”œâ”€â”€ metadata_generator.dart\\nâ”‚       â”‚       â””â”€â”€ pattern_matcher.dart\\nâ”‚       â”œâ”€â”€ core\\nâ”‚       â”‚   â”œâ”€â”€ cache_service.dart\\nâ”‚       â”‚   â”œâ”€â”€ incremental_analyzer.dart\\nâ”‚       â”‚   â”œâ”€â”€ local_analyzer_service.dart\\nâ”‚       â”‚   â”œâ”€â”€ remote_analyzer_service.dart\\nâ”‚       â”‚   â””â”€â”€ repository_analyzer.dart\\nâ”‚       â”œâ”€â”€ data\\nâ”‚       â”‚   â””â”€â”€ providers\\nâ”‚       â”‚       â”œâ”€â”€ github_api_provider.dart\\nâ”‚       â”‚       â””â”€â”€ zip_downloader.dart\\nâ”‚       â”œâ”€â”€ github_analyzer.dart\\nâ”‚       â”œâ”€â”€ infrastructure\\nâ”‚       â”‚   â”œâ”€â”€ file_system\\nâ”‚       â”‚   â”‚   â”œâ”€â”€ file_system.dart\\nâ”‚       â”‚   â”‚   â”œâ”€â”€ file_system_interface.dart\\nâ”‚       â”‚   â”‚   â”œâ”€â”€ file_system_io.dart\\nâ”‚       â”‚   â”‚   â”œâ”€â”€ file_system_stub.dart\\nâ”‚       â”‚   â”‚   â””â”€â”€ file_system_web.dart\\nâ”‚       â”‚   â”œâ”€â”€ http_client_manager.dart\\nâ”‚       â”‚   â”œâ”€â”€ interfaces\\nâ”‚       â”‚   â”‚   â”œâ”€â”€ i_github_api_provider.dart\\nâ”‚       â”‚   â”‚   â””â”€â”€ i_http_client_manager.dart\\nâ”‚       â”‚   â””â”€â”€ isolate_pool.dart\\nâ”‚       â””â”€â”€ models\\nâ”‚           â”œâ”€â”€ analysis_error.dart\\nâ”‚           â”œâ”€â”€ analysis_progress.dart\\nâ”‚           â”œâ”€â”€ analysis_result.dart\\nâ”‚           â”œâ”€â”€ analysis_statistics.dart\\nâ”‚           â”œâ”€â”€ repository_metadata.dart\\nâ”‚           â””â”€â”€ source_file.dart\\nâ””â”€â”€ pubspec.yaml\\n\"},\"files\":[{\"path\":\".gitignore\",\"content\":\"# Dart & Flutter íŒ¨í‚¤ì§€ ê¸°ë³¸ .gitignore\\n\\n# ë¹Œë“œ/ìºì‹œ íŒŒì¼\\nbuild/\\n.dart_tool/\\n.packages\\n.pub/\\n*.sqlite3\\n\\n# IDE/ì—ë””í„° íŒŒì¼\\n.idea/\\n.vscode/\\n*.swp\\n\\n# OS ë° ê¸°íƒ€ ë¶ˆí•„ìš” íŒŒì¼\\n.DS_Store\\nThumbs.db\\n\\n# í…ŒìŠ¤íŒ…/coverage\\ncoverage/\\ntest-results/\\n\\n# í™˜ê²½ íŒŒì¼ ë° ì‹¤ì œ í† í°\\n.env\\n*.log\\n\\n# generated files\\n*.generated.*\\n*.g.dart\\n*.iml\\n\\n# flutter/dart package ë°°í¬ ê´€ë ¨\\npubspec.lock\\n\\n# ë¶„ì„/ì •ì  ê²€ì‚¬ ê²°ê³¼\\n.mypy_cache/\\n.pytest_cache/\\n.eggs/\\n\\n# Windows\\n*.exe\\n*.dll\\n\\n# Linux/Mac\\n*.so\\n\\n# ê¸°íƒ€ ì„ì‹œ íŒŒì¼\\n*.tmp\\n\\n# ì¶”ê°€\\n*.env\\n*.cfs\\noutput/\\nexample/\\ndemo.dart\",\"size\":590,\"language\":null,\"is_binary\":false,\"line_count\":55,\"is_source_code\":false,\"is_configuration\":true,\"is_documentation\":false,\"timestamp\":\"2025-10-15T06:50:04.312132\"},{\"path\":\".metadata\",\"content\":\"# This file tracks properties of this Flutter project.\\n# Used by Flutter tool to assess capabilities and perform upgrades etc.\\n#\\n# This file should be version controlled and should not be manually edited.\\n\\nversion:\\n  revision: \\\"ac4e799d237041cf905519190471f657b657155a\\\"\\n  channel: \\\"stable\\\"\\n\\nproject_type: package\\n\",\"size\":313,\"language\":null,\"is_binary\":false,\"line_count\":11,\"is_source_code\":false,\"is_configuration\":false,\"is_documentation\":false,\"timestamp\":\"2025-10-15T06:50:04.315420\"},{\"path\":\"CHANGELOG.md\",\"content\":\"## 0.1.0\\n\\n### Breaking Changes\\n- **Output Format Changed**: Switched from CFS format to Markdown format for context generation\\n  - `ContextGenerator.generate()` now creates `.md` files instead of `.context.md`\\n  - Output is 90% more token-efficient for LLM consumption\\n  - Markdown format is human-readable and supported by all major AI platforms\\n\\n### Performance Improvements\\n- **Memory Efficiency**: Added stream-based markdown generation via `MarkdownGenerator.generateToFile()`\\n  - Reduces memory usage by 50% for large repositories\\n  - Can handle repositories of any size without memory issues\\n- **Smart Filtering**: New configuration options for intelligent file filtering\\n  - `excludeGeneratedFiles`: Automatically excludes *.g.dart, *.freezed.dart, etc.\\n  - `maxTotalFiles`: Limit total files analyzed\\n  - `prioritizeImportantFiles`: Focus on main code (lib/, main.dart)\\n- **Optimized Output**: Inline metadata formatting reduces token usage by additional 10%\\n\\n### New Features\\n- **MarkdownConfig**: Fine-grained control over markdown generation\\n  - `MarkdownConfig.standard`: Full output (default)\\n  - `MarkdownConfig.compact`: Limited files and content for smaller output\\n  - Custom limits: `maxFiles`, `maxContentSize`, `minPriority`\\n- **Convenience Functions**: Simpler API for common use cases\\n  - `analyzeAndGenerate()`: One-step analysis and markdown generation\\n  - `analyzeQuick()`: Fast analysis with optimized settings (no cache, limited files)\\n  - `analyzeForLLM()`: LLM-optimized analysis (excludes tests, limits files)\\n- **Smart File Naming**: Automatic output filename generation from repository name\\n  - `ContextGenerator.generate(result)` auto-generates `{repo_name}_analysis.md`\\n  - Manual naming still supported: `outputPath: './custom.md'`\\n- **Preset Configurations**: Ready-to-use configuration presets\\n  - `GithubAnalyzerConfig.quick()`: Fast analysis\\n  - `GithubAnalyzerConfig.forLLM()`: Optimized for LLM context\\n\\n### API Enhancements\\n- `ContextGenerator.generate()` now returns the output file path\\n- Automatic `.md` extension handling\\n- `outputDir` parameter for specifying output directory with auto-naming\\n- Improved error messages and validation\\n\\n### Removed\\n- Removed `cfs_writer.dart` (replaced by `markdown_generator.dart`)\\n- Removed CFS format support\\n\\n### Migration Guide\\n\\n#### Before (v0.0.4)\\n```dart\\n// Analysis\\nfinal result = await analyze('https://github.com/user/repo', \\n  config: GithubAnalyzerConfig(\\n    excludePatterns: [...],\\n  ),\\n);\\n\\n// Generate output\\nawait ContextGenerator.generate(result, './output.context.md');\\n```\\n\\n#### After (v0.1.0)\\n```dart\\n// Simplest way - one line\\nfinal path = await analyzeAndGenerate('https://github.com/user/repo');\\n\\n// Or with more control\\nfinal result = await analyzeQuick('https://github.com/user/repo');\\nfinal path = await ContextGenerator.generate(result);  // Auto-named\\n\\n// Or LLM-optimized\\nfinal path = await analyzeForLLM(\\n  'https://github.com/user/repo',\\n  maxFiles: 100,\\n);\\n\\n// Manual configuration\\nfinal result = await analyze('https://github.com/user/repo',\\n  config: GithubAnalyzerConfig(\\n    excludeGeneratedFiles: true,  // New option\\n    maxTotalFiles: 200,  // New option\\n    excludePatterns: [...],\\n  ),\\n);\\n\\nawait ContextGenerator.generate(\\n  result,\\n  config: MarkdownConfig.compact,  // New option\\n);\\n```\\n\\n### Performance Benchmarks\\n- **Token Reduction**: 90-95% fewer tokens compared to JSON (Phase 1)\\n- **Memory Usage**: 50% reduction for large repos (stream-based writing)\\n- **Analysis Speed**: Same or faster with smart filtering\\n- **Output Size**: 30% smaller with compact config and smart filtering\\n\\n## 0.0.4\\n\\n### Fix\\nAddressed issues from v0.0.3 and introduced new features.\\n\\n## 0.0.3\\n\\n### Fix\\nAddressed issues from v0.0.2 and introduced new features.\\n\\n## 0.0.2\\n\\n### Major Refactoring for Usability and Maintainability\\n\\n- **Added Top-Level `analyze` Function**: Drastically simplified the API. Users can now analyze repositories with a single function call, without needing to manually set up dependencies.\\n- **Implemented Dependency Injection**: Refactored the `GithubAnalyzer` class to accept dependencies via its constructor, improving modularity and testability.\\n- **Integrated Standard Logging Package**: Replaced the custom logger with the standard Dart logging package for more robust and flexible logging.\\n- **Improved Error Handling**: Enhanced `AnalyzerException` to include the original exception and stack trace, providing more context for debugging.\\n\\n## 0.0.1\\n\\nInitial release of the package. Provides core functionality for analyzing remote and local repositories.\\n\",\"size\":4592,\"language\":\"Markdown\",\"is_binary\":false,\"line_count\":118,\"is_source_code\":true,\"is_configuration\":false,\"is_documentation\":true,\"timestamp\":\"2025-10-15T06:50:04.315990\"},{\"path\":\"LICENSE\",\"content\":\"MIT License\\n\\nCopyright (c) 2025 CreatorJun\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \\\"Software\\\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \\\"AS IS\\\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\",\"size\":1066,\"language\":null,\"is_binary\":false,\"line_count\":21,\"is_source_code\":false,\"is_configuration\":false,\"is_documentation\":true,\"timestamp\":\"2025-10-15T06:50:04.316518\"},{\"path\":\"README.md\",\"content\":\"# GitHub Analyzer\\n\\n[![pub version](https://img.shields.io/pub/v/github_analyzer.svg)](https://pub.dev/packages/github_analyzer)\\n[![license](https://img.shields.io/badge/license-MIT-blue.svg)](https://opensource.org/licenses/MIT)\\n\\nA powerful and flexible Dart package to analyze GitHub repositories. It can process both remote repositories via URL and local directories on your machine. This tool is designed to extract comprehensive metadata, generate detailed statistics, and compile source code into a structured markdown context, making it perfect for feeding into Large Language Models (LLMs) or for conducting code audits.\\n\\n## Key Features\\n\\n- **Simple & Powerful API**: Analyze any public repository with a single function call. No complex setup required.\\n- **Dual Analysis Modes**: Analyze repositories from a remote GitHub URL or a local file path.\\n- **Comprehensive Reports**: Generates a detailed `AnalysisResult` object containing repository metadata, file-by-file analysis, language distribution, dependency detection, and more.\\n- **Smart Caching**: Avoids re-analyzing unchanged remote repositories by using a commit-based caching system, saving time and API calls.\\n- **High-Performance Scans**: Utilizes Dart Isolates for parallel processing of local files, ensuring fast and efficient analysis even on large codebases.\\n- **Incremental Analysis**: For local repositories, it can perform an incremental analysis by comparing against a previous result, processing only the files that have been added, modified, or deleted.\\n- **Real-time Progress**: Monitor the analysis progress through a callback, perfect for providing feedback in a UI or CLI.\\n- **Markdown Output**: Generates optimized markdown format that is 90% more token-efficient than JSON, perfect for AI context.\\n- **Smart Filtering**: Automatically excludes generated files, with configurable limits and priorities.\\n- **Memory Efficient**: Stream-based markdown generation for handling large repositories.\\n- **Customizable**: Fine-tune the analysis with a rich configuration object, or use dependency injection for complete control.\\n\\n## Getting Started\\n\\n### 1. Installation\\n\\n```bash\\ndart pub add github_analyzer\\n```\\n\\nOr add this to your package's `pubspec.yaml` file:\\n\\n```yaml\\ndependencies:\\n  github_analyzer: ^0.1.0\\n```\\n\\nThen install it by running:\\n\\n```bash\\ndart pub get\\n```\\n\\n### 2. Basic Usage\\n\\n#### Simplest Way - One Line\\n\\n```dart\\nimport 'package:github_analyzer/github_analyzer.dart';\\n\\nvoid main() async {\\n  // Analyze and generate markdown in one step\\n  final outputPath = await analyzeAndGenerate(\\n    'https://github.com/flutter/flutter',\\n  );\\n  \\n  print('âœ… Analysis saved to: $outputPath');\\n}\\n```\\n\\n#### Quick Analysis (Optimized for Speed)\\n\\n```dart\\nvoid main() async {\\n  // Fast analysis with optimized settings\\n  final result = await analyzeQuick('https://github.com/dart-lang/sdk');\\n  \\n  print('Repository: ${result.metadata.fullName}');\\n  print('Files: ${result.statistics.totalFiles}');\\n}\\n```\\n\\n#### LLM-Optimized Analysis\\n\\n```dart\\nvoid main() async {\\n  // Optimized for LLM context with automatic filtering\\n  final outputPath = await analyzeForLLM(\\n    'https://github.com/your/repo',\\n    maxFiles: 100,  // Limit to most important 100 files\\n    markdownConfig: MarkdownConfig.compact,  // Compact output\\n  );\\n  \\n  print('LLM context ready: $outputPath');\\n}\\n```\\n\\n### 3. Standard Usage with Progress\\n\\n```dart\\nimport 'package:github_analyzer/github_analyzer.dart';\\n\\nvoid main() async {\\n  try {\\n    // Analyze with progress tracking\\n    final result = await analyze(\\n      'https://github.com/flutter/flutter',\\n      \\n      progressCallback: (progress) {\\n        final percentage = (progress.progress * 100).toStringAsFixed(1);\\n        print('${progress.phase.name} $percentage% - ${progress.message}');\\n      },\\n      \\n      verbose: true,\\n    );\\n    \\n    print('âœ… Analysis Complete!');\\n    print('Repository: ${result.metadata.fullName}');\\n    print('Primary Language: ${result.metadata.language}');\\n    print('Total Files: ${result.files.length}');\\n    print('Total Lines: ${result.statistics.totalLines}');\\n    \\n    // Generate markdown with auto-naming\\n    final outputPath = await ContextGenerator.generate(result);\\n    print('ğŸ“„ Markdown saved to: $outputPath');\\n    \\n  } catch (e) {\\n    print('âŒ An error occurred: $e');\\n  }\\n}\\n```\\n\\n## Advanced Usage\\n\\n### Custom Configuration\\n\\nFor more control, use `GithubAnalyzerConfig`:\\n\\n```dart\\nfinal config = GithubAnalyzerConfig(\\n  githubToken: 'YOUR_GITHUB_TOKEN',\\n  \\n  // Smart filtering options\\n  excludeGeneratedFiles: true,  // Auto-exclude *.g.dart, etc.\\n  maxTotalFiles: 500,  // Limit total files analyzed\\n  prioritizeImportantFiles: true,  // Focus on main code\\n  \\n  // Additional exclude patterns\\n  excludePatterns: [\\n    ...kDefaultExcludePatterns,\\n    '*.g.dart',\\n    'test_data/**',\\n  ],\\n  \\n  maxFileSize: 5 * 1024 * 1024,  // 5MB limit\\n  enableCache: true,\\n  enableIsolatePool: true,\\n);\\n\\nfinal result = await analyze(\\n  'https://github.com/your/repo',\\n  config: config,\\n);\\n```\\n\\n### Preset Configurations\\n\\n```dart\\n// Quick analysis (no cache, limited files)\\nfinal quickConfig = GithubAnalyzerConfig.quick(\\n  githubToken: 'YOUR_TOKEN',\\n);\\n\\n// LLM-optimized (excludes tests, examples, max 200 files)\\nfinal llmConfig = GithubAnalyzerConfig.forLLM(\\n  githubToken: 'YOUR_TOKEN',\\n  maxFiles: 200,\\n);\\n```\\n\\n### Markdown Generation Options\\n\\n```dart\\n// Standard output (all content)\\nawait ContextGenerator.generate(\\n  result,\\n  config: MarkdownConfig.standard,\\n);\\n\\n// Compact output (limited files and content)\\nawait ContextGenerator.generate(\\n  result,\\n  config: MarkdownConfig.compact,\\n);\\n\\n// Custom output\\nawait ContextGenerator.generate(\\n  result,\\n  config: MarkdownConfig(\\n    maxFiles: 100,  // Limit to 100 files\\n    maxContentSize: 50000,  // Truncate large files\\n    includeBinaryStats: false,\\n    includeErrors: false,\\n  ),\\n);\\n\\n// Specify output location\\nawait ContextGenerator.generate(\\n  result,\\n  outputPath: './my_analysis.md',\\n  // or use outputDir to auto-generate filename\\n  outputDir: './output',\\n);\\n```\\n\\n### Analyzing Local Directories\\n\\n```dart\\n// Works the same way\\nfinal result = await analyze('/path/to/your/project');\\n\\n// Or quick local analysis\\nfinal result = await analyzeQuick('/path/to/your/project');\\n```\\n\\n### Memory-Efficient Large Repository Handling\\n\\nFor very large repositories, use streaming generation:\\n\\n```dart\\nfinal result = await analyze('https://github.com/large/repo');\\n\\n// Directly write to file without loading full content in memory\\nawait MarkdownGenerator.generateToFile(\\n  result,\\n  './large_repo_analysis.md',\\n  config: MarkdownConfig(\\n    maxFiles: 200,\\n    maxContentSize: 100000,\\n  ),\\n);\\n```\\n\\n## Output Format\\n\\nThe package generates markdown output with the following structure:\\n\\n```markdown\\n# Repository Name\\n\\n## Repository Information\\n**Repository:** `owner/repo`\\n**Language:** Dart | **Stars:** 1234 | **Forks:** 567\\n\\n## Statistics\\n- **Total Files:** 150\\n- **Total Lines:** 25000\\n- **Source Files:** 120\\n\\n## Directory Structure\\n`\\nlib/\\n  src/\\n    models/\\n    services/\\n`\\n\\n## Language Distribution\\n- **Dart:** 120 files (80.0%)\\n- **YAML:** 15 files (10.0%)\\n\\n## Source Code\\n### lib/main.dart\\n`dart\\n// Full source code with syntax highlighting\\n`\\n```\\n\\n### Performance Characteristics\\n\\n- **Token Efficiency**: 90% reduction compared to JSON format\\n- **Memory Usage**: Stream-based generation handles repositories of any size\\n- **Speed**: Parallel processing with automatic optimization\\n- **Smart Filtering**: Automatically excludes generated files and low-priority content\\n\\n## API Summary\\n\\n### Quick Functions\\n\\n- `analyzeAndGenerate()` - Analyze and create markdown in one step\\n- `analyzeQuick()` - Fast analysis with optimized settings\\n- `analyzeForLLM()` - LLM-optimized with automatic filtering\\n\\n### Core Functions\\n\\n- `analyze()` - Standard analysis with full control\\n- `ContextGenerator.generate()` - Generate markdown from result\\n- `MarkdownGenerator.generateToFile()` - Memory-efficient file generation\\n\\n### Configuration\\n\\n- `GithubAnalyzerConfig.quick()` - Fast analysis preset\\n- `GithubAnalyzerConfig.forLLM()` - LLM-optimized preset\\n- `MarkdownConfig.standard` - Full output\\n- `MarkdownConfig.compact` - Compact output\\n\\n## Contributing\\n\\nContributions are welcome! If you find a bug or have a feature request, please open an issue on the [GitHub issue tracker](https://github.com/cruxhan/github_analyzer/issues).\\n\\n## License\\n\\nThis package is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.\\n\",\"size\":8512,\"language\":\"Markdown\",\"is_binary\":false,\"line_count\":303,\"is_source_code\":true,\"is_configuration\":false,\"is_documentation\":true,\"timestamp\":\"2025-10-15T06:50:04.317209\"},{\"path\":\"analysis_options.yaml\",\"content\":\"\\n\\n# Additional information about this file can be found at\\n# https://dart.dev/guides/language/analysis-options\\n\",\"size\":111,\"language\":\"YAML\",\"is_binary\":false,\"line_count\":5,\"is_source_code\":true,\"is_configuration\":true,\"is_documentation\":false,\"timestamp\":\"2025-10-15T06:50:04.319098\"},{\"path\":\"lib/github_analyzer.dart\",\"content\":\"library;\\n\\nimport 'dart:async';\\n\\nimport 'package:github_analyzer/src/common/config.dart';\\nimport 'package:github_analyzer/src/common/logger.dart';\\nimport 'package:github_analyzer/src/core/cache_service.dart';\\nimport 'package:github_analyzer/src/core/local_analyzer_service.dart';\\nimport 'package:github_analyzer/src/core/remote_analyzer_service.dart';\\nimport 'package:github_analyzer/src/data/providers/github_api_provider.dart';\\nimport 'package:github_analyzer/src/data/providers/zip_downloader.dart';\\nimport 'package:github_analyzer/src/github_analyzer.dart';\\nimport 'package:github_analyzer/src/infrastructure/http_client_manager.dart';\\nimport 'package:github_analyzer/src/infrastructure/isolate_pool.dart';\\nimport 'package:github_analyzer/src/models/analysis_progress.dart';\\nimport 'package:github_analyzer/src/models/analysis_result.dart';\\nimport 'package:github_analyzer/src/common/utils/context_generator.dart';\\nimport 'package:github_analyzer/src/common/utils/markdown_generator.dart';\\n\\nexport 'src/common/config.dart';\\nexport 'src/common/errors/analyzer_exception.dart';\\nexport 'src/common/logger.dart';\\nexport 'src/github_analyzer.dart' show GithubAnalyzer;\\nexport 'src/models/analysis_error.dart';\\nexport 'src/models/analysis_progress.dart';\\nexport 'src/models/analysis_result.dart';\\nexport 'src/models/analysis_statistics.dart';\\nexport 'src/models/repository_metadata.dart';\\nexport 'src/models/source_file.dart';\\nexport 'src/common/utils/context_generator.dart';\\nexport 'src/common/utils/markdown_generator.dart';\\n\\n/// Analyzes a repository and returns the analysis result\\nFuture<AnalysisResult> analyze(\\n  String repositoryUrl, {\\n  GithubAnalyzerConfig? config,\\n  void Function(AnalysisProgress)? progressCallback,\\n  bool verbose = false,\\n}) async {\\n  setupLogger(verbose: verbose);\\n\\n  final effectiveConfig = config ?? GithubAnalyzerConfig();\\n\\n  final httpClientManager = HttpClientManager();\\n  final apiProvider = GithubApiProvider(\\n    httpClientManager: httpClientManager,\\n    token: effectiveConfig.githubToken,\\n  );\\n  final zipDownloader = ZipDownloader(httpClientManager: httpClientManager);\\n  final cacheService = effectiveConfig.enableCache\\n      ? CacheService(\\n          cacheDirectory: effectiveConfig.cacheDirectory,\\n          maxAge: effectiveConfig.cacheDuration,\\n        )\\n      : null;\\n  final isolatePool = effectiveConfig.enableIsolatePool\\n      ? IsolatePool(size: effectiveConfig.isolatePoolSize)\\n      : null;\\n  final localAnalyzer = LocalAnalyzerService(config: effectiveConfig);\\n  final remoteAnalyzer = RemoteAnalyzerService(\\n    config: effectiveConfig,\\n    apiProvider: apiProvider,\\n    zipDownloader: zipDownloader,\\n    cacheService: cacheService,\\n  );\\n\\n  final analyzer = GithubAnalyzer(\\n    config: effectiveConfig,\\n    httpClientManager: httpClientManager,\\n    apiProvider: apiProvider,\\n    zipDownloader: zipDownloader,\\n    localAnalyzer: localAnalyzer,\\n    remoteAnalyzer: remoteAnalyzer,\\n    cacheService: cacheService,\\n    isolatePool: isolatePool,\\n  );\\n\\n  StreamSubscription<AnalysisProgress>? progressSubscription;\\n  if (progressCallback != null) {\\n    progressSubscription = analyzer.progressStream.listen(progressCallback);\\n  }\\n\\n  try {\\n    return await analyzer.analyze(repositoryUrl);\\n  } finally {\\n    await progressSubscription?.cancel();\\n    await analyzer.dispose();\\n  }\\n}\\n\\n/// Analyzes a repository and generates markdown output in one step\\nFuture<String> analyzeAndGenerate(\\n  String repositoryUrl, {\\n  String? outputPath,\\n  String? outputDir,\\n  GithubAnalyzerConfig? analyzerConfig,\\n  MarkdownConfig markdownConfig = MarkdownConfig.standard,\\n  void Function(AnalysisProgress)? progressCallback,\\n  bool verbose = false,\\n}) async {\\n  final result = await analyze(\\n    repositoryUrl,\\n    config: analyzerConfig,\\n    progressCallback: progressCallback,\\n    verbose: verbose,\\n  );\\n\\n  return await ContextGenerator.generate(\\n    result,\\n    outputPath: outputPath,\\n    outputDir: outputDir,\\n    config: markdownConfig,\\n  );\\n}\\n\\n/// Quick analysis with optimized settings for fast results\\nFuture<AnalysisResult> analyzeQuick(\\n  String repositoryUrl, {\\n  String? githubToken,\\n  void Function(AnalysisProgress)? progressCallback,\\n}) async {\\n  return await analyze(\\n    repositoryUrl,\\n    config: GithubAnalyzerConfig.quick(githubToken: githubToken),\\n    progressCallback: progressCallback,\\n  );\\n}\\n\\n/// Analysis optimized for LLM context generation\\nFuture<String> analyzeForLLM(\\n  String repositoryUrl, {\\n  String? outputPath,\\n  String? outputDir,\\n  String? githubToken,\\n  int maxFiles = 200,\\n  MarkdownConfig markdownConfig = MarkdownConfig.standard,\\n  void Function(AnalysisProgress)? progressCallback,\\n  bool verbose = false,\\n}) async {\\n  final result = await analyze(\\n    repositoryUrl,\\n    config: GithubAnalyzerConfig.forLLM(\\n      githubToken: githubToken,\\n      maxFiles: maxFiles,\\n    ),\\n    progressCallback: progressCallback,\\n    verbose: verbose,\\n  );\\n\\n  return await ContextGenerator.generate(\\n    result,\\n    outputPath: outputPath,\\n    outputDir: outputDir,\\n    config: markdownConfig,\\n  );\\n}\\n\",\"size\":5057,\"language\":\"Dart\",\"is_binary\":false,\"line_count\":157,\"is_source_code\":true,\"is_configuration\":false,\"is_documentation\":false,\"timestamp\":\"2025-10-15T06:50:04.319798\"},{\"path\":\"lib/src/common/config.dart\",\"content\":\"import 'dart:io';\\nimport 'package:github_analyzer/src/common/constants.dart';\\n\\n/// Configuration class for the GithubAnalyzer\\nclass GithubAnalyzerConfig {\\n  final String? githubToken;\\n  final List<String> excludePatterns;\\n  final List<String> includePatterns;\\n  final int maxFileSize;\\n  final bool enableCache;\\n  final String cacheDirectory;\\n  final Duration cacheDuration;\\n  final int maxConcurrentRequests;\\n  final bool enableIsolatePool;\\n  final int isolatePoolSize;\\n  final int maxRetries;\\n  final Duration retryDelay;\\n\\n  final bool excludeGeneratedFiles;\\n  final int maxTotalFiles;\\n  final bool prioritizeImportantFiles;\\n\\n  const GithubAnalyzerConfig._private({\\n    this.githubToken,\\n    required this.excludePatterns,\\n    this.includePatterns = const [],\\n    this.maxFileSize = kDefaultMaxFileSize,\\n    this.enableCache = true,\\n    required this.cacheDirectory,\\n    this.cacheDuration = kDefaultCacheDuration,\\n    this.maxConcurrentRequests = kDefaultMaxConcurrentRequests,\\n    this.enableIsolatePool = true,\\n    this.isolatePoolSize = 4,\\n    this.maxRetries = kDefaultMaxRetries,\\n    this.retryDelay = const Duration(seconds: 2),\\n    this.excludeGeneratedFiles = true,\\n    this.maxTotalFiles = 0,\\n    this.prioritizeImportantFiles = true,\\n  });\\n\\n  factory GithubAnalyzerConfig({\\n    String? githubToken,\\n    List<String>? excludePatterns,\\n    List<String>? includePatterns,\\n    int maxFileSize = kDefaultMaxFileSize,\\n    bool enableCache = true,\\n    String? cacheDirectory,\\n    Duration cacheDuration = kDefaultCacheDuration,\\n    int maxConcurrentRequests = kDefaultMaxConcurrentRequests,\\n    bool enableIsolatePool = true,\\n    int? isolatePoolSize,\\n    int maxRetries = kDefaultMaxRetries,\\n    Duration retryDelay = const Duration(seconds: 2),\\n    bool excludeGeneratedFiles = true,\\n    int maxTotalFiles = 0,\\n    bool prioritizeImportantFiles = true,\\n  }) {\\n    final size =\\n        isolatePoolSize ?? (Platform.isAndroid || Platform.isIOS ? 2 : 4);\\n\\n    return GithubAnalyzerConfig._private(\\n      githubToken: githubToken,\\n      excludePatterns: excludePatterns ?? kDefaultExcludePatterns,\\n      includePatterns: includePatterns ?? const [],\\n      maxFileSize: maxFileSize,\\n      enableCache: enableCache,\\n      cacheDirectory: cacheDirectory ?? '.github_analyzer_cache',\\n      cacheDuration: cacheDuration,\\n      maxConcurrentRequests: maxConcurrentRequests,\\n      enableIsolatePool: enableIsolatePool,\\n      isolatePoolSize: size,\\n      maxRetries: maxRetries,\\n      retryDelay: retryDelay,\\n      excludeGeneratedFiles: excludeGeneratedFiles,\\n      maxTotalFiles: maxTotalFiles,\\n      prioritizeImportantFiles: prioritizeImportantFiles,\\n    );\\n  }\\n\\n  factory GithubAnalyzerConfig.quick({\\n    String? githubToken,\\n    List<String>? excludePatterns,\\n  }) {\\n    return GithubAnalyzerConfig(\\n      githubToken: githubToken,\\n      excludePatterns: excludePatterns,\\n      enableCache: false,\\n      enableIsolatePool: false,\\n      maxTotalFiles: 100,\\n      excludeGeneratedFiles: true,\\n      prioritizeImportantFiles: true,\\n    );\\n  }\\n\\n  factory GithubAnalyzerConfig.forLLM({\\n    String? githubToken,\\n    List<String>? excludePatterns,\\n    int maxFiles = 200,\\n  }) {\\n    return GithubAnalyzerConfig(\\n      githubToken: githubToken,\\n      excludePatterns: [\\n        ...kDefaultExcludePatterns,\\n        ...?excludePatterns,\\n        'test/**',\\n        'tests/**',\\n        '**_test.dart',\\n        'example/**',\\n      ],\\n      excludeGeneratedFiles: true,\\n      maxTotalFiles: maxFiles,\\n      prioritizeImportantFiles: true,\\n    );\\n  }\\n\\n  List<String> get effectiveExcludePatterns {\\n    if (!excludeGeneratedFiles) {\\n      return excludePatterns;\\n    }\\n\\n    return [\\n      ...excludePatterns,\\n      '*.g.dart',\\n      '*.freezed.dart',\\n      '*.gr.dart',\\n      '*.config.dart',\\n      '*.pb.dart',\\n      '*.pbenum.dart',\\n      '*.pbgrpc.dart',\\n      '*.pbjson.dart',\\n    ];\\n  }\\n}\\n\",\"size\":3879,\"language\":\"Dart\",\"is_binary\":false,\"line_count\":134,\"is_source_code\":true,\"is_configuration\":false,\"is_documentation\":false,\"timestamp\":\"2025-10-15T06:50:04.320351\"},{\"path\":\"lib/src/common/constants.dart\",\"content\":\"// lib/src/common/constants.dart\\n\\nconst List<String> kDefaultExcludePatterns = [\\n  'node_modules/**',\\n  '.git/**',\\n  '.svn/**',\\n  '.hg/**',\\n  'build/**',\\n  'dist/**',\\n  'out/**',\\n  'target/**',\\n  '.dart_tool/**',\\n  '.pub-cache/**',\\n  '.gradle/**',\\n  '.idea/**',\\n  '.vscode/**',\\n  '*.lock',\\n  '*.log',\\n  '.DS_Store',\\n  'Thumbs.db',\\n  '__pycache__/**',\\n  '*.pyc',\\n  '.pytest_cache/**',\\n  'venv/**',\\n  'env/**',\\n  '.env/**',\\n  'coverage/**',\\n  '.nyc_output/**',\\n  'tmp/**',\\n  'temp/**',\\n  '*.min.js',\\n  '*.min.css',\\n  'package-lock.json',\\n  'yarn.lock',\\n  'Cargo.lock',\\n];\\n\\nconst Set<String> kBinaryExtensions = {\\n  'png',\\n  'jpg',\\n  'jpeg',\\n  'gif',\\n  'bmp',\\n  'ico',\\n  'svg',\\n  'webp',\\n  'tiff',\\n  'tif',\\n  'mp3',\\n  'wav',\\n  'ogg',\\n  'flac',\\n  'aac',\\n  'm4a',\\n  'wma',\\n  'mp4',\\n  'avi',\\n  'mov',\\n  'wmv',\\n  'flv',\\n  'mkv',\\n  'webm',\\n  'm4v',\\n  'zip',\\n  'tar',\\n  'gz',\\n  'bz2',\\n  'rar',\\n  '7z',\\n  'xz',\\n  'tgz',\\n  'exe',\\n  'dll',\\n  'so',\\n  'dylib',\\n  'bin',\\n  'app',\\n  'dmg',\\n  'pkg',\\n  'deb',\\n  'rpm',\\n  'pdf',\\n  'doc',\\n  'docx',\\n  'xls',\\n  'xlsx',\\n  'ppt',\\n  'pptx',\\n  'odt',\\n  'ods',\\n  'odp',\\n  'ttf',\\n  'otf',\\n  'woff',\\n  'woff2',\\n  'eot',\\n  'class',\\n  'jar',\\n  'war',\\n  'ear',\\n  'pyc',\\n  'pyo',\\n  'db',\\n  'sqlite',\\n  'cache',\\n};\\n\\nconst Set<String> kDocumentationFileNames = {\\n  'readme',\\n  'changelog',\\n  'license',\\n  'contributing',\\n  'authors',\\n  'history',\\n};\\n\\nconst Set<String> kDocumentationExtensions = {\\n  'md',\\n  'markdown',\\n  'rst',\\n  'txt',\\n};\\n\\nconst Set<String> kConfigurationFileNames = {\\n  'pubspec.yaml',\\n  'package.json',\\n  'tsconfig.json',\\n  'jsconfig.json',\\n  'webpack.config.js',\\n  'babel.config.js',\\n  'rollup.config.js',\\n  'vite.config.js',\\n  '.eslintrc',\\n  '.prettierrc',\\n  '.editorconfig',\\n  'cargo.toml',\\n  'go.mod',\\n  'pom.xml',\\n  'build.gradle',\\n  'settings.gradle',\\n  'cmakelists.txt',\\n  '.gitignore',\\n  '.dockerignore',\\n  'dockerfile',\\n};\\n\\nconst Set<String> kConfigurationExtensions = {\\n  'json',\\n  'yaml',\\n  'yml',\\n  'toml',\\n  'ini',\\n  'cfg',\\n  'conf',\\n  'config',\\n};\\n\\n// ## ìˆ˜ì •ëœ ë¶€ë¶„ ##\\n// main íŒŒì¼ ëª©ë¡ì„ ìƒìˆ˜ë¡œ ì¶”ê°€\\nconst Set<String> kMainFilePatterns = {\\n  'main.dart',\\n  'main.py',\\n  'main.js',\\n  'main.ts',\\n  'index.js',\\n  'index.ts',\\n  'app.js',\\n  'app.ts',\\n  'server.js',\\n  'server.ts',\\n  'Main.java',\\n  'main.go',\\n  'main.rs',\\n  'main.c',\\n  'main.cpp',\\n  'main.swift',\\n  'MainActivity.kt',\\n  'AppDelegate.swift',\\n};\\n\\nconst int kDefaultMaxFileSize = 10 * 1024 * 1024; // 10 MB\\nconst Duration kDefaultCacheDuration = Duration(hours: 24);\\nconst Duration kDefaultRequestTimeout = Duration(seconds: 30);\\nconst int kDefaultMaxConcurrentRequests = 10;\\nconst int kDefaultMaxRetries = 3;\\n\",\"size\":2637,\"language\":\"Dart\",\"is_binary\":false,\"line_count\":186,\"is_source_code\":true,\"is_configuration\":false,\"is_documentation\":false,\"timestamp\":\"2025-10-15T06:50:04.321006\"},{\"path\":\"lib/src/common/errors/analyzer_exception.dart\",\"content\":\"/// Defines the types of errors that can occur during analysis.\\nenum AnalyzerErrorCode {\\n  invalidUrl,\\n  repositoryNotFound,\\n  accessDenied,\\n  rateLimitExceeded,\\n  networkError,\\n  cacheError,\\n  analysisError,\\n  directoryNotFound,\\n  fileReadError,\\n  archiveError,\\n  configurationError,\\n}\\n\\n/// A custom exception class for handling errors within the analyzer.\\n///\\n/// This class standardizes error reporting by providing a consistent\\n/// structure that includes a message, detailed information, an error code,\\n/// and the original exception that was caught.\\nclass AnalyzerException implements Exception {\\n  final String message;\\n  final String? details;\\n  final AnalyzerErrorCode code;\\n  final Object? originalException;\\n  final StackTrace? stackTrace;\\n\\n  /// Creates an instance of [AnalyzerException].\\n  AnalyzerException(\\n    this.message, {\\n    this.details,\\n    required this.code,\\n    this.originalException,\\n    this.stackTrace,\\n  });\\n\\n  @override\\n  String toString() {\\n    final buffer = StringBuffer('AnalyzerException [$code]: $message');\\n    if (details != null) {\\n      buffer.write('\\\\n  Details: $details');\\n    }\\n    if (originalException != null) {\\n      buffer.write('\\\\n  Original Exception: ${originalException.toString()}');\\n    }\\n    if (stackTrace != null) {\\n      buffer.write('\\\\n  Stack Trace:\\\\n$stackTrace');\\n    }\\n    return buffer.toString();\\n  }\\n}\\n\",\"size\":1372,\"language\":\"Dart\",\"is_binary\":false,\"line_count\":52,\"is_source_code\":true,\"is_configuration\":false,\"is_documentation\":false,\"timestamp\":\"2025-10-15T06:50:04.321796\"},{\"path\":\"lib/src/common/language_info.dart\",\"content\":\"import 'package:path/path.dart' as path;\\n\\nconst Map<String, String> kLanguageExtensions = {\\n  // Programming Languages\\n  'dart': 'Dart',\\n  'py': 'Python',\\n  'js': 'JavaScript',\\n  'ts': 'TypeScript',\\n  'jsx': 'JavaScript',\\n  'tsx': 'TypeScript',\\n  'java': 'Java',\\n  'kt': 'Kotlin',\\n  'kts': 'Kotlin',\\n  'swift': 'Swift',\\n  'c': 'C',\\n  'cpp': 'C++',\\n  'cc': 'C++',\\n  'cxx': 'C++',\\n  'h': 'C',\\n  'hpp': 'C++',\\n  'cs': 'C#',\\n  'go': 'Go',\\n  'rs': 'Rust',\\n  'rb': 'Ruby',\\n  'php': 'PHP',\\n  'scala': 'Scala',\\n  'r': 'R',\\n  'lua': 'Lua',\\n  'pl': 'Perl',\\n  'sh': 'Shell',\\n  'bash': 'Shell',\\n  'zsh': 'Shell',\\n  'fish': 'Shell',\\n  'ps1': 'PowerShell',\\n  'bat': 'Batch',\\n  'cmd': 'Batch',\\n\\n  // Web\\n  'html': 'HTML',\\n  'htm': 'HTML',\\n  'css': 'CSS',\\n  'scss': 'SCSS',\\n  'sass': 'Sass',\\n  'less': 'Less',\\n  'vue': 'Vue',\\n  'svelte': 'Svelte',\\n\\n  // Data & Config\\n  'json': 'JSON',\\n  'yaml': 'YAML',\\n  'yml': 'YAML',\\n  'toml': 'TOML',\\n  'xml': 'XML',\\n  'ini': 'INI',\\n  'conf': 'Config',\\n  'config': 'Config',\\n  'env': 'Environment',\\n\\n  // Documentation\\n  'md': 'Markdown',\\n  'markdown': 'Markdown',\\n  'rst': 'reStructuredText',\\n  'txt': 'Text',\\n  'tex': 'LaTeX',\\n\\n  // Database\\n  'sql': 'SQL',\\n  'sqlite': 'SQLite',\\n  'psql': 'PostgreSQL',\\n\\n  // Build & Package\\n  'gradle': 'Gradle',\\n  'maven': 'Maven',\\n  'cmake': 'CMake',\\n  'make': 'Makefile',\\n\\n  // Others\\n  'asm': 'Assembly',\\n  's': 'Assembly',\\n  'vim': 'VimScript',\\n  'el': 'Emacs Lisp',\\n  'clj': 'Clojure',\\n  'ex': 'Elixir',\\n  'exs': 'Elixir',\\n  'erl': 'Erlang',\\n  'hrl': 'Erlang',\\n  'hs': 'Haskell',\\n  'ml': 'OCaml',\\n  'nim': 'Nim',\\n  'v': 'V',\\n  'zig': 'Zig',\\n};\\n\\nconst Map<String, String> kSyntaxHighlighting = {\\n  'Dart': 'dart',\\n  'Python': 'python',\\n  'JavaScript': 'javascript',\\n  'TypeScript': 'typescript',\\n  'Java': 'java',\\n  'Kotlin': 'kotlin',\\n  'Swift': 'swift',\\n  'C': 'c',\\n  'C++': 'cpp',\\n  'C#': 'csharp',\\n  'Go': 'go',\\n  'Rust': 'rust',\\n  'Ruby': 'ruby',\\n  'PHP': 'php',\\n  'Scala': 'scala',\\n  'R': 'r',\\n  'Lua': 'lua',\\n  'Perl': 'perl',\\n  'Shell': 'bash',\\n  'Bash': 'bash',\\n  'Zsh': 'zsh',\\n  'Fish': 'fish',\\n  'PowerShell': 'powershell',\\n  'Batch': 'batch',\\n  'HTML': 'html',\\n  'CSS': 'css',\\n  'SCSS': 'scss',\\n  'Sass': 'sass',\\n  'Less': 'less',\\n  'Vue': 'vue',\\n  'Svelte': 'svelte',\\n  'JSON': 'json',\\n  'YAML': 'yaml',\\n  'TOML': 'toml',\\n  'XML': 'xml',\\n  'Markdown': 'markdown',\\n  'SQL': 'sql',\\n  'Gradle': 'gradle',\\n  'CMake': 'cmake',\\n  'Makefile': 'makefile',\\n  'Dockerfile': 'dockerfile',\\n  'Assembly': 'asm',\\n  'VimScript': 'vim',\\n  'Clojure': 'clojure',\\n  'Elixir': 'elixir',\\n  'Erlang': 'erlang',\\n  'Haskell': 'haskell',\\n  'OCaml': 'ocaml',\\n  'Nim': 'nim',\\n  'Zig': 'zig',\\n};\\n\\nString? detectLanguage(String filePath) {\\n  final fileName = path.basename(filePath);\\n  final fileNameLower = fileName.toLowerCase();\\n\\n  if (fileNameLower == 'dockerfile' ||\\n      fileNameLower.startsWith('dockerfile.')) {\\n    return 'Dockerfile';\\n  }\\n  if (fileNameLower == 'makefile' || fileNameLower.startsWith('makefile.')) {\\n    return 'Makefile';\\n  }\\n  if (fileNameLower == 'gemfile') {\\n    return 'Ruby';\\n  }\\n  if (fileNameLower == 'rakefile') {\\n    return 'Ruby';\\n  }\\n  if (fileNameLower == '.bashrc' || fileNameLower == '.zshrc') {\\n    return 'Shell';\\n  }\\n\\n  final extension = path.extension(filePath).toLowerCase();\\n  if (extension.isEmpty) {\\n    return null;\\n  }\\n\\n  final ext = extension.substring(1);\\n  return kLanguageExtensions[ext];\\n}\\n\\nString? getSyntaxHighlighting(String filePath) {\\n  final language = detectLanguage(filePath);\\n  if (language == null) return null;\\n  return kSyntaxHighlighting[language];\\n}\\n\",\"size\":3570,\"language\":\"Dart\",\"is_binary\":false,\"line_count\":182,\"is_source_code\":true,\"is_configuration\":false,\"is_documentation\":false,\"timestamp\":\"2025-10-15T06:50:04.322252\"},{\"path\":\"lib/src/common/logger.dart\",\"content\":\"import 'package:logging/logging.dart';\\n\\n/// The root logger for the application.\\nfinal logger = Logger('GithubAnalyzer');\\n\\n/// Sets up the logger for the application.\\n///\\n/// Configures the logging level and sets up a listener to print log records\\n/// to the console with a specific format.\\n///\\n/// If [verbose] is true, the log level is set to [Level.ALL], otherwise it\\n/// defaults to [Level.INFO].\\nvoid setupLogger({bool verbose = false}) {\\n  Logger.root.level = verbose ? Level.ALL : Level.INFO;\\n  Logger.root.onRecord.listen((record) {\\n    print(\\n      '${record.time} [${record.level.name}] ${record.loggerName}: ${record.message}',\\n    );\\n    if (record.error != null) {\\n      print('  Error: ${record.error}');\\n    }\\n    if (record.stackTrace != null) {\\n      print('  Stack Trace:\\\\n${record.stackTrace}');\\n    }\\n  });\\n}\\n\",\"size\":829,\"language\":\"Dart\",\"is_binary\":false,\"line_count\":27,\"is_source_code\":true,\"is_configuration\":false,\"is_documentation\":false,\"timestamp\":\"2025-10-15T06:50:04.322755\"},{\"path\":\"lib/src/common/utils/context_generator.dart\",\"content\":\"import 'package:path/path.dart' as path;\\nimport 'package:github_analyzer/src/models/analysis_result.dart';\\nimport 'package:github_analyzer/src/common/utils/markdown_generator.dart';\\nimport 'package:github_analyzer/src/infrastructure/file_system/file_system.dart';\\n\\n/// Generates context output files from analysis results using platform-independent file system\\nclass ContextGenerator {\\n  static final IFileSystem _fs = getFileSystem();\\n\\n  /// Generates a markdown file from the analysis result with automatic naming\\n  static Future<String> generate(\\n    AnalysisResult result, {\\n    String? outputPath,\\n    String? outputDir,\\n    MarkdownConfig config = MarkdownConfig.standard,\\n  }) async {\\n    final filePath = _resolveOutputPath(result, outputPath, outputDir);\\n\\n    // Ensure output directory exists\\n    final dirPath = path.dirname(filePath);\\n    final dirExists = await _fs.directoryExists(dirPath);\\n    if (!dirExists) {\\n      await _fs.createDirectory(dirPath);\\n    }\\n\\n    await MarkdownGenerator.generateToFile(result, filePath, config: config);\\n\\n    return filePath;\\n  }\\n\\n  /// Generates a markdown string from the analysis result\\n  static String generateString(\\n    AnalysisResult result, {\\n    MarkdownConfig config = MarkdownConfig.standard,\\n  }) {\\n    return MarkdownGenerator.generate(result, config: config);\\n  }\\n\\n  /// Resolves the output path with smart defaults\\n  static String _resolveOutputPath(\\n    AnalysisResult result,\\n    String? outputPath,\\n    String? outputDir,\\n  ) {\\n    if (outputPath != null) {\\n      return _ensureMarkdownExtension(outputPath);\\n    }\\n\\n    final dir = outputDir ?? '.';\\n    final fileName = _generateFileName(result);\\n\\n    return path.join(dir, fileName);\\n  }\\n\\n  /// Generates a clean file name from repository metadata\\n  static String _generateFileName(AnalysisResult result) {\\n    var name = result.metadata.name;\\n\\n    // Clean the name for file system\\n    name = name.replaceAll(RegExp(r'[^\\\\w\\\\-\\\\.]'), '_');\\n    name = name.replaceAll(RegExp(r'_+'), '_');\\n\\n    return '${name}_analysis.md';\\n  }\\n\\n  /// Ensures the file has .md extension\\n  static String _ensureMarkdownExtension(String filePath) {\\n    if (!filePath.endsWith('.md')) {\\n      return '$filePath.md';\\n    }\\n    return filePath;\\n  }\\n}\\n\",\"size\":2246,\"language\":\"Dart\",\"is_binary\":false,\"line_count\":74,\"is_source_code\":true,\"is_configuration\":false,\"is_documentation\":false,\"timestamp\":\"2025-10-15T06:50:04.323216\"},{\"path\":\"lib/src/common/utils/directory_tree_generator.dart\",\"content\":\"import 'package:path/path.dart' as p;\\n\\n/// Generates a textual representation of a directory tree from a list of file paths.\\n///\\n/// This utility is useful for providing a high-level overview of the\\n/// repository structure, which can be beneficial for context in LLMs.\\nclass DirectoryTreeGenerator {\\n  /// Generates a directory tree string from a list of file paths.\\n  ///\\n  /// The [paths] should be relative paths from the repository root.\\n  /// The [maxDepth] parameter can be used to limit the depth of the tree.\\n  static String generate(List<String> paths, {int? maxDepth}) {\\n    if (paths.isEmpty) {\\n      return '';\\n    }\\n\\n    // Use a map to represent the directory structure as a tree.\\n    final Map<String, dynamic> tree = {};\\n\\n    for (final path in paths) {\\n      final parts = p.split(path);\\n      Map<String, dynamic> currentNode = tree;\\n      for (final part in parts) {\\n        currentNode = currentNode.putIfAbsent(part, () => <String, dynamic>{});\\n      }\\n    }\\n\\n    final buffer = StringBuffer();\\n    _buildTree(buffer, tree, '', true, maxDepth, 0);\\n    return buffer.toString();\\n  }\\n\\n  static void _buildTree(\\n    StringBuffer buffer,\\n    Map<String, dynamic> node,\\n    String prefix,\\n    bool isLast,\\n    int? maxDepth,\\n    int currentDepth,\\n  ) {\\n    if (maxDepth != null && currentDepth > maxDepth) {\\n      return;\\n    }\\n\\n    final sortedKeys = node.keys.toList()..sort();\\n    for (int i = 0; i < sortedKeys.length; i++) {\\n      final key = sortedKeys[i];\\n      final childNode = node[key] as Map<String, dynamic>;\\n      final isCurrentLast = i == sortedKeys.length - 1;\\n\\n      buffer.write(prefix);\\n      buffer.write(isCurrentLast ? 'â””â”€â”€ ' : 'â”œâ”€â”€ ');\\n      buffer.writeln(key);\\n\\n      final newPrefix = prefix + (isCurrentLast ? '    ' : 'â”‚   ');\\n      if (childNode.isNotEmpty) {\\n        _buildTree(\\n          buffer,\\n          childNode,\\n          newPrefix,\\n          isCurrentLast,\\n          maxDepth,\\n          currentDepth + 1,\\n        );\\n      }\\n    }\\n  }\\n}\\n\",\"size\":2005,\"language\":\"Dart\",\"is_binary\":false,\"line_count\":69,\"is_source_code\":true,\"is_configuration\":false,\"is_documentation\":false,\"timestamp\":\"2025-10-15T06:50:04.323674\"},{\"path\":\"lib/src/common/utils/file_utils.dart\",\"content\":\"// lib/src/common/utils/file_utils.dart\\n\\nimport 'dart:io';\\nimport 'package:path/path.dart' as path;\\nimport 'package:github_analyzer/src/common/constants.dart';\\nimport 'package:github_analyzer/src/models/source_file.dart';\\nimport 'package:glob/glob.dart';\\n\\nString formatFileSize(int bytes) {\\n  if (bytes < 1024) return '$bytes B';\\n  if (bytes < 1024 * 1024) return '${(bytes / 1024).toStringAsFixed(2)} KB';\\n  if (bytes < 1024 * 1024 * 1024) {\\n    return '${(bytes / (1024 * 1024)).toStringAsFixed(2)} MB';\\n  }\\n  return '${(bytes / (1024 * 1024 * 1024)).toStringAsFixed(2)} GB';\\n}\\n\\nbool _matchesAnyPattern(String filePath, List<String> patterns) {\\n  final normalizedPath = path.normalize(filePath).replaceAll('\\\\\\\\', '/');\\n  for (final pattern in patterns) {\\n    if (Glob(pattern).matches(normalizedPath)) {\\n      return true;\\n    }\\n  }\\n  return false;\\n}\\n\\nbool shouldExclude(String filePath, List<String>? excludePatterns) {\\n  return _matchesAnyPattern(\\n      filePath, excludePatterns ?? kDefaultExcludePatterns);\\n}\\n\\nbool isBinaryFile(String filePath) {\\n  final extension = path.extension(filePath).toLowerCase().replaceAll('.', '');\\n  return kBinaryExtensions.contains(extension);\\n}\\n\\nbool isConfigurationFile(String filePath) {\\n  final fileName = path.basename(filePath).toLowerCase();\\n  final extension = path.extension(fileName).replaceAll('.', '');\\n\\n  if (kConfigurationFileNames.contains(fileName)) {\\n    return true;\\n  }\\n  return kConfigurationExtensions.contains(extension);\\n}\\n\\nbool isDocumentationFile(String filePath) {\\n  final fileNameWithExt = path.basename(filePath).toLowerCase();\\n  final extension = path.extension(fileNameWithExt).replaceAll('.', '');\\n  final fileNameWithoutExt = path.basenameWithoutExtension(fileNameWithExt);\\n\\n  if (kDocumentationFileNames.contains(fileNameWithoutExt) ||\\n      kDocumentationFileNames.contains(fileNameWithExt)) {\\n    return true;\\n  }\\n  return kDocumentationExtensions.contains(extension);\\n}\\n\\n// ## ìˆ˜ì •ëœ ë¶€ë¶„ ##\\n// kMainFilePatterns ìƒìˆ˜ë¥¼ ì‚¬ìš©í•˜ë„ë¡ ë³€ê²½\\nList<String> identifyMainFiles(List<SourceFile> files) {\\n  final mainFiles = <String>[];\\n  for (final file in files) {\\n    final fileName = path.basename(file.path);\\n    if (kMainFilePatterns.contains(fileName)) {\\n      mainFiles.add(file.path);\\n    }\\n  }\\n  return mainFiles;\\n}\\n\\nMap<String, List<String>> extractDependencies(List<SourceFile> files) {\\n  final dependencies = <String, List<String>>{};\\n  for (final file in files) {\\n    if (file.content == null) continue;\\n    final fileName = path.basename(file.path);\\n    List<String>? deps;\\n\\n    switch (fileName) {\\n      case 'package.json':\\n        deps = ['Node.js/npm'];\\n        break;\\n      case 'pubspec.yaml':\\n        deps = ['Dart/pub'];\\n        break;\\n      case 'requirements.txt':\\n      case 'setup.py':\\n        deps = ['Python/pip'];\\n        break;\\n      case 'Cargo.toml':\\n        deps = ['Rust/cargo'];\\n        break;\\n      case 'go.mod':\\n        deps = ['Go modules'];\\n        break;\\n      case 'pom.xml':\\n      case 'build.gradle':\\n        deps = ['Java/Maven or Gradle'];\\n        break;\\n      case 'Gemfile':\\n        deps = ['Ruby/bundler'];\\n        break;\\n      case 'composer.json':\\n        deps = ['PHP/composer'];\\n        break;\\n    }\\n\\n    if (deps != null) {\\n      dependencies[file.path] = deps;\\n    }\\n  }\\n  return dependencies;\\n}\\n\\nFuture<String?> readFileContent(File file, int maxFileSize) async {\\n  final stat = await file.stat();\\n  if (stat.size > maxFileSize) {\\n    return null;\\n  }\\n  try {\\n    return await file.readAsString();\\n  } catch (e) {\\n    return null;\\n  }\\n}\\n\",\"size\":3569,\"language\":\"Dart\",\"is_binary\":false,\"line_count\":127,\"is_source_code\":true,\"is_configuration\":false,\"is_documentation\":false,\"timestamp\":\"2025-10-15T06:50:04.324291\"},{\"path\":\"lib/src/common/utils/github_utils.dart\",\"content\":\"Map<String, String>? parseGitHubUrl(String url) {\\n  final patterns = [\\n    RegExp(r'https?://github\\\\.com/([^/]+)/([^/]+?)(?:\\\\.git)?/?$'),\\n    RegExp(r'git@github\\\\.com:([^/]+)/([^/]+?)(?:\\\\.git)?$'),\\n    RegExp(r'([^/]+)/([^/]+)$'),\\n  ];\\n\\n  for (final pattern in patterns) {\\n    final match = pattern.firstMatch(url);\\n    if (match != null) {\\n      return {\\n        'owner': match.group(1)!,\\n        'repo': match.group(2)!.replaceAll('.git', ''),\\n      };\\n    }\\n  }\\n\\n  return null;\\n}\\n\\nString normalizeGitHubUrl(String url) {\\n  final parsed = parseGitHubUrl(url);\\n  if (parsed == null) {\\n    throw ArgumentError('Invalid GitHub URL: $url');\\n  }\\n  return 'https://github.com/${parsed['owner']}/${parsed['repo']}';\\n}\\n\\nbool isValidGitHubUrl(String url) {\\n  return parseGitHubUrl(url) != null;\\n}\\n\",\"size\":790,\"language\":\"Dart\",\"is_binary\":false,\"line_count\":32,\"is_source_code\":true,\"is_configuration\":false,\"is_documentation\":false,\"timestamp\":\"2025-10-15T06:50:04.325279\"},{\"path\":\"lib/src/common/utils/markdown_generator.dart\",\"content\":\"import 'package:github_analyzer/src/models/analysis_result.dart';\\nimport 'package:github_analyzer/src/infrastructure/file_system/file_system.dart';\\n\\n/// Configuration for markdown generation\\nclass MarkdownConfig {\\n  final int? maxFiles;\\n  final int? maxContentSize;\\n  final bool includeBinaryStats;\\n  final bool includeErrors;\\n  final int? minPriority;\\n\\n  const MarkdownConfig({\\n    this.maxFiles,\\n    this.maxContentSize,\\n    this.includeBinaryStats = true,\\n    this.includeErrors = true,\\n    this.minPriority,\\n  });\\n\\n  static const standard = MarkdownConfig();\\n\\n  static const compact = MarkdownConfig(\\n    maxFiles: 50,\\n    maxContentSize: 50000,\\n    includeBinaryStats: false,\\n  );\\n}\\n\\n/// Generates markdown formatted output from analysis results\\nclass MarkdownGenerator {\\n  static final IFileSystem _fs = getFileSystem();\\n\\n  /// Generates and writes markdown directly to a file asynchronously\\n  static Future<void> generateToFile(\\n    AnalysisResult result,\\n    String outputPath, {\\n    MarkdownConfig config = MarkdownConfig.standard,\\n  }) async {\\n    final buffer = StringBuffer();\\n\\n    _writeHeaderSync(buffer, result);\\n    _writeMetadataSync(buffer, result);\\n    _writeStatisticsSync(buffer, result, config);\\n    _writeDirectoryTreeSync(buffer, result);\\n    _writeLanguageDistributionSync(buffer, result);\\n    _writeMainFilesSync(buffer, result);\\n    _writeDependenciesSync(buffer, result);\\n\\n    if (config.includeErrors) {\\n      _writeErrorsSync(buffer, result);\\n    }\\n\\n    _writeSourceCodeSync(buffer, result, config);\\n\\n    await _fs.writeFile(outputPath, buffer.toString());\\n  }\\n\\n  /// Generates markdown string synchronously\\n  static String generate(\\n    AnalysisResult result, {\\n    MarkdownConfig config = MarkdownConfig.standard,\\n  }) {\\n    final buffer = StringBuffer();\\n\\n    _writeHeaderSync(buffer, result);\\n    _writeMetadataSync(buffer, result);\\n    _writeStatisticsSync(buffer, result, config);\\n    _writeDirectoryTreeSync(buffer, result);\\n    _writeLanguageDistributionSync(buffer, result);\\n    _writeMainFilesSync(buffer, result);\\n    _writeDependenciesSync(buffer, result);\\n\\n    if (config.includeErrors) {\\n      _writeErrorsSync(buffer, result);\\n    }\\n\\n    _writeSourceCodeSync(buffer, result, config);\\n\\n    return buffer.toString();\\n  }\\n\\n  // Synchronous helpers for building markdown content\\n\\n  static void _writeHeaderSync(StringBuffer buffer, AnalysisResult result) {\\n    buffer.writeln('# ${result.metadata.name}');\\n    buffer.writeln();\\n\\n    if (result.metadata.description != null &&\\n        result.metadata.description!.isNotEmpty) {\\n      buffer.writeln(result.metadata.description);\\n      buffer.writeln();\\n    }\\n  }\\n\\n  static void _writeMetadataSync(StringBuffer buffer, AnalysisResult result) {\\n    buffer.writeln('## Repository Information');\\n    buffer.writeln();\\n\\n    final meta = result.metadata;\\n    if (meta.fullName != null) {\\n      buffer.writeln('**Repository:** `${meta.fullName}`');\\n    }\\n\\n    final info = <String>[];\\n    if (meta.language != null) info.add('**Language:** ${meta.language}');\\n    if (meta.stars > 0) info.add('**Stars:** ${meta.stars}');\\n    if (meta.forks > 0) info.add('**Forks:** ${meta.forks}');\\n\\n    if (info.isNotEmpty) {\\n      buffer.writeln(info.join(' | '));\\n    }\\n\\n    buffer.writeln();\\n  }\\n\\n  static void _writeStatisticsSync(\\n    StringBuffer buffer,\\n    AnalysisResult result,\\n    MarkdownConfig config,\\n  ) {\\n    buffer.writeln('## Statistics');\\n    buffer.writeln();\\n\\n    final stats = result.statistics;\\n    buffer.writeln('- **Total Files:** ${stats.totalFiles}');\\n    buffer.writeln('- **Total Lines:** ${stats.totalLines}');\\n    buffer.writeln('- **Total Size:** ${_formatBytes(stats.totalSize)}');\\n    buffer.writeln('- **Source Files:** ${stats.sourceFiles}');\\n\\n    if (config.includeBinaryStats && stats.binaryFiles > 0) {\\n      buffer.writeln('- **Binary Files:** ${stats.binaryFiles}');\\n    }\\n\\n    buffer.writeln();\\n  }\\n\\n  static void _writeDirectoryTreeSync(\\n    StringBuffer buffer,\\n    AnalysisResult result,\\n  ) {\\n    if (result.metadata.directoryTree.isEmpty) return;\\n\\n    buffer.writeln('## Directory Structure');\\n    buffer.writeln();\\n    buffer.writeln('```');\\n    buffer.writeln(result.metadata.directoryTree);\\n    buffer.writeln('```');\\n    buffer.writeln();\\n  }\\n\\n  static void _writeLanguageDistributionSync(\\n    StringBuffer buffer,\\n    AnalysisResult result,\\n  ) {\\n    if (result.statistics.languageDistribution.isEmpty) return;\\n\\n    buffer.writeln('## Language Distribution');\\n    buffer.writeln();\\n\\n    final sorted = result.statistics.languageDistribution.entries.toList()\\n      ..sort((a, b) => b.value.compareTo(a.value));\\n\\n    for (final entry in sorted.take(10)) {\\n      final percentage =\\n          (entry.value / result.statistics.totalFiles * 100).toStringAsFixed(1);\\n      buffer.writeln('- **${entry.key}:** ${entry.value} files ($percentage%)');\\n    }\\n\\n    buffer.writeln();\\n  }\\n\\n  static void _writeMainFilesSync(StringBuffer buffer, AnalysisResult result) {\\n    if (result.mainFiles.isEmpty) return;\\n\\n    buffer.writeln('## Main Entry Points');\\n    buffer.writeln();\\n\\n    for (final file in result.mainFiles) {\\n      buffer.writeln('- `$file`');\\n    }\\n\\n    buffer.writeln();\\n  }\\n\\n  static void _writeDependenciesSync(\\n    StringBuffer buffer,\\n    AnalysisResult result,\\n  ) {\\n    if (result.dependencies.isEmpty) return;\\n\\n    buffer.writeln('## Dependencies');\\n    buffer.writeln();\\n\\n    for (final entry in result.dependencies.entries) {\\n      if (entry.value.isEmpty) continue;\\n\\n      buffer.writeln('**${entry.key}:**');\\n      for (final dep in entry.value) {\\n        buffer.writeln('- $dep');\\n      }\\n      buffer.writeln();\\n    }\\n  }\\n\\n  static void _writeErrorsSync(StringBuffer buffer, AnalysisResult result) {\\n    if (result.errors.isEmpty) return;\\n\\n    buffer.writeln('## Analysis Errors');\\n    buffer.writeln();\\n\\n    for (final error in result.errors) {\\n      buffer.writeln('- **${error.path}:** ${error.message}');\\n    }\\n\\n    buffer.writeln();\\n  }\\n\\n  static void _writeSourceCodeSync(\\n    StringBuffer buffer,\\n    AnalysisResult result,\\n    MarkdownConfig config,\\n  ) {\\n    buffer.writeln('## Source Code');\\n    buffer.writeln();\\n\\n    var sourceFiles = result.files\\n        .where(\\n            (f) => f.isSourceCode && f.content != null && f.content!.isNotEmpty)\\n        .toList();\\n\\n    if (config.minPriority != null) {\\n      sourceFiles = sourceFiles.where((f) {\\n        if (f.path.startsWith('lib/')) return true;\\n        if (f.path.contains('main.dart')) return true;\\n        return false;\\n      }).toList();\\n    }\\n\\n    if (config.maxFiles != null && sourceFiles.length > config.maxFiles!) {\\n      sourceFiles = sourceFiles.take(config.maxFiles!).toList();\\n      buffer.writeln(\\n          '> **Note:** Showing ${config.maxFiles} of ${result.files.length} files');\\n      buffer.writeln();\\n    }\\n\\n    for (final file in sourceFiles) {\\n      buffer.writeln('### ${file.path}');\\n      buffer.writeln();\\n\\n      var content = file.content!;\\n\\n      if (config.maxContentSize != null &&\\n          content.length > config.maxContentSize!) {\\n        content = content.substring(0, config.maxContentSize!);\\n        content += '\\\\n\\\\n// ... truncated ...';\\n      }\\n\\n      final language = file.language ?? '';\\n      buffer.writeln('```$language');\\n      buffer.writeln(content);\\n      buffer.writeln('```');\\n      buffer.writeln();\\n    }\\n  }\\n\\n  static String _formatBytes(int bytes) {\\n    if (bytes < 1024) return '$bytes B';\\n    if (bytes < 1024 * 1024) return '${(bytes / 1024).toStringAsFixed(1)} KB';\\n    if (bytes < 1024 * 1024 * 1024) {\\n      return '${(bytes / (1024 * 1024)).toStringAsFixed(1)} MB';\\n    }\\n    return '${(bytes / (1024 * 1024 * 1024)).toStringAsFixed(1)} GB';\\n  }\\n}\\n\",\"size\":7710,\"language\":\"Dart\",\"is_binary\":false,\"line_count\":275,\"is_source_code\":true,\"is_configuration\":false,\"is_documentation\":false,\"timestamp\":\"2025-10-15T06:50:04.325785\"},{\"path\":\"lib/src/common/utils/metadata_generator.dart\",\"content\":\"import 'dart:io';\\nimport 'dart:convert';\\nimport 'package:github_analyzer/src/models/analysis_result.dart';\\n\\nclass MetadataGenerator {\\n  static Future<void> generate(AnalysisResult result, String outputPath) async {\\n    final metadata = {\\n      'repository': {\\n        'name': result.metadata.name,\\n        'full_name': result.metadata.fullName,\\n        'description': result.metadata.description,\\n        'is_private': result.metadata.isPrivate,\\n        'default_branch': result.metadata.defaultBranch,\\n        'language': result.metadata.language,\\n        'languages': result.metadata.languages,\\n        'stars': result.metadata.stars,\\n        'forks': result.metadata.forks,\\n      },\\n      'statistics': {\\n        'total_files': result.statistics.totalFiles,\\n        'total_lines': result.statistics.totalLines,\\n        'total_size': result.statistics.totalSize,\\n        'binary_files': result.statistics.binaryFiles,\\n        'source_files': result.statistics.sourceFiles,\\n        'config_files': result.statistics.configFiles,\\n        'documentation_files': result.statistics.documentationFiles,\\n        'language_distribution': result.statistics.languageDistribution,\\n      },\\n      'main_files': result.mainFiles,\\n      'dependencies': result.dependencies,\\n      'errors': result.errors.map((e) => e.toJson()).toList(),\\n      'generated_at': DateTime.now().toIso8601String(),\\n    };\\n\\n    final file = File(outputPath);\\n    await file.writeAsString(\\n      const JsonEncoder.withIndent('  ').convert(metadata),\\n    );\\n  }\\n\\n  static Map<String, dynamic> generateMap(AnalysisResult result) {\\n    return {\\n      'repository': {\\n        'name': result.metadata.name,\\n        'full_name': result.metadata.fullName,\\n        'description': result.metadata.description,\\n        'is_private': result.metadata.isPrivate,\\n        'default_branch': result.metadata.defaultBranch,\\n        'language': result.metadata.language,\\n        'languages': result.metadata.languages,\\n        'stars': result.metadata.stars,\\n        'forks': result.metadata.forks,\\n      },\\n      'statistics': {\\n        'total_files': result.statistics.totalFiles,\\n        'total_lines': result.statistics.totalLines,\\n        'total_size': result.statistics.totalSize,\\n        'binary_files': result.statistics.binaryFiles,\\n        'source_files': result.statistics.sourceFiles,\\n        'config_files': result.statistics.configFiles,\\n        'documentation_files': result.statistics.documentationFiles,\\n        'language_distribution': result.statistics.languageDistribution,\\n      },\\n      'main_files': result.mainFiles,\\n      'dependencies': result.dependencies,\\n      'errors': result.errors.map((e) => e.toJson()).toList(),\\n      'generated_at': DateTime.now().toIso8601String(),\\n    };\\n  }\\n}\\n\",\"size\":2762,\"language\":\"Dart\",\"is_binary\":false,\"line_count\":71,\"is_source_code\":true,\"is_configuration\":false,\"is_documentation\":false,\"timestamp\":\"2025-10-15T06:50:04.326494\"},{\"path\":\"lib/src/common/utils/pattern_matcher.dart\",\"content\":\"/// Pattern matching utility to replace glob package for web compatibility\\nclass PatternMatcher {\\n  /// Check if a path matches any of the given patterns\\n  static bool matchesAny(String path, List<String> patterns) {\\n    for (final pattern in patterns) {\\n      if (matches(path, pattern)) {\\n        return true;\\n      }\\n    }\\n    return false;\\n  }\\n\\n  /// Check if a path matches a single pattern\\n  static bool matches(String path, String pattern) {\\n    final regexPattern = _convertGlobToRegex(pattern);\\n    return regexPattern.hasMatch(path);\\n  }\\n\\n  /// Convert glob pattern to RegExp\\n  static RegExp _convertGlobToRegex(String pattern) {\\n    var regexStr = pattern;\\n\\n    // Escape special regex characters except glob wildcards\\n    regexStr = regexStr.replaceAllMapped(\\n      RegExp(r'[.+^${}()|[\\\\]\\\\\\\\]'),\\n      (match) => '\\\\\\\\${match.group(0)}',\\n    );\\n\\n    // Convert glob wildcards to regex\\n    regexStr = regexStr.replaceAll('**/', '(?:.*/)');\\n    regexStr = regexStr.replaceAll('**', '.*');\\n    regexStr = regexStr.replaceAll('*', '[^/]*');\\n    regexStr = regexStr.replaceAll('?', '[^/]');\\n\\n    // Anchor the pattern\\n    regexStr = '^$regexStr\\\\$';\\n\\n    return RegExp(regexStr);\\n  }\\n\\n  /// Check if a path should be excluded based on patterns\\n  static bool shouldExclude(String path, List<String> excludePatterns) {\\n    return matchesAny(path, excludePatterns);\\n  }\\n\\n  /// Check if a path should be included based on patterns\\n  static bool shouldInclude(String path, List<String> includePatterns) {\\n    if (includePatterns.isEmpty) return true;\\n    return matchesAny(path, includePatterns);\\n  }\\n\\n  /// Filter a list of paths based on include/exclude patterns\\n  static List<String> filterPaths(\\n    List<String> paths,\\n    List<String> includePatterns,\\n    List<String> excludePatterns,\\n  ) {\\n    return paths.where((path) {\\n      if (shouldExclude(path, excludePatterns)) return false;\\n      return shouldInclude(path, includePatterns);\\n    }).toList();\\n  }\\n}\\n\",\"size\":1964,\"language\":\"Dart\",\"is_binary\":false,\"line_count\":64,\"is_source_code\":true,\"is_configuration\":false,\"is_documentation\":false,\"timestamp\":\"2025-10-15T06:50:04.326947\"},{\"path\":\"lib/src/core/cache_service.dart\",\"content\":\"import 'dart:io';\\nimport 'dart:convert';\\nimport 'package:crypto/crypto.dart';\\nimport 'package:github_analyzer/src/common/logger.dart';\\nimport 'package:github_analyzer/src/common/errors/analyzer_exception.dart';\\nimport 'package:github_analyzer/src/models/analysis_result.dart';\\n\\n/// Manages caching of analysis results to avoid redundant computations.\\nclass CacheService {\\n  final String cacheDirectory;\\n  final Duration? maxAge;\\n  bool _isInitialized = false;\\n\\n  /// Creates an instance of [CacheService].\\n  CacheService({\\n    required this.cacheDirectory,\\n    this.maxAge,\\n  });\\n\\n  /// Initializes the cache service by creating the cache directory if it\\n  /// doesn't exist.\\n  Future<void> initialize() async {\\n    if (_isInitialized) return;\\n    final dir = Directory(cacheDirectory);\\n    if (!await dir.exists()) {\\n      await dir.create(recursive: true);\\n      logger.info('Cache directory created: $cacheDirectory');\\n    }\\n    _isInitialized = true;\\n  }\\n\\n  String _generateCacheKey(String repositoryUrl, String commitHash) {\\n    final input = '$repositoryUrl@$commitHash';\\n    return sha256.convert(utf8.encode(input)).toString();\\n  }\\n\\n  /// Retrieves a cached [AnalysisResult] if available and not expired.\\n  Future<AnalysisResult?> get(String repositoryUrl, String commitHash) async {\\n    if (!_isInitialized) {\\n      throw AnalyzerException(\\n        'CacheService not initialized',\\n        code: AnalyzerErrorCode.cacheError,\\n      );\\n    }\\n\\n    final key = _generateCacheKey(repositoryUrl, commitHash);\\n    final cacheFile = File('$cacheDirectory/$key.json');\\n\\n    if (!await cacheFile.exists()) {\\n      logger.fine('Cache miss for $repositoryUrl (commit: $commitHash)');\\n      return null;\\n    }\\n\\n    if (maxAge != null) {\\n      final stat = await cacheFile.stat();\\n      final age = DateTime.now().difference(stat.modified);\\n      if (age > maxAge!) {\\n        logger.info('Cache expired for $repositoryUrl. Deleting.');\\n        await delete(repositoryUrl, commitHash);\\n        return null;\\n      }\\n    }\\n\\n    try {\\n      final content = await cacheFile.readAsString();\\n      final json = jsonDecode(content) as Map<String, dynamic>;\\n      logger.info('Cache hit for $repositoryUrl (commit: $commitHash)');\\n      return AnalysisResult.fromJson(json);\\n    } catch (e, stackTrace) {\\n      logger.warning(\\n        'Failed to read or parse cache file for $key. Deleting. Error: $e',\\n        e,\\n        stackTrace,\\n      );\\n      await delete(repositoryUrl, commitHash);\\n      return null;\\n    }\\n  }\\n\\n  /// Saves an [AnalysisResult] to the cache.\\n  Future<void> set(\\n    String repositoryUrl,\\n    String commitHash,\\n    AnalysisResult result,\\n  ) async {\\n    if (!_isInitialized) {\\n      throw AnalyzerException(\\n        'CacheService not initialized',\\n        code: AnalyzerErrorCode.cacheError,\\n      );\\n    }\\n\\n    final key = _generateCacheKey(repositoryUrl, commitHash);\\n    final cacheFile = File('$cacheDirectory/$key.json');\\n\\n    try {\\n      final json = jsonEncode(result.toJson());\\n      await cacheFile.writeAsString(json);\\n      logger.info('Saved cache for $repositoryUrl (commit: $commitHash)');\\n    } catch (e, stackTrace) {\\n      logger.severe('Failed to write cache for $key', e, stackTrace);\\n      throw AnalyzerException(\\n        'Failed to write to cache',\\n        code: AnalyzerErrorCode.cacheError,\\n        details: e.toString(),\\n      );\\n    }\\n  }\\n\\n  /// Deletes a specific entry from the cache.\\n  Future<void> delete(String repositoryUrl, String commitHash) async {\\n    final key = _generateCacheKey(repositoryUrl, commitHash);\\n    final cacheFile = File('$cacheDirectory/$key.json');\\n    if (await cacheFile.exists()) {\\n      await cacheFile.delete();\\n      logger.info('Deleted cache for $key');\\n    }\\n  }\\n\\n  /// Clears the entire cache directory.\\n  Future<void> clear() async {\\n    final dir = Directory(cacheDirectory);\\n    if (await dir.exists()) {\\n      await for (final entity in dir.list()) {\\n        await entity.delete(recursive: true);\\n      }\\n      logger.info('Cache directory cleared.');\\n    }\\n  }\\n\\n  /// Gets statistics about the cache, such as total files and size.\\n  Future<Map<String, dynamic>> getStatistics() async {\\n    final dir = Directory(cacheDirectory);\\n    if (!await dir.exists()) {\\n      return {'totalFiles': 0, 'totalSize': 0};\\n    }\\n\\n    int totalFiles = 0;\\n    int totalSize = 0;\\n    await for (final entity in dir.list()) {\\n      if (entity is File) {\\n        totalFiles++;\\n        totalSize += await entity.length();\\n      }\\n    }\\n    return {'totalFiles': totalFiles, 'totalSize': totalSize};\\n  }\\n}\\n\",\"size\":4563,\"language\":\"Dart\",\"is_binary\":false,\"line_count\":149,\"is_source_code\":true,\"is_configuration\":false,\"is_documentation\":false,\"timestamp\":\"2025-10-15T06:50:04.327369\"},{\"path\":\"lib/src/core/incremental_analyzer.dart\",\"content\":\"import 'dart:io';\\nimport 'package:github_analyzer/src/common/config.dart';\\nimport 'package:github_analyzer/src/common/logger.dart';\\nimport 'package:github_analyzer/src/models/analysis_result.dart';\\nimport 'package:github_analyzer/src/models/source_file.dart';\\nimport 'package:github_analyzer/src/models/analysis_statistics.dart';\\nimport 'package:github_analyzer/src/common/utils/file_utils.dart';\\nimport 'package:path/path.dart' as path;\\nimport 'package:github_analyzer/src/common/language_info.dart';\\n\\n/// Represents the changes detected between two analysis runs.\\nclass FileChange {\\n  final List<String> added;\\n  final List<String> modified;\\n  final List<String> deleted;\\n\\n  /// Creates an instance of [FileChange].\\n  FileChange({\\n    required this.added,\\n    required this.modified,\\n    required this.deleted,\\n  });\\n\\n  /// Returns true if no changes were detected.\\n  bool get isEmpty => added.isEmpty && modified.isEmpty && deleted.isEmpty;\\n\\n  /// The total number of changed files.\\n  int get length => added.length + modified.length + deleted.length;\\n}\\n\\n/// Performs an incremental analysis by comparing the current state of a\\n/// repository with a previous analysis result.\\nclass IncrementalAnalyzer {\\n  final GithubAnalyzerConfig config;\\n\\n  /// Creates an instance of [IncrementalAnalyzer].\\n  IncrementalAnalyzer({required this.config});\\n\\n  /// Analyzes a local directory based on a previous analysis result.\\n  Future<AnalysisResult> analyze(\\n    String directoryPath, {\\n    required AnalysisResult previousResult,\\n  }) async {\\n    logger.info('Starting incremental analysis: $directoryPath');\\n\\n    final changes = await _detectChanges(directoryPath, previousResult);\\n\\n    if (changes.isEmpty) {\\n      logger.info('No changes detected, returning previous result');\\n      return previousResult;\\n    }\\n\\n    logger.info('Changes detected: ${changes.length} files');\\n    logger.fine(\\n      'Added: ${changes.added.length}, Modified: ${changes.modified.length}, Deleted: ${changes.deleted.length}',\\n    );\\n\\n    return await _analyzeChanges(directoryPath, previousResult, changes);\\n  }\\n\\n  Future<AnalysisResult> _analyzeChanges(\\n    String directoryPath,\\n    AnalysisResult previousResult,\\n    FileChange changes,\\n  ) async {\\n    final fileMap = {for (var f in previousResult.files) f.path: f};\\n\\n    for (final changedPath in [...changes.added, ...changes.modified]) {\\n      final file = File(path.join(directoryPath, changedPath));\\n      if (!await file.exists()) continue;\\n\\n      try {\\n        final analyzed = await _analyzeFile(file, changedPath);\\n        if (analyzed != null) {\\n          fileMap[changedPath] = analyzed;\\n        }\\n      } catch (e, stackTrace) {\\n        logger.warning('Failed to analyze file $changedPath', e, stackTrace);\\n      }\\n    }\\n\\n    for (final deletedPath in changes.deleted) {\\n      fileMap.remove(deletedPath);\\n    }\\n\\n    final allFiles = fileMap.values.toList();\\n    final statistics = AnalysisStatistics.fromSourceFiles(allFiles);\\n\\n    final primaryLanguage = statistics.languageDistribution.isEmpty\\n        ? null\\n        : statistics.languageDistribution.entries\\n            .reduce((a, b) => a.value > b.value ? a : b)\\n            .key;\\n\\n    final updatedMetadata = previousResult.metadata.copyWith(\\n      language: primaryLanguage,\\n      languages: statistics.languageDistribution.keys.toList(),\\n      fileCount: allFiles.length,\\n    );\\n\\n    return AnalysisResult(\\n      metadata: updatedMetadata,\\n      files: allFiles,\\n      statistics: statistics,\\n      mainFiles: identifyMainFiles(allFiles),\\n      dependencies: extractDependencies(allFiles),\\n      errors:\\n          previousResult.errors, // Note: errors from previous run are preserved\\n    );\\n  }\\n\\n  Future<SourceFile?> _analyzeFile(File file, String relativePath) async {\\n    final stat = await file.stat();\\n    if (stat.size > config.maxFileSize) {\\n      logger.finer('Skipping large file in incremental scan: $relativePath');\\n      return null;\\n    }\\n\\n    final isBinary = isBinaryFile(relativePath);\\n    String? content;\\n    int lineCount = 0;\\n\\n    if (!isBinary) {\\n      try {\\n        content = await file.readAsString();\\n        lineCount = content.split('\\\\n').length;\\n      } catch (e) {\\n        logger.finer(\\n          'Failed to read file as text in incremental scan: $relativePath, error: $e',\\n        );\\n        // If reading as text fails, treat it as a binary file for statistics.\\n        return SourceFile(\\n          path: relativePath,\\n          content: null,\\n          size: stat.size,\\n          language: null,\\n          isBinary: true,\\n          lineCount: 0,\\n          isSourceCode: false,\\n          isConfiguration: isConfigurationFile(relativePath),\\n          isDocumentation: isDocumentationFile(relativePath),\\n          timestamp: stat.modified,\\n        );\\n      }\\n    }\\n\\n    final language = detectLanguage(relativePath);\\n\\n    return SourceFile(\\n      path: relativePath,\\n      content: content,\\n      size: stat.size,\\n      language: language,\\n      isBinary: isBinary,\\n      lineCount: lineCount,\\n      isSourceCode: language != null && !isBinary,\\n      isConfiguration: isConfigurationFile(relativePath),\\n      isDocumentation: isDocumentationFile(relativePath),\\n      timestamp: stat.modified,\\n    );\\n  }\\n\\n  Future<FileChange> _detectChanges(\\n    String directoryPath,\\n    AnalysisResult previousResult,\\n  ) async {\\n    final added = <String>[];\\n    final modified = <String>[];\\n\\n    final dir = Directory(directoryPath);\\n    if (!await dir.exists()) {\\n      throw Exception('Directory not found: $directoryPath');\\n    }\\n\\n    final previousFilesMap = {for (var f in previousResult.files) f.path: f};\\n    final currentFilePaths = <String>{};\\n\\n    await for (final entity in dir.list(recursive: true)) {\\n      if (entity is File) {\\n        final relativePath = path.relative(entity.path, from: directoryPath);\\n\\n        if (shouldExclude(relativePath, config.excludePatterns)) {\\n          continue;\\n        }\\n\\n        currentFilePaths.add(relativePath);\\n\\n        final previousFile = previousFilesMap[relativePath];\\n        final stat = await entity.stat();\\n\\n        if (previousFile == null) {\\n          added.add(relativePath);\\n        } else if (stat.modified.isAfter(previousFile.timestamp) ||\\n            stat.size != previousFile.size) {\\n          modified.add(relativePath);\\n        }\\n      }\\n    }\\n\\n    final deleted =\\n        previousFilesMap.keys.toSet().difference(currentFilePaths).toList();\\n\\n    return FileChange(added: added, modified: modified, deleted: deleted);\\n  }\\n}\\n\",\"size\":6510,\"language\":\"Dart\",\"is_binary\":false,\"line_count\":206,\"is_source_code\":true,\"is_configuration\":false,\"is_documentation\":false,\"timestamp\":\"2025-10-15T06:50:04.327860\"},{\"path\":\"lib/src/core/local_analyzer_service.dart\",\"content\":\"import 'package:github_analyzer/src/common/config.dart';\\nimport 'package:github_analyzer/src/common/logger.dart';\\nimport 'package:github_analyzer/src/core/repository_analyzer.dart';\\nimport 'package:github_analyzer/src/common/utils/directory_tree_generator.dart';\\nimport 'package:github_analyzer/src/common/utils/file_utils.dart';\\nimport 'package:github_analyzer/src/models/analysis_result.dart';\\nimport 'package:github_analyzer/src/models/repository_metadata.dart';\\nimport 'package:github_analyzer/src/models/analysis_statistics.dart';\\nimport 'package:github_analyzer/src/core/incremental_analyzer.dart';\\nimport 'package:path/path.dart' as path;\\n\\n/// Service responsible for analyzing local file directories.\\n///\\n/// It can perform a full analysis or an incremental analysis if a previous\\n/// result is provided.\\nclass LocalAnalyzerService {\\n  final GithubAnalyzerConfig config;\\n  final RepositoryAnalyzer _repositoryAnalyzer;\\n  final IncrementalAnalyzer _incrementalAnalyzer;\\n\\n  /// Creates an instance of [LocalAnalyzerService].\\n  LocalAnalyzerService({\\n    required this.config,\\n    RepositoryAnalyzer? repositoryAnalyzer,\\n  })  : _repositoryAnalyzer =\\n            repositoryAnalyzer ?? RepositoryAnalyzer(config: config),\\n        _incrementalAnalyzer = IncrementalAnalyzer(config: config);\\n\\n  /// Analyzes a local directory.\\n  ///\\n  /// If [previousResult] is provided, it performs an incremental analysis\\n  /// by comparing the current state with the previous one. Otherwise, it\\n  /// performs a full analysis.\\n  Future<AnalysisResult> analyze(\\n    String directoryPath, {\\n    AnalysisResult? previousResult,\\n  }) async {\\n    logger.info('Starting local analysis: $directoryPath');\\n\\n    if (previousResult != null) {\\n      logger.info(\\n        'Previous analysis result found, performing incremental analysis.',\\n      );\\n      try {\\n        // Note: Incremental analysis will re-use the directory tree from the\\n        // previous result for now. A more sophisticated approach could update\\n        // the tree based on changes, but that adds significant complexity.\\n        final result = await _incrementalAnalyzer.analyze(\\n          directoryPath,\\n          previousResult: previousResult,\\n        );\\n        logger.info(\\n          'Incremental analysis completed: ${result.files.length} files analyzed',\\n        );\\n        return result;\\n      } catch (e, stackTrace) {\\n        logger.warning(\\n          'Incremental analysis failed. Performing full analysis instead.',\\n          e,\\n          stackTrace,\\n        );\\n      }\\n    }\\n\\n    logger.info('Performing full analysis.');\\n    final files = await _repositoryAnalyzer.analyzeDirectory(directoryPath);\\n    final statistics = AnalysisStatistics.fromSourceFiles(files);\\n    final filePaths = files.map((f) => f.path).toList();\\n    final directoryTree = DirectoryTreeGenerator.generate(filePaths);\\n\\n    final primaryLanguage = statistics.languageDistribution.isEmpty\\n        ? null\\n        : statistics.languageDistribution.entries\\n            .reduce((a, b) => a.value > b.value ? a : b)\\n            .key;\\n\\n    final metadata = RepositoryMetadata(\\n      name: path.basename(directoryPath),\\n      fullName: path.basename(directoryPath),\\n      description: 'Local repository analysis',\\n      isPrivate: false,\\n      defaultBranch: null,\\n      language: primaryLanguage,\\n      languages: statistics.languageDistribution.keys.toList(),\\n      stars: 0,\\n      forks: 0,\\n      fileCount: files.length,\\n      commitSha: null,\\n      directoryTree: directoryTree,\\n    );\\n\\n    final mainFiles = identifyMainFiles(files);\\n    final dependencies = extractDependencies(files);\\n    final errors = _repositoryAnalyzer.getErrors();\\n\\n    logger.info(\\n      'Full local analysis completed: ${files.length} files analyzed',\\n    );\\n\\n    return AnalysisResult(\\n      metadata: metadata,\\n      files: files,\\n      statistics: statistics,\\n      mainFiles: mainFiles,\\n      dependencies: dependencies,\\n      errors: errors,\\n    );\\n  }\\n}\\n\",\"size\":3970,\"language\":\"Dart\",\"is_binary\":false,\"line_count\":110,\"is_source_code\":true,\"is_configuration\":false,\"is_documentation\":false,\"timestamp\":\"2025-10-15T06:50:04.328279\"},{\"path\":\"lib/src/core/remote_analyzer_service.dart\",\"content\":\"import 'dart:async';\\nimport 'package:archive/archive.dart';\\nimport 'package:github_analyzer/src/common/config.dart';\\nimport 'package:github_analyzer/src/common/logger.dart';\\nimport 'package:github_analyzer/src/common/utils/directory_tree_generator.dart';\\nimport 'package:github_analyzer/src/common/utils/file_utils.dart';\\nimport 'package:github_analyzer/src/common/utils/github_utils.dart';\\nimport 'package:github_analyzer/src/common/errors/analyzer_exception.dart';\\nimport 'package:github_analyzer/src/infrastructure/interfaces/i_github_api_provider.dart';\\nimport 'package:github_analyzer/src/data/providers/zip_downloader.dart';\\nimport 'package:github_analyzer/src/core/cache_service.dart';\\nimport 'package:github_analyzer/src/core/repository_analyzer.dart';\\nimport 'package:github_analyzer/src/models/analysis_result.dart';\\nimport 'package:github_analyzer/src/models/analysis_statistics.dart';\\nimport 'package:github_analyzer/src/models/analysis_progress.dart';\\n\\n/// Service responsible for analyzing remote GitHub repositories.\\n///\\n/// It handles fetching repository metadata, downloading the archive,\\n/// analyzing the contents, and managing the cache.\\nclass RemoteAnalyzerService {\\n  final GithubAnalyzerConfig config;\\n  final IGithubApiProvider apiProvider;\\n  final ZipDownloader zipDownloader;\\n  final CacheService? cacheService;\\n  final StreamController<AnalysisProgress>? progressController;\\n\\n  /// Creates an instance of [RemoteAnalyzerService].\\n  RemoteAnalyzerService({\\n    required this.config,\\n    required this.apiProvider,\\n    required this.zipDownloader,\\n    this.cacheService,\\n    this.progressController,\\n  });\\n\\n  /// Creates a copy of this service with the given fields replaced.\\n  /// This is useful for modifying the service's behavior, such as providing\\n  /// a progress controller for a specific analysis run.\\n  RemoteAnalyzerService copyWith({\\n    GithubAnalyzerConfig? config,\\n    IGithubApiProvider? apiProvider,\\n    ZipDownloader? zipDownloader,\\n    CacheService? cacheService,\\n    StreamController<AnalysisProgress>? progressController,\\n  }) {\\n    return RemoteAnalyzerService(\\n      config: config ?? this.config,\\n      apiProvider: apiProvider ?? this.apiProvider,\\n      zipDownloader: zipDownloader ?? this.zipDownloader,\\n      cacheService: cacheService ?? this.cacheService,\\n      progressController: progressController ?? this.progressController,\\n    );\\n  }\\n\\n  /// Analyzes a remote repository.\\n  Future<AnalysisResult> analyze({\\n    required String repositoryUrl,\\n    String? branch,\\n    bool useCache = true,\\n  }) async {\\n    logger.info('Starting remote analysis: $repositoryUrl');\\n\\n    _emitProgress(\\n      AnalysisProgress(\\n        phase: AnalysisPhase.initializing,\\n        progress: 0.0,\\n        message: 'Initializing analysis',\\n        timestamp: DateTime.now(),\\n      ),\\n    );\\n\\n    final parsedUrl = parseGitHubUrl(repositoryUrl);\\n    if (parsedUrl == null) {\\n      throw AnalyzerException(\\n        'Invalid GitHub URL: $repositoryUrl',\\n        code: AnalyzerErrorCode.invalidUrl,\\n        details: 'Expected format: https://github.com/owner/repo',\\n      );\\n    }\\n\\n    final owner = parsedUrl['owner']!;\\n    final repo = parsedUrl['repo']!;\\n\\n    try {\\n      _emitProgress(\\n        AnalysisProgress(\\n          phase: AnalysisPhase.initializing,\\n          progress: 0.1,\\n          message: 'Fetching repository metadata',\\n          timestamp: DateTime.now(),\\n        ),\\n      );\\n\\n      final metadata = await apiProvider.getRepositoryMetadata(owner, repo);\\n      final targetBranch = branch ?? metadata.defaultBranch ?? 'main';\\n      final commitSha = metadata.commitSha;\\n\\n      if (commitSha == null) {\\n        logger.warning(\\n          'Could not determine commit SHA. Caching will be disabled.',\\n        );\\n        useCache = false;\\n      }\\n\\n      if (useCache && cacheService != null && commitSha != null) {\\n        _emitProgress(\\n          AnalysisProgress(\\n            phase: AnalysisPhase.initializing,\\n            progress: 0.2,\\n            message: 'Checking cache',\\n            timestamp: DateTime.now(),\\n          ),\\n        );\\n\\n        final cached = await cacheService!.get(repositoryUrl, commitSha);\\n        if (cached != null) {\\n          logger.info('Using cached result');\\n          _emitProgress(\\n            AnalysisProgress(\\n              phase: AnalysisPhase.completed,\\n              progress: 1.0,\\n              message: 'Loaded from cache',\\n              timestamp: DateTime.now(),\\n            ),\\n          );\\n          return cached;\\n        }\\n      }\\n\\n      logger.info('Downloading repository as ZIP (branch: $targetBranch)');\\n      _emitProgress(\\n        AnalysisProgress(\\n          phase: AnalysisPhase.downloading,\\n          progress: 0.3,\\n          message: 'Downloading repository archive',\\n          timestamp: DateTime.now(),\\n        ),\\n      );\\n\\n      final zipBytes = await zipDownloader.downloadRepositoryAsBytes(\\n        owner: owner,\\n        repo: repo,\\n        branch: targetBranch,\\n        token: config.githubToken,\\n      );\\n\\n      _emitProgress(\\n        AnalysisProgress(\\n          phase: AnalysisPhase.extracting,\\n          progress: 0.5,\\n          message: 'Extracting archive',\\n          timestamp: DateTime.now(),\\n        ),\\n      );\\n\\n      logger.info('Archive downloaded, analyzing from memory...');\\n      final archive = ZipDecoder().decodeBytes(zipBytes);\\n\\n      _emitProgress(\\n        AnalysisProgress(\\n          phase: AnalysisPhase.analyzing,\\n          progress: 0.6,\\n          message: 'Analyzing files',\\n          timestamp: DateTime.now(),\\n        ),\\n      );\\n\\n      final repositoryAnalyzer = RepositoryAnalyzer(\\n        config: config,\\n      );\\n\\n      final files = await repositoryAnalyzer.analyzeArchive(archive);\\n      final filePaths = files.map((f) => f.path).toList();\\n      final directoryTree = DirectoryTreeGenerator.generate(filePaths);\\n\\n      _emitProgress(\\n        AnalysisProgress(\\n          phase: AnalysisPhase.processing,\\n          progress: 0.8,\\n          message: 'Processing analysis results',\\n          timestamp: DateTime.now(),\\n        ),\\n      );\\n\\n      final statistics = AnalysisStatistics.fromSourceFiles(files);\\n      final mainFiles = identifyMainFiles(files);\\n      final dependencies = extractDependencies(files);\\n      final errors = repositoryAnalyzer.getErrors();\\n\\n      final result = AnalysisResult(\\n        metadata: metadata.copyWith(\\n          fileCount: files.length,\\n          languages: statistics.languageDistribution.keys.toList(),\\n          directoryTree: directoryTree,\\n        ),\\n        files: files,\\n        statistics: statistics,\\n        mainFiles: mainFiles,\\n        dependencies: dependencies,\\n        errors: errors,\\n      );\\n\\n      if (useCache && cacheService != null && commitSha != null) {\\n        _emitProgress(\\n          AnalysisProgress(\\n            phase: AnalysisPhase.caching,\\n            progress: 0.9,\\n            message: 'Caching results',\\n            timestamp: DateTime.now(),\\n          ),\\n        );\\n        await cacheService!.set(repositoryUrl, commitSha, result);\\n      }\\n\\n      _emitProgress(\\n        AnalysisProgress(\\n          phase: AnalysisPhase.completed,\\n          progress: 1.0,\\n          message: 'Analysis completed',\\n          timestamp: DateTime.now(),\\n        ),\\n      );\\n\\n      logger.info('Remote analysis completed: ${files.length} files analyzed');\\n      return result;\\n    } catch (e, stackTrace) {\\n      logger.severe('Remote analysis failed.', e, stackTrace);\\n\\n      _emitProgress(\\n        AnalysisProgress(\\n          phase: AnalysisPhase.error,\\n          progress: 0.0,\\n          message: 'Analysis failed: ${e.toString()}',\\n          timestamp: DateTime.now(),\\n        ),\\n      );\\n\\n      if (e is AnalyzerException) {\\n        rethrow;\\n      }\\n\\n      throw AnalyzerException(\\n        'Remote analysis failed',\\n        code: AnalyzerErrorCode.analysisError,\\n        details: e.toString(),\\n        originalException: e,\\n        stackTrace: stackTrace,\\n      );\\n    }\\n  }\\n\\n  void _emitProgress(AnalysisProgress progress) {\\n    if (progressController != null && !progressController!.isClosed) {\\n      progressController!.add(progress);\\n    }\\n  }\\n}\\n\",\"size\":8140,\"language\":\"Dart\",\"is_binary\":false,\"line_count\":259,\"is_source_code\":true,\"is_configuration\":false,\"is_documentation\":false,\"timestamp\":\"2025-10-15T06:50:04.328687\"},{\"path\":\"lib/src/core/repository_analyzer.dart\",\"content\":\"// lib/src/core/repository_analyzer.dart\\n\\nimport 'dart:io';\\nimport 'dart:convert';\\nimport 'package:archive/archive.dart';\\nimport 'package:github_analyzer/src/common/config.dart';\\nimport 'package:github_analyzer/src/common/language_info.dart';\\nimport 'package:github_analyzer/src/common/logger.dart';\\nimport 'package:github_analyzer/src/common/utils/file_utils.dart';\\nimport 'package:github_analyzer/src/common/errors/analyzer_exception.dart';\\nimport 'package:github_analyzer/src/models/source_file.dart';\\nimport 'package:github_analyzer/src/models/analysis_error.dart';\\nimport 'package:github_analyzer/src/infrastructure/isolate_pool.dart';\\nimport 'package:path/path.dart' as path;\\n\\n/// Analyzes the repository files from a local directory or a memory archive.\\nclass RepositoryAnalyzer {\\n  final GithubAnalyzerConfig config;\\n  final IsolatePool? isolatePool;\\n  final List<AnalysisError> errors = [];\\n\\n  /// Creates an instance of [RepositoryAnalyzer].\\n  RepositoryAnalyzer({\\n    required this.config,\\n    this.isolatePool,\\n  });\\n\\n  /// Analyzes a directory recursively to extract source files.\\n  Future<List<SourceFile>> analyzeDirectory(String directoryPath) async {\\n    logger.info('Analyzing directory: $directoryPath');\\n\\n    final dir = Directory(directoryPath);\\n    if (!await dir.exists()) {\\n      throw AnalyzerException(\\n        'Directory not found: $directoryPath',\\n        code: AnalyzerErrorCode.directoryNotFound,\\n      );\\n    }\\n\\n    final fileEntities = <File>[];\\n    try {\\n      final entities = dir.list(recursive: true);\\n      await for (final entity in entities) {\\n        if (entity is File) {\\n          final relativePath = path.relative(entity.path, from: directoryPath);\\n          if (shouldExclude(relativePath, config.excludePatterns)) {\\n            logger.finer('Excluded by pattern: $relativePath');\\n            continue;\\n          }\\n          fileEntities.add(entity);\\n        }\\n      }\\n    } catch (e, stackTrace) {\\n      logger.severe('Error listing directory contents.', e, stackTrace);\\n    }\\n\\n    final files = await _processFiles(fileEntities, directoryPath);\\n\\n    logger.info(\\n      'Analysis completed: ${files.length} files, ${errors.length} errors',\\n    );\\n    return files;\\n  }\\n\\n  /// Analyzes an in-memory archive to extract source files.\\n  Future<List<SourceFile>> analyzeArchive(Archive archive) async {\\n    logger.info('Analyzing archive from memory...');\\n    final files = <SourceFile>[];\\n    String? baseDir;\\n\\n    if (archive.isNotEmpty && archive.first.name.contains('/')) {\\n      baseDir = archive.first.name.split('/').first;\\n    }\\n\\n    final archiveFiles = archive.where((f) => f.isFile).toList();\\n\\n    for (final file in archiveFiles) {\\n      final relativePath =\\n          (baseDir != null && file.name.startsWith('$baseDir/'))\\n              ? file.name.substring(baseDir.length + 1)\\n              : file.name;\\n\\n      if (relativePath.isEmpty ||\\n          shouldExclude(relativePath, config.excludePatterns)) {\\n        logger.finer('Excluded by pattern: $relativePath');\\n        continue;\\n      }\\n\\n      try {\\n        final sourceFile = await _analyzeArchiveFile(\\n          file,\\n          relativePath,\\n          config.maxFileSize,\\n        );\\n        if (sourceFile != null) {\\n          files.add(sourceFile);\\n        }\\n      } catch (e, stackTrace) {\\n        logger.warning(\\n            'Failed to analyze archive file ${file.name}', e, stackTrace);\\n        errors.add(\\n          AnalysisError(\\n            path: relativePath,\\n            message: e.toString(),\\n            stackTrace: stackTrace.toString(),\\n            timestamp: DateTime.now(),\\n          ),\\n        );\\n      }\\n    }\\n\\n    logger.info(\\n      'Archive analysis completed: ${files.length} files, ${errors.length} errors',\\n    );\\n    return files;\\n  }\\n\\n  Future<List<SourceFile>> _processFiles(\\n    List<File> fileEntities,\\n    String basePath,\\n  ) async {\\n    if (isolatePool != null && fileEntities.length > 50) {\\n      logger.info(\\n        'Using isolate pool for parallel analysis of ${fileEntities.length} files.',\\n      );\\n\\n      final fileDataForIsolates = fileEntities.map((file) {\\n        return {\\n          'filePath': file.path,\\n          'basePath': basePath,\\n          'maxFileSize': config.maxFileSize,\\n        };\\n      }).toList();\\n\\n      final results = await isolatePool!.executeAll(\\n        _analyzeFileInIsolate,\\n        fileDataForIsolates,\\n      );\\n\\n      final files = <SourceFile>[];\\n      for (var result in results) {\\n        if (result is SourceFile) {\\n          files.add(result);\\n        } else if (result is Map<String, String>) {\\n          errors.add(\\n            AnalysisError(\\n              path: result['path']!,\\n              message: result['error']!,\\n              stackTrace: result['stackTrace'],\\n              timestamp: DateTime.now(),\\n            ),\\n          );\\n        }\\n      }\\n      return files;\\n    } else {\\n      return _analyzeFilesSequentially(fileEntities, basePath);\\n    }\\n  }\\n\\n  Future<List<SourceFile>> _analyzeFilesSequentially(\\n    List<File> fileEntities,\\n    String basePath,\\n  ) async {\\n    final files = <SourceFile>[];\\n    for (final entity in fileEntities) {\\n      final relativePath = path.relative(entity.path, from: basePath);\\n      try {\\n        final sourceFile = await _analyzeFile(\\n          entity,\\n          relativePath,\\n          config.maxFileSize,\\n        );\\n        if (sourceFile != null) {\\n          files.add(sourceFile);\\n        }\\n      } catch (e, stackTrace) {\\n        logger.warning('Failed to analyze file ${entity.path}', e, stackTrace);\\n        errors.add(\\n          AnalysisError(\\n            path: relativePath,\\n            message: e.toString(),\\n            stackTrace: stackTrace.toString(),\\n            timestamp: DateTime.now(),\\n          ),\\n        );\\n      }\\n    }\\n    return files;\\n  }\\n\\n  static Future<dynamic> _analyzeFileInIsolate(\\n    Map<String, dynamic> args,\\n  ) async {\\n    final String filePath = args['filePath'];\\n    final String basePath = args['basePath'];\\n    final int maxFileSize = args['maxFileSize'];\\n    final File file = File(filePath);\\n\\n    final relativePath = path.relative(filePath, from: basePath);\\n\\n    try {\\n      final stat = await file.stat();\\n      if (stat.size > maxFileSize) {\\n        return null;\\n      }\\n\\n      final isBinary = isBinaryFile(relativePath);\\n      String? content;\\n      int lineCount = 0;\\n      if (!isBinary) {\\n        try {\\n          content = await file.readAsString();\\n          lineCount = content.split('\\\\n').length;\\n        } catch (e) {\\n          return _createFileModelFromData(\\n            relativePath,\\n            stat.size,\\n            null,\\n            true,\\n            0,\\n            stat.modified,\\n          );\\n        }\\n      }\\n      return _createFileModelFromData(\\n        relativePath,\\n        stat.size,\\n        content,\\n        isBinary,\\n        lineCount,\\n        stat.modified,\\n      );\\n    } catch (e, stackTrace) {\\n      return {\\n        'path': relativePath,\\n        'error': e.toString(),\\n        'stackTrace': stackTrace.toString(),\\n      };\\n    }\\n  }\\n\\n  Future<SourceFile?> _analyzeArchiveFile(\\n    ArchiveFile file,\\n    String relativePath,\\n    int maxFileSize,\\n  ) async {\\n    if (file.size > maxFileSize) {\\n      logger.finer('Excluded large file: $relativePath');\\n      return null;\\n    }\\n\\n    final isBinary = isBinaryFile(relativePath);\\n    String? content;\\n    int lineCount = 0;\\n    final timestamp = DateTime.now();\\n\\n    if (!isBinary) {\\n      try {\\n        content = utf8.decode(file.content as List<int>, allowMalformed: true);\\n        lineCount = content.split('\\\\n').length;\\n      } catch (e) {\\n        logger.finer('Failed to read archive file as text $relativePath: $e');\\n        return _createFileModel(\\n          relativePath,\\n          file.size,\\n          null,\\n          true,\\n          0,\\n          timestamp,\\n        );\\n      }\\n    }\\n\\n    return _createFileModel(\\n      relativePath,\\n      file.size,\\n      content,\\n      isBinary,\\n      lineCount,\\n      timestamp,\\n    );\\n  }\\n\\n  Future<SourceFile?> _analyzeFile(\\n    File file,\\n    String relativePath,\\n    int maxFileSize,\\n  ) async {\\n    final stat = await file.stat();\\n    if (stat.size > maxFileSize) {\\n      logger.finer('Excluded large file: $relativePath');\\n      return null;\\n    }\\n\\n    final isBinary = isBinaryFile(relativePath);\\n    String? content;\\n    int lineCount = 0;\\n\\n    if (!isBinary) {\\n      try {\\n        content = await file.readAsString();\\n        lineCount = content.split('\\\\n').length;\\n      } catch (e) {\\n        logger.finer('Failed to read file as text $relativePath: $e');\\n        return _createFileModel(\\n          relativePath,\\n          stat.size,\\n          null,\\n          true,\\n          0,\\n          stat.modified,\\n        );\\n      }\\n    }\\n    return _createFileModel(\\n      relativePath,\\n      stat.size,\\n      content,\\n      isBinary,\\n      lineCount,\\n      stat.modified,\\n    );\\n  }\\n\\n  SourceFile _createFileModel(\\n    String path,\\n    int size,\\n    String? content,\\n    bool isBinary,\\n    int lineCount,\\n    DateTime timestamp,\\n  ) {\\n    final language = detectLanguage(path);\\n    final isDoc = isDocumentationFile(path);\\n    final isConfig = isConfigurationFile(path);\\n    final isSrc = language != null && !isBinary && !isDoc && !isConfig;\\n\\n    return SourceFile(\\n      path: path,\\n      content: content,\\n      size: size,\\n      language: language,\\n      isBinary: isBinary,\\n      lineCount: lineCount,\\n      isSourceCode: isSrc,\\n      isConfiguration: isConfig,\\n      isDocumentation: isDoc,\\n      timestamp: timestamp,\\n    );\\n  }\\n\\n  static SourceFile _createFileModelFromData(\\n    String path,\\n    int size,\\n    String? content,\\n    bool isBinary,\\n    int lineCount,\\n    DateTime timestamp,\\n  ) {\\n    final language = detectLanguage(path);\\n    final isDoc = isDocumentationFile(path);\\n    final isConfig = isConfigurationFile(path);\\n    final isSrc = language != null && !isBinary && !isDoc && !isConfig;\\n\\n    return SourceFile(\\n      path: path,\\n      content: content,\\n      size: size,\\n      language: language,\\n      isBinary: isBinary,\\n      lineCount: lineCount,\\n      isSourceCode: isSrc,\\n      isConfiguration: isConfig,\\n      isDocumentation: isDoc,\\n      timestamp: timestamp,\\n    );\\n  }\\n\\n  List<AnalysisError> getErrors() => List.unmodifiable(errors);\\n\\n  void clearErrors() => errors.clear();\\n}\\n\",\"size\":10315,\"language\":\"Dart\",\"is_binary\":false,\"line_count\":384,\"is_source_code\":true,\"is_configuration\":false,\"is_documentation\":false,\"timestamp\":\"2025-10-15T06:50:04.329172\"},{\"path\":\"lib/src/data/providers/github_api_provider.dart\",\"content\":\"import 'package:dio/dio.dart';\\nimport 'package:github_analyzer/src/common/logger.dart';\\nimport 'package:github_analyzer/src/common/errors/analyzer_exception.dart';\\nimport 'package:github_analyzer/src/infrastructure/interfaces/i_http_client_manager.dart';\\nimport 'package:github_analyzer/src/infrastructure/interfaces/i_github_api_provider.dart';\\nimport 'package:github_analyzer/src/models/repository_metadata.dart';\\n\\n/// Provides access to the GitHub API for fetching repository metadata.\\nclass GithubApiProvider implements IGithubApiProvider {\\n  final String? token;\\n  final IHttpClientManager httpClientManager;\\n\\n  /// Creates an instance of [GithubApiProvider].\\n  GithubApiProvider({\\n    this.token,\\n    required this.httpClientManager,\\n  });\\n\\n  @override\\n  Future<RepositoryMetadata> getRepositoryMetadata(\\n    String owner,\\n    String repo,\\n  ) async {\\n    logger.info('Fetching repository metadata: $owner/$repo');\\n\\n    final uri = Uri.parse('https://api.github.com/repos/$owner/$repo');\\n    final headers = {\\n      'Accept': 'application/vnd.github.v3+json',\\n      if (token != null) 'Authorization': 'token $token',\\n    };\\n\\n    try {\\n      final response = await httpClientManager.get(\\n        uri,\\n        headers: headers,\\n        responseType: ResponseType.json,\\n      );\\n\\n      final json = response.data as Map<String, dynamic>;\\n\\n      // The following API calls are best-effort. If they fail, we proceed\\n      // with the data we have.\\n\\n      final languages = await _fetchLanguages(owner, repo, headers);\\n      final commitSha = await _fetchCommitSha(\\n        owner,\\n        repo,\\n        json['default_branch'] as String? ?? 'main',\\n        headers,\\n      );\\n\\n      return RepositoryMetadata(\\n        name: json['name'] as String,\\n        fullName: json['full_name'] as String?,\\n        description: json['description'] as String?,\\n        isPrivate: json['private'] as bool? ?? false,\\n        defaultBranch: json['default_branch'] as String? ?? 'main',\\n        language: json['language'] as String?,\\n        languages: languages,\\n        stars: json['stargazers_count'] as int? ?? 0,\\n        forks: json['forks_count'] as int? ?? 0,\\n        commitSha: commitSha,\\n        fileCount: 0, // Populated later.\\n        directoryTree: '', // Populated later.\\n      );\\n    } on DioException catch (e, stackTrace) {\\n      logger.severe('Failed to fetch repository metadata.', e, stackTrace);\\n      if (e.response?.statusCode == 404) {\\n        throw AnalyzerException(\\n          'Repository not found: $owner/$repo',\\n          code: AnalyzerErrorCode.repositoryNotFound,\\n          details: 'The repository does not exist or is not accessible.',\\n          originalException: e,\\n        );\\n      } else if (e.response?.statusCode == 403) {\\n        throw AnalyzerException(\\n          'Access forbidden.',\\n          code: AnalyzerErrorCode.accessDenied,\\n          details: 'Check your token permissions or rate limits.',\\n          originalException: e,\\n        );\\n      }\\n      throw AnalyzerException(\\n        'Failed to fetch repository metadata due to a network error.',\\n        code: AnalyzerErrorCode.networkError,\\n        details: e.message,\\n        originalException: e,\\n        stackTrace: stackTrace,\\n      );\\n    } catch (e, stackTrace) {\\n      logger.severe('An unexpected error occurred.', e, stackTrace);\\n      throw AnalyzerException(\\n        'An unexpected error occurred while fetching repository metadata.',\\n        code: AnalyzerErrorCode.analysisError,\\n        originalException: e,\\n        stackTrace: stackTrace,\\n      );\\n    }\\n  }\\n\\n  Future<List<String>> _fetchLanguages(\\n    String owner,\\n    String repo,\\n    Map<String, String> headers,\\n  ) async {\\n    try {\\n      final languagesUri =\\n          Uri.parse('https://api.github.com/repos/$owner/$repo/languages');\\n      final languagesResponse = await httpClientManager.get(\\n        languagesUri,\\n        headers: headers,\\n        responseType: ResponseType.json,\\n      );\\n      if (languagesResponse.statusCode == 200) {\\n        final languagesJson = languagesResponse.data as Map<String, dynamic>;\\n        return languagesJson.keys.toList();\\n      }\\n    } catch (e, stackTrace) {\\n      logger.warning(\\n        'Could not fetch repository languages. Proceeding without it.',\\n        e,\\n        stackTrace,\\n      );\\n    }\\n    return [];\\n  }\\n\\n  Future<String?> _fetchCommitSha(\\n    String owner,\\n    String repo,\\n    String defaultBranch,\\n    Map<String, String> headers,\\n  ) async {\\n    try {\\n      final branchInfoUri = Uri.parse(\\n        'https://api.github.com/repos/$owner/$repo/branches/$defaultBranch',\\n      );\\n      final branchResponse = await httpClientManager.get(\\n        branchInfoUri,\\n        headers: headers,\\n        responseType: ResponseType.json,\\n      );\\n      if (branchResponse.statusCode == 200) {\\n        final branchJson = branchResponse.data as Map<String, dynamic>;\\n        return branchJson['commit']?['sha'] as String?;\\n      }\\n    } catch (e, stackTrace) {\\n      logger.warning(\\n        'Could not fetch branch information for $defaultBranch. Proceeding without commit SHA.',\\n        e,\\n        stackTrace,\\n      );\\n    }\\n    return null;\\n  }\\n\\n  @override\\n  void dispose() {\\n    // HttpClientManager is disposed by the GithubAnalyzer class\\n  }\\n}\\n\",\"size\":5252,\"language\":\"Dart\",\"is_binary\":false,\"line_count\":162,\"is_source_code\":true,\"is_configuration\":false,\"is_documentation\":false,\"timestamp\":\"2025-10-15T06:50:04.329615\"},{\"path\":\"lib/src/data/providers/zip_downloader.dart\",\"content\":\"import 'dart:io';\\nimport 'dart:typed_data';\\nimport 'package:dio/dio.dart';\\nimport 'package:github_analyzer/src/common/logger.dart';\\nimport 'package:github_analyzer/src/common/errors/analyzer_exception.dart';\\nimport 'package:github_analyzer/src/infrastructure/interfaces/i_http_client_manager.dart';\\n\\n/// Handles downloading of repository zip archives from GitHub.\\nclass ZipDownloader {\\n  final IHttpClientManager httpClientManager;\\n\\n  /// Creates an instance of [ZipDownloader].\\n  ZipDownloader({required this.httpClientManager});\\n\\n  /// Downloads a repository as a byte array (Uint8List).\\n  Future<Uint8List> downloadRepositoryAsBytes({\\n    required String owner,\\n    required String repo,\\n    required String branch,\\n    String? token,\\n  }) async {\\n    logger.info(\\n      'Downloading repository as bytes: $owner/$repo (branch: $branch)',\\n    );\\n\\n    final url =\\n        'https://github.com/$owner/$repo/archive/refs/heads/$branch.zip';\\n    final uri = Uri.parse(url);\\n\\n    final headers = <String, String>{\\n      'Accept': 'application/zip',\\n      if (token != null) 'Authorization': 'token $token',\\n    };\\n\\n    try {\\n      final response = await httpClientManager.get(\\n        uri,\\n        headers: headers,\\n        responseType: ResponseType.bytes,\\n      );\\n      // Dio is configured to throw exceptions for non-2xx status codes,\\n      // so we don't need to check for response.statusCode != 200 here.\\n      logger.info('Repository downloaded as bytes successfully');\\n      return Uint8List.fromList(response.data as List<int>);\\n    } on DioException catch (e, stackTrace) {\\n      if (e.response?.statusCode == 404) {\\n        throw AnalyzerException(\\n          'Repository or branch not found: $owner/$repo@$branch',\\n          code: AnalyzerErrorCode.repositoryNotFound,\\n          details: 'The repository or specified branch does not exist.',\\n          originalException: e,\\n        );\\n      }\\n      logger.severe('Failed to download repository as bytes.', e, stackTrace);\\n      throw AnalyzerException(\\n        'Failed to download repository as bytes',\\n        code: AnalyzerErrorCode.networkError,\\n        details: e.toString(),\\n        originalException: e,\\n        stackTrace: stackTrace,\\n      );\\n    } catch (e, stackTrace) {\\n      if (e is AnalyzerException) {\\n        rethrow;\\n      }\\n      logger.severe('An unexpected error occurred.', e, stackTrace);\\n      throw AnalyzerException(\\n        'An unexpected error occurred while downloading repository as bytes.',\\n        code: AnalyzerErrorCode.networkError,\\n        details: e.toString(),\\n        originalException: e,\\n        stackTrace: stackTrace,\\n      );\\n    }\\n  }\\n\\n  /// Downloads a repository to a temporary local zip file.\\n  Future<String> downloadRepository({\\n    required String owner,\\n    required String repo,\\n    required String branch,\\n    String? token,\\n  }) async {\\n    try {\\n      final zipBytes = await downloadRepositoryAsBytes(\\n        owner: owner,\\n        repo: repo,\\n        branch: branch,\\n        token: token,\\n      );\\n\\n      final tempDir = await Directory.systemTemp.createTemp('github_analyzer_');\\n      final zipPath = '${tempDir.path}/$repo-$branch.zip';\\n      final zipFile = File(zipPath);\\n\\n      await zipFile.writeAsBytes(zipBytes);\\n      logger.info('Repository downloaded to: $zipPath');\\n\\n      return zipPath;\\n    } catch (e, stackTrace) {\\n      if (e is AnalyzerException) {\\n        rethrow;\\n      }\\n      logger.severe('Failed to download repository.', e, stackTrace);\\n      throw AnalyzerException(\\n        'Failed to download repository',\\n        code: AnalyzerErrorCode.networkError,\\n        details: e.toString(),\\n        originalException: e,\\n        stackTrace: stackTrace,\\n      );\\n    }\\n  }\\n}\\n\",\"size\":3708,\"language\":\"Dart\",\"is_binary\":false,\"line_count\":115,\"is_source_code\":true,\"is_configuration\":false,\"is_documentation\":false,\"timestamp\":\"2025-10-15T06:50:04.330026\"},{\"path\":\"lib/src/github_analyzer.dart\",\"content\":\"import 'dart:async';\\nimport 'dart:io';\\nimport 'package:github_analyzer/src/common/config.dart';\\nimport 'package:github_analyzer/src/common/logger.dart';\\nimport 'package:github_analyzer/src/infrastructure/interfaces/i_github_api_provider.dart';\\nimport 'package:github_analyzer/src/data/providers/zip_downloader.dart';\\nimport 'package:github_analyzer/src/core/cache_service.dart';\\nimport 'package:github_analyzer/src/core/local_analyzer_service.dart';\\nimport 'package:github_analyzer/src/core/remote_analyzer_service.dart';\\nimport 'package:github_analyzer/src/infrastructure/interfaces/i_http_client_manager.dart';\\nimport 'package:github_analyzer/src/infrastructure/isolate_pool.dart';\\nimport 'package:github_analyzer/src/models/analysis_result.dart';\\nimport 'package:github_analyzer/src/models/analysis_progress.dart';\\n\\n/// The main class for analyzing GitHub repositories.\\n///\\n/// This class coordinates the analysis process for both local and remote\\n/// repositories by delegating tasks to specialized services. It is configured\\n/// via a [GithubAnalyzerConfig] object and reports progress through a stream.\\n///\\n/// Dependencies are injected through the constructor to promote modularity\\n/// and testability.\\nclass GithubAnalyzer {\\n  final GithubAnalyzerConfig config;\\n  final IHttpClientManager httpClientManager;\\n  final IGithubApiProvider apiProvider;\\n  final ZipDownloader zipDownloader;\\n  final CacheService? cacheService;\\n  final IsolatePool? isolatePool;\\n  final LocalAnalyzerService localAnalyzer;\\n  final RemoteAnalyzerService remoteAnalyzer;\\n\\n  final StreamController<AnalysisProgress> _progressController =\\n      StreamController.broadcast();\\n\\n  /// A stream of [AnalysisProgress] updates.\\n  Stream<AnalysisProgress> get progressStream => _progressController.stream;\\n\\n  /// Creates an instance of [GithubAnalyzer].\\n  ///\\n  /// All service dependencies must be provided. This allows for flexible\\n  /// configuration and easy mocking for tests.\\n  GithubAnalyzer({\\n    required this.config,\\n    required this.httpClientManager,\\n    required this.apiProvider,\\n    required this.zipDownloader,\\n    required this.localAnalyzer,\\n    required this.remoteAnalyzer,\\n    this.cacheService,\\n    this.isolatePool,\\n  }) {\\n    // Initialize services that require it\\n    cacheService?.initialize();\\n    isolatePool?.initialize();\\n  }\\n\\n  /// Analyzes a local directory.\\n  Future<AnalysisResult> analyzeLocal(String directoryPath) async {\\n    logger.info('Starting local analysis: $directoryPath');\\n\\n    _progressController.add(\\n      AnalysisProgress(\\n        phase: AnalysisPhase.initializing,\\n        progress: 0.0,\\n        message: 'Starting local analysis',\\n        timestamp: DateTime.now(),\\n      ),\\n    );\\n\\n    final result = await localAnalyzer.analyze(directoryPath);\\n\\n    _progressController.add(\\n      AnalysisProgress(\\n        phase: AnalysisPhase.completed,\\n        progress: 1.0,\\n        message: 'Local analysis completed',\\n        timestamp: DateTime.now(),\\n      ),\\n    );\\n\\n    return result;\\n  }\\n\\n  /// Analyzes a remote repository from a URL.\\n  Future<AnalysisResult> analyzeRemote({\\n    required String repositoryUrl,\\n    String? branch,\\n    bool useCache = true,\\n  }) async {\\n    logger.info('Starting remote analysis: $repositoryUrl');\\n    // Pass the progress controller to the remote analyzer service\\n    final remoteServiceWithProgress = remoteAnalyzer.copyWith(\\n      progressController: _progressController,\\n    );\\n    final result = await remoteServiceWithProgress.analyze(\\n      repositoryUrl: repositoryUrl,\\n      branch: branch,\\n      useCache: useCache,\\n    );\\n\\n    return result;\\n  }\\n\\n  /// Analyzes a target which can be either a local path or a remote URL.\\n  Future<AnalysisResult> analyze(String target, {String? branch}) async {\\n    // Basic check to differentiate between a URL and a local path.\\n    if (target.startsWith('http') || target.startsWith('git@')) {\\n      return await analyzeRemote(repositoryUrl: target, branch: branch);\\n    } else {\\n      return await analyzeLocal(target);\\n    }\\n  }\\n\\n  /// Clears the cache if it is enabled.\\n  Future<void> clearCache() async {\\n    if (cacheService != null) {\\n      await cacheService!.clear();\\n      logger.info('Cache cleared');\\n    }\\n  }\\n\\n  /// Gets statistics about the cache.\\n  Future<Map<String, dynamic>?> getCacheStatistics() async {\\n    return await cacheService?.getStatistics();\\n  }\\n\\n  /// Disposes all resources used by the analyzer.\\n  Future<void> dispose() async {\\n    _progressController.close();\\n    httpClientManager.dispose();\\n    isolatePool?.dispose();\\n    if (cacheService != null) {\\n      final dir = Directory(config.cacheDirectory);\\n      if (await dir.exists()) {\\n        try {\\n          // This recursively deletes the directory and all its contents,\\n          // making the separate `clear()` call redundant.\\n          await dir.delete(recursive: true);\\n          logger.info('Cache directory removed: ${config.cacheDirectory}');\\n        } catch (e, stackTrace) {\\n          logger.severe('Failed to delete cache directory.', e, stackTrace);\\n        }\\n      }\\n    }\\n    logger.info('GithubAnalyzer disposed');\\n  }\\n}\\n\",\"size\":5124,\"language\":\"Dart\",\"is_binary\":false,\"line_count\":149,\"is_source_code\":true,\"is_configuration\":false,\"is_documentation\":false,\"timestamp\":\"2025-10-15T06:50:04.330474\"},{\"path\":\"lib/src/infrastructure/file_system/file_system.dart\",\"content\":\"export 'file_system_interface.dart';\\n\\n// Conditional import for platform-specific factory\\nimport 'file_system_interface.dart';\\nimport 'file_system_stub.dart'\\n    if (dart.library.io) 'file_system_io.dart'\\n    if (dart.library.html) 'file_system_web.dart';\\n\\n/// Get platform-specific file system instance\\nIFileSystem getFileSystem() => createPlatformFileSystem();\\n\",\"size\":363,\"language\":\"Dart\",\"is_binary\":false,\"line_count\":11,\"is_source_code\":true,\"is_configuration\":false,\"is_documentation\":false,\"timestamp\":\"2025-10-15T06:50:04.330919\"},{\"path\":\"lib/src/infrastructure/file_system/file_system_interface.dart\",\"content\":\"/// Abstract file system interface to enable platform-independent file operations\\nabstract class IFileSystem {\\n  /// Check if a directory exists\\n  Future<bool> directoryExists(String path);\\n\\n  /// Create a directory (recursive)\\n  Future<void> createDirectory(String path);\\n\\n  /// Write a string to a file (overwrites if exists)\\n  Future<void> writeFile(String path, String contents);\\n\\n  /// Read a string from a file\\n  Future<String> readFile(String path);\\n\\n  /// Check if the file exists\\n  Future<bool> fileExists(String path);\\n\\n  /// List all files in a directory recursively\\n  Future<List<String>> listFilesRecursively(String directory);\\n\\n  /// Delete a file or directory recursively\\n  Future<void> delete(String path);\\n}\\n\",\"size\":725,\"language\":\"Dart\",\"is_binary\":false,\"line_count\":24,\"is_source_code\":true,\"is_configuration\":false,\"is_documentation\":false,\"timestamp\":\"2025-10-15T06:50:04.331324\"},{\"path\":\"lib/src/infrastructure/file_system/file_system_io.dart\",\"content\":\"import 'package:universal_io/io.dart';\\nimport 'file_system_interface.dart';\\n\\n/// Implementation of IFileSystem using universal_io for native platforms\\nclass IOFileSystem implements IFileSystem {\\n  @override\\n  Future<bool> directoryExists(String path) async {\\n    return Directory(path).exists();\\n  }\\n\\n  @override\\n  Future<void> createDirectory(String path) async {\\n    final dir = Directory(path);\\n    if (!await dir.exists()) {\\n      await dir.create(recursive: true);\\n    }\\n  }\\n\\n  @override\\n  Future<void> writeFile(String path, String contents) async {\\n    final file = File(path);\\n    await file.writeAsString(contents, flush: true);\\n  }\\n\\n  @override\\n  Future<String> readFile(String path) async {\\n    final file = File(path);\\n    return file.readAsString();\\n  }\\n\\n  @override\\n  Future<bool> fileExists(String path) async {\\n    return File(path).exists();\\n  }\\n\\n  @override\\n  Future<List<String>> listFilesRecursively(String directory) async {\\n    final dir = Directory(directory);\\n    if (!await dir.exists()) return [];\\n\\n    final List<String> files = [];\\n    await for (final entity in dir.list(recursive: true, followLinks: false)) {\\n      if (entity is File) {\\n        files.add(entity.path);\\n      }\\n    }\\n    return files;\\n  }\\n\\n  @override\\n  Future<void> delete(String path) async {\\n    final file = File(path);\\n    final dir = Directory(path);\\n\\n    if (await file.exists()) {\\n      await file.delete(recursive: true);\\n    } else if (await dir.exists()) {\\n      await dir.delete(recursive: true);\\n    }\\n  }\\n}\\n\\n/// Factory function for IO platform\\nIFileSystem createPlatformFileSystem() => IOFileSystem();\\n\",\"size\":1614,\"language\":\"Dart\",\"is_binary\":false,\"line_count\":65,\"is_source_code\":true,\"is_configuration\":false,\"is_documentation\":false,\"timestamp\":\"2025-10-15T06:50:04.331765\"},{\"path\":\"lib/src/infrastructure/file_system/file_system_stub.dart\",\"content\":\"import 'file_system_interface.dart';\\n\\n/// Stub implementation for unsupported platforms (fallback)\\nclass StubFileSystem implements IFileSystem {\\n  @override\\n  Future<void> createDirectory(String path) {\\n    throw UnsupportedError('File system is not supported on this platform');\\n  }\\n\\n  @override\\n  Future<void> delete(String path) {\\n    throw UnsupportedError('File system is not supported on this platform');\\n  }\\n\\n  @override\\n  Future<String> readFile(String path) {\\n    throw UnsupportedError('File system is not supported on this platform');\\n  }\\n\\n  @override\\n  Future<List<String>> listFilesRecursively(String directory) {\\n    throw UnsupportedError('File system is not supported on this platform');\\n  }\\n\\n  @override\\n  Future<bool> directoryExists(String path) {\\n    throw UnsupportedError('File system is not supported on this platform');\\n  }\\n\\n  @override\\n  Future<bool> fileExists(String path) {\\n    throw UnsupportedError('File system is not supported on this platform');\\n  }\\n\\n  @override\\n  Future<void> writeFile(String path, String contents) {\\n    throw UnsupportedError('File system is not supported on this platform');\\n  }\\n}\\n\\n/// Factory function for Stub platform\\nIFileSystem createPlatformFileSystem() => StubFileSystem();\\n\",\"size\":1236,\"language\":\"Dart\",\"is_binary\":false,\"line_count\":43,\"is_source_code\":true,\"is_configuration\":false,\"is_documentation\":false,\"timestamp\":\"2025-10-15T06:50:04.332157\"},{\"path\":\"lib/src/infrastructure/file_system/file_system_web.dart\",\"content\":\"import 'dart:async';\\nimport 'file_system_interface.dart';\\n\\n/// In-memory file system implementation for web platforms\\nclass WebFileSystem implements IFileSystem {\\n  final Map<String, String> _files = {};\\n  final Map<String, Set<String>> _directories = {};\\n\\n  @override\\n  Future<bool> directoryExists(String path) async {\\n    return _directories.containsKey(path);\\n  }\\n\\n  @override\\n  Future<void> createDirectory(String path) async {\\n    if (!_directories.containsKey(path)) {\\n      _directories[path] = <String>{};\\n    }\\n  }\\n\\n  @override\\n  Future<void> writeFile(String path, String contents) async {\\n    _files[path] = contents;\\n\\n    final idx = path.lastIndexOf('/');\\n    if (idx != -1) {\\n      final dir = path.substring(0, idx);\\n      _directories.putIfAbsent(dir, () => <String>{});\\n      _directories[dir]!.add(path);\\n    }\\n  }\\n\\n  @override\\n  Future<String> readFile(String path) async {\\n    final content = _files[path];\\n    if (content == null) {\\n      throw Exception('File not found: $path');\\n    }\\n    return content;\\n  }\\n\\n  @override\\n  Future<bool> fileExists(String path) async {\\n    return _files.containsKey(path);\\n  }\\n\\n  @override\\n  Future<List<String>> listFilesRecursively(String directory) async {\\n    if (!_directories.containsKey(directory)) return [];\\n\\n    final result = <String>[];\\n    void collectFiles(String dirPath) {\\n      if (_directories.containsKey(dirPath)) {\\n        for (final filePath in _directories[dirPath]!) {\\n          result.add(filePath);\\n        }\\n      }\\n    }\\n\\n    collectFiles(directory);\\n    return result;\\n  }\\n\\n  @override\\n  Future<void> delete(String path) async {\\n    _files.remove(path);\\n\\n    final idx = path.lastIndexOf('/');\\n    if (idx != -1) {\\n      final dir = path.substring(0, idx);\\n      _directories[dir]?.remove(path);\\n      if ((_directories[dir]?.isEmpty ?? false)) {\\n        _directories.remove(dir);\\n      }\\n    }\\n  }\\n}\\n\\n/// Factory function for Web platform\\nIFileSystem createPlatformFileSystem() => WebFileSystem();\\n\",\"size\":1985,\"language\":\"Dart\",\"is_binary\":false,\"line_count\":81,\"is_source_code\":true,\"is_configuration\":false,\"is_documentation\":false,\"timestamp\":\"2025-10-15T06:50:04.332541\"},{\"path\":\"lib/src/infrastructure/http_client_manager.dart\",\"content\":\"import 'dart:async';\\nimport 'dart:io';\\nimport 'package:dio/dio.dart';\\nimport 'package:github_analyzer/src/common/logger.dart';\\nimport 'package:github_analyzer/src/infrastructure/interfaces/i_http_client_manager.dart';\\n\\n/// An HTTP client manager using the dio package for robust networking.\\n/// It handles retries, timeouts, and concurrent requests automatically.\\nclass HttpClientManager implements IHttpClientManager {\\n  final Dio _dio;\\n\\n  /// Creates an instance of [HttpClientManager].\\n  HttpClientManager({\\n    Duration requestTimeout = const Duration(seconds: 30),\\n    int maxConcurrentRequests = 10,\\n    int maxRetries = 3,\\n  }) : _dio = Dio(\\n          BaseOptions(\\n            connectTimeout: requestTimeout,\\n            receiveTimeout: requestTimeout,\\n          ),\\n        ) {\\n    _dio.interceptors.add(\\n      InterceptorsWrapper(\\n        onRequest: (options, handler) {\\n          logger.finer('Request: ${options.method} ${options.uri}');\\n          return handler.next(options);\\n        },\\n        onResponse: (response, handler) {\\n          logger.finer('Response: ${response.statusCode}');\\n          return handler.next(response);\\n        },\\n        onError: (DioException e, handler) async {\\n          logger.warning('Request Error: ${e.message}', e, e.stackTrace);\\n          // Simple retry logic, dio has more advanced retry packages if needed.\\n          if (e.type == DioExceptionType.connectionTimeout ||\\n              e.type == DioExceptionType.receiveTimeout ||\\n              e.type == DioExceptionType.sendTimeout) {\\n            logger.info('Retrying request due to timeout...');\\n            try {\\n              final response = await _dio.request(\\n                e.requestOptions.path,\\n                options: Options(\\n                  method: e.requestOptions.method,\\n                  headers: e.requestOptions.headers,\\n                ),\\n              );\\n              return handler.resolve(response);\\n            } catch (err) {\\n              return handler.next(e);\\n            }\\n          }\\n          return handler.next(e);\\n        },\\n      ),\\n    );\\n    // Adjust the pool manager for concurrent requests\\n    (_dio.httpClientAdapter as dynamic).createHttpClient = () {\\n      final client = HttpClient();\\n      client.maxConnectionsPerHost = maxConcurrentRequests;\\n      return client;\\n    };\\n  }\\n\\n  @override\\n  Future<Response> get(\\n    Uri uri, {\\n    Map<String, String>? headers,\\n    ResponseType? responseType,\\n  }) async {\\n    try {\\n      final response = await _dio.getUri(\\n        uri,\\n        options: Options(headers: headers, responseType: responseType),\\n      );\\n      return response;\\n    } on DioException {\\n      // Re-throw the original DioException to preserve status codes and other details.\\n      rethrow;\\n    }\\n  }\\n\\n  @override\\n  Future<Response> post(\\n    Uri uri, {\\n    Map<String, String>? headers,\\n    Object? body,\\n  }) async {\\n    try {\\n      final response = await _dio.postUri(\\n        uri,\\n        data: body,\\n        options: Options(headers: headers),\\n      );\\n      return response;\\n    } on DioException {\\n      // Re-throw the original DioException.\\n      rethrow;\\n    }\\n  }\\n\\n  @override\\n  void dispose() {\\n    _dio.close();\\n    logger.info('HttpClientManager disposed');\\n  }\\n}\\n\",\"size\":3242,\"language\":\"Dart\",\"is_binary\":false,\"line_count\":108,\"is_source_code\":true,\"is_configuration\":false,\"is_documentation\":false,\"timestamp\":\"2025-10-15T06:50:04.332999\"},{\"path\":\"lib/src/infrastructure/interfaces/i_github_api_provider.dart\",\"content\":\"import 'package:github_analyzer/src/models/repository_metadata.dart';\\n\\nabstract class IGithubApiProvider {\\n  Future<RepositoryMetadata> getRepositoryMetadata(String owner, String repo);\\n  void dispose();\\n}\\n\",\"size\":206,\"language\":\"Dart\",\"is_binary\":false,\"line_count\":7,\"is_source_code\":true,\"is_configuration\":false,\"is_documentation\":false,\"timestamp\":\"2025-10-15T06:50:04.333430\"},{\"path\":\"lib/src/infrastructure/interfaces/i_http_client_manager.dart\",\"content\":\"import 'package:dio/dio.dart';\\n\\n/// Abstract interface for an HTTP client manager.\\n/// This allows for interchangeable HTTP client implementations.\\nabstract class IHttpClientManager {\\n  /// Performs a GET request.\\n  Future<Response> get(\\n    Uri uri, {\\n    Map<String, String>? headers,\\n    ResponseType? responseType,\\n  });\\n\\n  /// Performs a POST request.\\n  Future<Response> post(Uri uri, {Map<String, String>? headers, Object? body});\\n\\n  /// Disposes of the client's resources.\\n  void dispose();\\n}\\n\",\"size\":500,\"language\":\"Dart\",\"is_binary\":false,\"line_count\":19,\"is_source_code\":true,\"is_configuration\":false,\"is_documentation\":false,\"timestamp\":\"2025-10-15T06:50:04.333822\"},{\"path\":\"lib/src/infrastructure/isolate_pool.dart\",\"content\":\"import 'dart:isolate';\\nimport 'package:github_analyzer/src/common/logger.dart';\\n\\n/// Manages a pool of isolates to perform tasks in parallel.\\nclass IsolatePool {\\n  final int size;\\n  final List<_IsolateWorker> _workers = [];\\n  int _currentWorkerIndex = 0;\\n  bool _isInitialized = false;\\n\\n  /// Creates an instance of [IsolatePool].\\n  IsolatePool({required this.size});\\n\\n  /// Initializes the isolate pool by spawning the configured number of workers.\\n  Future<void> initialize() async {\\n    if (_isInitialized) return;\\n\\n    logger.info('Initializing isolate pool with $size workers');\\n\\n    for (int i = 0; i < size; i++) {\\n      final worker = _IsolateWorker(id: i);\\n      await worker.spawn();\\n      _workers.add(worker);\\n    }\\n\\n    _isInitialized = true;\\n    logger.info('Isolate pool initialized');\\n  }\\n\\n  /// Executes a task on the next available isolate in the pool.\\n  Future<R> execute<T, R>(Future<R> Function(T) task, T argument) async {\\n    if (!_isInitialized) {\\n      throw StateError('IsolatePool not initialized. Call initialize() first.');\\n    }\\n\\n    final worker = _workers[_currentWorkerIndex];\\n    _currentWorkerIndex = (_currentWorkerIndex + 1) % _workers.length;\\n\\n    return await worker.execute(task, argument);\\n  }\\n\\n  /// Executes a list of tasks distributed across the isolate pool.\\n  Future<List<R>> executeAll<T, R>(\\n    Future<R> Function(T) task,\\n    List<T> arguments,\\n  ) async {\\n    if (!_isInitialized) {\\n      throw StateError('IsolatePool not initialized. Call initialize() first.');\\n    }\\n\\n    final futures = <Future<R>>[];\\n\\n    for (int i = 0; i < arguments.length; i++) {\\n      final worker = _workers[i % _workers.length];\\n      futures.add(worker.execute(task, arguments[i]));\\n    }\\n\\n    return await Future.wait(futures);\\n  }\\n\\n  /// Disposes the isolate pool by terminating all worker isolates.\\n  Future<void> dispose() async {\\n    if (!_isInitialized) return;\\n\\n    logger.info('Disposing isolate pool');\\n\\n    for (final worker in _workers) {\\n      await worker.kill();\\n    }\\n\\n    _workers.clear();\\n    _isInitialized = false;\\n    logger.info('Isolate pool disposed');\\n  }\\n}\\n\\nclass _IsolateWorker {\\n  final int id;\\n  Isolate? _isolate;\\n  SendPort? _sendPort;\\n  final ReceivePort _receivePort = ReceivePort();\\n\\n  _IsolateWorker({required this.id});\\n\\n  Future<void> spawn() async {\\n    logger.fine('Spawning isolate worker $id');\\n\\n    _isolate = await Isolate.spawn(_isolateEntryPoint, _receivePort.sendPort);\\n\\n    _sendPort = await _receivePort.first as SendPort;\\n    logger.fine('Isolate worker $id spawned');\\n  }\\n\\n  Future<R> execute<T, R>(Future<R> Function(T) task, T argument) async {\\n    if (_sendPort == null) {\\n      throw StateError('Isolate not spawned');\\n    }\\n\\n    final responsePort = ReceivePort();\\n    _sendPort!.send([task, argument, responsePort.sendPort]);\\n\\n    final result = await responsePort.first;\\n    responsePort.close();\\n\\n    if (result is _IsolateError) {\\n      throw Exception('Isolate error: ${result.message}\\\\n${result.stackTrace}');\\n    }\\n\\n    return result as R;\\n  }\\n\\n  Future<void> kill() async {\\n    _isolate?.kill(priority: Isolate.immediate);\\n    _receivePort.close();\\n    logger.fine('Isolate worker $id killed');\\n  }\\n\\n  static void _isolateEntryPoint(SendPort sendPort) {\\n    final receivePort = ReceivePort();\\n    sendPort.send(receivePort.sendPort);\\n\\n    receivePort.listen((message) async {\\n      final task = message[0] as Future<dynamic> Function(dynamic);\\n      final argument = message[1];\\n      final responsePort = message[2] as SendPort;\\n\\n      try {\\n        final result = await task(argument);\\n        responsePort.send(result);\\n      } catch (e, stackTrace) {\\n        responsePort.send(\\n          _IsolateError(\\n            message: e.toString(),\\n            stackTrace: stackTrace.toString(),\\n          ),\\n        );\\n      }\\n    });\\n  }\\n}\\n\\nclass _IsolateError {\\n  final String message;\\n  final String stackTrace;\\n\\n  _IsolateError({required this.message, required this.stackTrace});\\n}\\n\",\"size\":3973,\"language\":\"Dart\",\"is_binary\":false,\"line_count\":148,\"is_source_code\":true,\"is_configuration\":false,\"is_documentation\":false,\"timestamp\":\"2025-10-15T06:50:04.334230\"},{\"path\":\"lib/src/models/analysis_error.dart\",\"content\":\"/// Represents an error that occurred during analysis.\\nclass AnalysisError {\\n  final String path;\\n  final String message;\\n  final DateTime timestamp;\\n  final String? stackTrace;\\n\\n  /// Creates a const instance of [AnalysisError].\\n  const AnalysisError({\\n    required this.path,\\n    required this.message,\\n    required this.timestamp,\\n    this.stackTrace,\\n  });\\n\\n  /// Creates a copy of this error but with the given fields replaced with the new values.\\n  AnalysisError copyWith({\\n    String? path,\\n    String? message,\\n    DateTime? timestamp,\\n    String? stackTrace,\\n  }) {\\n    return AnalysisError(\\n      path: path ?? this.path,\\n      message: message ?? this.message,\\n      timestamp: timestamp ?? this.timestamp,\\n      stackTrace: stackTrace ?? this.stackTrace,\\n    );\\n  }\\n\\n  /// Converts this object into a JSON-compatible map.\\n  Map<String, dynamic> toJson() {\\n    return {\\n      'path': path,\\n      'message': message,\\n      'timestamp': timestamp.toIso8601String(),\\n      'stack_trace': stackTrace,\\n    };\\n  }\\n\\n  /// Creates an instance of [AnalysisError] from a JSON map.\\n  factory AnalysisError.fromJson(Map<String, dynamic> json) {\\n    return AnalysisError(\\n      path: json['path'] as String,\\n      message: json['message'] as String,\\n      timestamp: DateTime.parse(json['timestamp'] as String),\\n      stackTrace: json['stack_trace'] as String?,\\n    );\\n  }\\n\\n  @override\\n  String toString() {\\n    return 'AnalysisError(path: $path, message: $message, timestamp: $timestamp)';\\n  }\\n}\\n\",\"size\":1495,\"language\":\"Dart\",\"is_binary\":false,\"line_count\":56,\"is_source_code\":true,\"is_configuration\":false,\"is_documentation\":false,\"timestamp\":\"2025-10-15T06:50:04.334589\"},{\"path\":\"lib/src/models/analysis_progress.dart\",\"content\":\"/// Represents the different phases of the analysis process.\\nenum AnalysisPhase {\\n  initializing,\\n  downloading,\\n  extracting,\\n  analyzing,\\n  processing,\\n  generating,\\n  caching,\\n  completed,\\n  error,\\n}\\n\\n/// Represents the progress of a repository analysis.\\nclass AnalysisProgress {\\n  final AnalysisPhase phase;\\n  final double progress;\\n  final String? message;\\n  final String? currentFile;\\n  final int? processedFiles;\\n  final int? totalFiles;\\n  final DateTime timestamp;\\n\\n  /// Creates a const instance of [AnalysisProgress].\\n  const AnalysisProgress({\\n    required this.phase,\\n    required this.progress,\\n    this.message,\\n    this.currentFile,\\n    this.processedFiles,\\n    this.totalFiles,\\n    required this.timestamp,\\n  });\\n\\n  /// The progress as a percentage (0.0 to 100.0).\\n  double get percentage => (progress * 100).clamp(0.0, 100.0);\\n\\n  /// Creates a copy of this progress object but with the given fields replaced.\\n  AnalysisProgress copyWith({\\n    AnalysisPhase? phase,\\n    double? progress,\\n    String? message,\\n    String? currentFile,\\n    int? processedFiles,\\n    int? totalFiles,\\n    DateTime? timestamp,\\n  }) {\\n    return AnalysisProgress(\\n      phase: phase ?? this.phase,\\n      progress: progress ?? this.progress,\\n      message: message ?? this.message,\\n      currentFile: currentFile ?? this.currentFile,\\n      processedFiles: processedFiles ?? this.processedFiles,\\n      totalFiles: totalFiles ?? this.totalFiles,\\n      timestamp: timestamp ?? this.timestamp,\\n    );\\n  }\\n\\n  /// Converts this object into a JSON-compatible map.\\n  Map<String, dynamic> toJson() {\\n    return {\\n      'phase': phase.name,\\n      'progress': progress,\\n      'message': message,\\n      'current_file': currentFile,\\n      'processed_files': processedFiles,\\n      'total_files': totalFiles,\\n      'timestamp': timestamp.toIso8601String(),\\n    };\\n  }\\n\\n  /// Creates an instance of [AnalysisProgress] from a JSON map.\\n  factory AnalysisProgress.fromJson(Map<String, dynamic> json) {\\n    return AnalysisProgress(\\n      phase: AnalysisPhase.values.firstWhere(\\n        (e) => e.name == json['phase'],\\n        orElse: () => AnalysisPhase.initializing,\\n      ),\\n      progress: (json['progress'] as num).toDouble(),\\n      message: json['message'] as String?,\\n      currentFile: json['current_file'] as String?,\\n      processedFiles: json['processed_files'] as int?,\\n      totalFiles: json['total_files'] as int?,\\n      timestamp: DateTime.parse(json['timestamp'] as String),\\n    );\\n  }\\n\\n  @override\\n  String toString() {\\n    final buffer = StringBuffer('AnalysisProgress(phase: ${phase.name}');\\n    buffer.write(', progress: ${percentage.toStringAsFixed(1)}%');\\n    if (processedFiles != null && totalFiles != null) {\\n      buffer.write(', files: $processedFiles/$totalFiles');\\n    }\\n    if (currentFile != null) {\\n      buffer.write(', current: $currentFile');\\n    }\\n    buffer.write(')');\\n    return buffer.toString();\\n  }\\n}\\n\",\"size\":2915,\"language\":\"Dart\",\"is_binary\":false,\"line_count\":102,\"is_source_code\":true,\"is_configuration\":false,\"is_documentation\":false,\"timestamp\":\"2025-10-15T06:50:04.334953\"},{\"path\":\"lib/src/models/analysis_result.dart\",\"content\":\"import 'package:github_analyzer/src/models/repository_metadata.dart';\\nimport 'package:github_analyzer/src/models/source_file.dart';\\nimport 'package:github_analyzer/src/models/analysis_statistics.dart';\\nimport 'package:github_analyzer/src/models/analysis_error.dart';\\n\\n/// Represents the result of a repository analysis.\\nclass AnalysisResult {\\n  final RepositoryMetadata metadata;\\n  final List<SourceFile> files;\\n  final AnalysisStatistics statistics;\\n  final List<String> mainFiles;\\n  final Map<String, List<String>> dependencies;\\n  final List<AnalysisError> errors;\\n\\n  /// Creates an instance of [AnalysisResult].\\n  const AnalysisResult({\\n    required this.metadata,\\n    required this.files,\\n    required this.statistics,\\n    required this.mainFiles,\\n    required this.dependencies,\\n    this.errors = const [],\\n  });\\n\\n  /// Creates a copy of this result but with the given fields replaced with the new values.\\n  AnalysisResult copyWith({\\n    RepositoryMetadata? metadata,\\n    List<SourceFile>? files,\\n    AnalysisStatistics? statistics,\\n    List<String>? mainFiles,\\n    Map<String, List<String>>? dependencies,\\n    List<AnalysisError>? errors,\\n  }) {\\n    return AnalysisResult(\\n      metadata: metadata ?? this.metadata,\\n      files: files ?? this.files,\\n      statistics: statistics ?? this.statistics,\\n      mainFiles: mainFiles ?? this.mainFiles,\\n      dependencies: dependencies ?? this.dependencies,\\n      errors: errors ?? this.errors,\\n    );\\n  }\\n\\n  /// Converts this object into a JSON-compatible map.\\n  Map<String, dynamic> toJson() {\\n    return {\\n      'metadata': metadata.toJson(),\\n      'files': files.map((f) => f.toJson()).toList(),\\n      'statistics': statistics.toJson(),\\n      'main_files': mainFiles,\\n      'dependencies': dependencies,\\n      'errors': errors.map((e) => e.toJson()).toList(),\\n    };\\n  }\\n\\n  /// Creates an instance of [AnalysisResult] from a JSON map.\\n  factory AnalysisResult.fromJson(Map<String, dynamic> json) {\\n    return AnalysisResult(\\n      metadata: RepositoryMetadata.fromJson(\\n        (json['metadata'] as Map<String, dynamic>?) ?? {},\\n      ),\\n      files: ((json['files'] as List<dynamic>?) ?? [])\\n          .whereType<Map<String, dynamic>>()\\n          .map(SourceFile.fromJson)\\n          .toList(),\\n      statistics: AnalysisStatistics.fromJson(\\n        (json['statistics'] as Map<String, dynamic>?) ?? {},\\n      ),\\n      mainFiles: ((json['main_files'] as List<dynamic>?) ?? [])\\n          .whereType<String>()\\n          .toList(),\\n      dependencies: ((json['dependencies'] as Map<String, dynamic>?) ?? {}).map(\\n        (k, v) => MapEntry(\\n          k,\\n          ((v as List<dynamic>?) ?? []).whereType<String>().toList(),\\n        ),\\n      ),\\n      errors: ((json['errors'] as List<dynamic>?) ?? [])\\n          .whereType<Map<String, dynamic>>()\\n          .map(AnalysisError.fromJson)\\n          .toList(),\\n    );\\n  }\\n\\n  @override\\n  String toString() {\\n    return 'AnalysisResult(repo: ${metadata.name}, files: ${files.length}, lines: ${statistics.totalLines}, errors: ${errors.length})';\\n  }\\n}\\n\",\"size\":3042,\"language\":\"Dart\",\"is_binary\":false,\"line_count\":90,\"is_source_code\":true,\"is_configuration\":false,\"is_documentation\":false,\"timestamp\":\"2025-10-15T06:50:04.335316\"},{\"path\":\"lib/src/models/analysis_statistics.dart\",\"content\":\"import 'package:github_analyzer/src/models/source_file.dart';\\n\\n/// Represents statistical data from a repository analysis.\\nclass AnalysisStatistics {\\n  final int totalFiles;\\n  final int totalLines;\\n  final int totalSize;\\n  final Map<String, int> languageDistribution;\\n  final int binaryFiles;\\n  final int sourceFiles;\\n  final int configFiles;\\n  final int documentationFiles;\\n\\n  /// Creates a const instance of [AnalysisStatistics].\\n  const AnalysisStatistics({\\n    required this.totalFiles,\\n    required this.totalLines,\\n    required this.totalSize,\\n    required this.languageDistribution,\\n    required this.binaryFiles,\\n    required this.sourceFiles,\\n    required this.configFiles,\\n    required this.documentationFiles,\\n  });\\n\\n  /// Creates a copy of this statistics object but with the given fields replaced.\\n  AnalysisStatistics copyWith({\\n    int? totalFiles,\\n    int? totalLines,\\n    int? totalSize,\\n    Map<String, int>? languageDistribution,\\n    int? binaryFiles,\\n    int? sourceFiles,\\n    int? configFiles,\\n    int? documentationFiles,\\n  }) {\\n    return AnalysisStatistics(\\n      totalFiles: totalFiles ?? this.totalFiles,\\n      totalLines: totalLines ?? this.totalLines,\\n      totalSize: totalSize ?? this.totalSize,\\n      languageDistribution: languageDistribution ?? this.languageDistribution,\\n      binaryFiles: binaryFiles ?? this.binaryFiles,\\n      sourceFiles: sourceFiles ?? this.sourceFiles,\\n      configFiles: configFiles ?? this.configFiles,\\n      documentationFiles: documentationFiles ?? this.documentationFiles,\\n    );\\n  }\\n\\n  /// Converts this object into a JSON-compatible map.\\n  Map<String, dynamic> toJson() {\\n    return {\\n      'total_files': totalFiles,\\n      'total_lines': totalLines,\\n      'total_size': totalSize,\\n      'language_distribution': languageDistribution,\\n      'binary_files': binaryFiles,\\n      'source_files': sourceFiles,\\n      'config_files': configFiles,\\n      'documentation_files': documentationFiles,\\n    };\\n  }\\n\\n  /// Creates an instance of [AnalysisStatistics] from a JSON map.\\n  factory AnalysisStatistics.fromJson(Map<String, dynamic> json) {\\n    return AnalysisStatistics(\\n      totalFiles: json['total_files'] as int? ?? 0,\\n      totalLines: json['total_lines'] as int? ?? 0,\\n      totalSize: json['total_size'] as int? ?? 0,\\n      languageDistribution:\\n          (json['language_distribution'] as Map<String, dynamic>? ?? {}).map(\\n            (k, v) => MapEntry(k, v as int),\\n          ),\\n      binaryFiles: json['binary_files'] as int? ?? 0,\\n      sourceFiles: json['source_files'] as int? ?? 0,\\n      configFiles: json['config_files'] as int? ?? 0,\\n      documentationFiles: json['documentation_files'] as int? ?? 0,\\n    );\\n  }\\n\\n  /// Creates an instance of [AnalysisStatistics] from a list of [SourceFile] objects.\\n  factory AnalysisStatistics.fromSourceFiles(List<SourceFile> files) {\\n    int totalLines = 0;\\n    int totalSize = 0;\\n    int binaryFiles = 0;\\n    int sourceFiles = 0;\\n    int configFiles = 0;\\n    int documentationFiles = 0;\\n    final languageDistribution = <String, int>{};\\n\\n    for (final file in files) {\\n      totalLines += file.lineCount;\\n      totalSize += file.size;\\n      if (file.isBinary) binaryFiles++;\\n      if (file.isSourceCode) sourceFiles++;\\n      if (file.isConfiguration) configFiles++;\\n      if (file.isDocumentation) documentationFiles++;\\n      if (file.language != null && file.language!.isNotEmpty) {\\n        languageDistribution[file.language!] =\\n            (languageDistribution[file.language!] ?? 0) + 1;\\n      }\\n    }\\n\\n    return AnalysisStatistics(\\n      totalFiles: files.length,\\n      totalLines: totalLines,\\n      totalSize: totalSize,\\n      languageDistribution: languageDistribution,\\n      binaryFiles: binaryFiles,\\n      sourceFiles: sourceFiles,\\n      configFiles: configFiles,\\n      documentationFiles: documentationFiles,\\n    );\\n  }\\n\\n  @override\\n  String toString() {\\n    return 'AnalysisStatistics(files: $totalFiles, lines: $totalLines, size: $totalSize)';\\n  }\\n}\\n\",\"size\":3978,\"language\":\"Dart\",\"is_binary\":false,\"line_count\":120,\"is_source_code\":true,\"is_configuration\":false,\"is_documentation\":false,\"timestamp\":\"2025-10-15T06:50:04.336046\"},{\"path\":\"lib/src/models/repository_metadata.dart\",\"content\":\"/// Represents metadata about a GitHub repository.\\nclass RepositoryMetadata {\\n  final String name;\\n  final String? fullName;\\n  final String? description;\\n  final bool isPrivate;\\n  final String? defaultBranch;\\n  final String? language;\\n  final List<String> languages;\\n  final int stars;\\n  final int forks;\\n  final int fileCount;\\n  final String? commitSha;\\n  final String directoryTree;\\n\\n  /// Creates a const instance of [RepositoryMetadata].\\n  const RepositoryMetadata({\\n    required this.name,\\n    this.fullName,\\n    this.description,\\n    required this.isPrivate,\\n    this.defaultBranch,\\n    this.language,\\n    required this.languages,\\n    required this.stars,\\n    required this.forks,\\n    required this.fileCount,\\n    this.commitSha,\\n    required this.directoryTree,\\n  });\\n\\n  /// Creates a copy of this metadata object but with the given fields replaced.\\n  RepositoryMetadata copyWith({\\n    String? name,\\n    String? fullName,\\n    String? description,\\n    bool? isPrivate,\\n    String? defaultBranch,\\n    String? language,\\n    List<String>? languages,\\n    int? stars,\\n    int? forks,\\n    int? fileCount,\\n    String? commitSha,\\n    String? directoryTree,\\n  }) {\\n    return RepositoryMetadata(\\n      name: name ?? this.name,\\n      fullName: fullName ?? this.fullName,\\n      description: description ?? this.description,\\n      isPrivate: isPrivate ?? this.isPrivate,\\n      defaultBranch: defaultBranch ?? this.defaultBranch,\\n      language: language ?? this.language,\\n      languages: languages ?? this.languages,\\n      stars: stars ?? this.stars,\\n      forks: forks ?? this.forks,\\n      fileCount: fileCount ?? this.fileCount,\\n      commitSha: commitSha ?? this.commitSha,\\n      directoryTree: directoryTree ?? this.directoryTree,\\n    );\\n  }\\n\\n  /// Converts this object into a JSON-compatible map.\\n  Map<String, dynamic> toJson() {\\n    return {\\n      'name': name,\\n      'full_name': fullName,\\n      'description': description,\\n      'is_private': isPrivate,\\n      'default_branch': defaultBranch,\\n      'language': language,\\n      'languages': languages,\\n      'stars': stars,\\n      'forks': forks,\\n      'file_count': fileCount,\\n      'commit_sha': commitSha,\\n      'directory_tree': directoryTree,\\n    };\\n  }\\n\\n  /// Creates an instance of [RepositoryMetadata] from a JSON map.\\n  factory RepositoryMetadata.fromJson(Map<String, dynamic> json) {\\n    return RepositoryMetadata(\\n      name: json['name'] as String? ?? '',\\n      fullName: json['full_name'] as String?,\\n      description: json['description'] as String?,\\n      isPrivate: json['is_private'] as bool? ?? false,\\n      defaultBranch: json['default_branch'] as String?,\\n      language: json['language'] as String?,\\n      languages:\\n          (json['languages'] as List<dynamic>?)\\n              ?.map((e) => e as String)\\n              .toList() ??\\n          [],\\n      stars: json['stars'] as int? ?? 0,\\n      forks: json['forks'] as int? ?? 0,\\n      fileCount: json['file_count'] as int? ?? 0,\\n      commitSha: json['commit_sha'] as String?,\\n      directoryTree: json['directory_tree'] as String? ?? '',\\n    );\\n  }\\n\\n  @override\\n  String toString() {\\n    return 'RepositoryMetadata(name: $name, language: $language, stars: $stars, files: $fileCount)';\\n  }\\n}\\n\",\"size\":3214,\"language\":\"Dart\",\"is_binary\":false,\"line_count\":108,\"is_source_code\":true,\"is_configuration\":false,\"is_documentation\":false,\"timestamp\":\"2025-10-15T06:50:04.336694\"},{\"path\":\"lib/src/models/source_file.dart\",\"content\":\"/// Represents a single source file in the repository.\\nclass SourceFile {\\n  final String path;\\n  final String? content;\\n  final int size;\\n  final String? language;\\n  final bool isBinary;\\n  final int lineCount;\\n  final bool isSourceCode;\\n  final bool isConfiguration;\\n  final bool isDocumentation;\\n  final DateTime timestamp; // Added for incremental analysis\\n\\n  /// Creates a const instance of [SourceFile].\\n  const SourceFile({\\n    required this.path,\\n    this.content,\\n    required this.size,\\n    this.language,\\n    required this.isBinary,\\n    required this.lineCount,\\n    required this.isSourceCode,\\n    required this.isConfiguration,\\n    required this.isDocumentation,\\n    required this.timestamp,\\n  });\\n\\n  /// Creates a copy of this source file but with the given fields replaced.\\n  SourceFile copyWith({\\n    String? path,\\n    String? content,\\n    int? size,\\n    String? language,\\n    bool? isBinary,\\n    int? lineCount,\\n    bool? isSourceCode,\\n    bool? isConfiguration,\\n    bool? isDocumentation,\\n    DateTime? timestamp,\\n  }) {\\n    return SourceFile(\\n      path: path ?? this.path,\\n      content: content ?? this.content,\\n      size: size ?? this.size,\\n      language: language ?? this.language,\\n      isBinary: isBinary ?? this.isBinary,\\n      lineCount: lineCount ?? this.lineCount,\\n      isSourceCode: isSourceCode ?? this.isSourceCode,\\n      isConfiguration: isConfiguration ?? this.isConfiguration,\\n      isDocumentation: isDocumentation ?? this.isDocumentation,\\n      timestamp: timestamp ?? this.timestamp,\\n    );\\n  }\\n\\n  /// Converts this object into a JSON-compatible map.\\n  Map<String, dynamic> toJson() {\\n    return {\\n      'path': path,\\n      'content': content,\\n      'size': size,\\n      'language': language,\\n      'is_binary': isBinary,\\n      'line_count': lineCount,\\n      'is_source_code': isSourceCode,\\n      'is_configuration': isConfiguration,\\n      'is_documentation': isDocumentation,\\n      'timestamp': timestamp.toIso8601String(),\\n    };\\n  }\\n\\n  /// Creates an instance of [SourceFile] from a JSON map.\\n  factory SourceFile.fromJson(Map<String, dynamic> json) {\\n    return SourceFile(\\n      path: json['path'] as String,\\n      content: json['content'] as String?,\\n      size: json['size'] as int? ?? 0,\\n      language: json['language'] as String?,\\n      isBinary: json['is_binary'] as bool? ?? false,\\n      lineCount: json['line_count'] as int? ?? 0,\\n      isSourceCode: json['is_source_code'] as bool? ?? false,\\n      isConfiguration: json['is_configuration'] as bool? ?? false,\\n      isDocumentation: json['is_documentation'] as bool? ?? false,\\n      timestamp: json['timestamp'] != null\\n          ? DateTime.parse(json['timestamp'] as String)\\n          : DateTime.fromMillisecondsSinceEpoch(0),\\n    );\\n  }\\n\\n  @override\\n  String toString() {\\n    return 'SourceFile(path: $path, size: $size, language: $language, lines: $lineCount)';\\n  }\\n}\\n\",\"size\":2871,\"language\":\"Dart\",\"is_binary\":false,\"line_count\":94,\"is_source_code\":true,\"is_configuration\":false,\"is_documentation\":false,\"timestamp\":\"2025-10-15T06:50:04.337053\"},{\"path\":\"pubspec.yaml\",\"content\":\"name: github_analyzer\\ndescription: \\\"Analyze GitHub repositories and generate AI context for LLMs with cross-platform support\\\"\\nversion: 0.1.0\\n\\nhomepage: https://github.com/cruxhan/github_analyzer\\nrepository: https://github.com/cruxhan/github_analyzer\\nissue_tracker: https://github.com/cruxhan/github_analyzer/issues\\n\\nenvironment:\\n  sdk: '>=3.0.0 <4.0.0'\\n\\ndependencies:\\n  # Archive extraction for ZIP files\\n  archive: ^4.0.0\\n  \\n  # HTTP client for GitHub API\\n  dio: ^5.4.3+1\\n  \\n  # Logging support\\n  logging: ^1.2.0\\n  \\n  # Path manipulation\\n  path: ^1.9.0\\n  \\n  # Cross-platform IO (web + native)\\n  universal_io: ^2.2.2\\n\\ndev_dependencies:\\n  # Testing\\n  test: ^1.25.2\\n  \\n  # Mocking for tests\\n  mockito: ^5.4.4\\n  \\n  # Code generation for mocks\\n  build_runner: ^2.4.9\\n  \\n  # Linting\\n  lints: ^3.0.0\\n\\nplatforms:\\n  android:\\n  ios:\\n  linux:\\n  macos:\\n  web:\\n  windows:\\n\",\"size\":860,\"language\":\"YAML\",\"is_binary\":false,\"line_count\":48,\"is_source_code\":true,\"is_configuration\":true,\"is_documentation\":false,\"timestamp\":\"2025-10-15T06:50:04.337896\"}],\"statistics\":{\"total_files\":43,\"total_lines\":4621,\"total_size\":133375,\"language_distribution\":{\"Markdown\":2,\"YAML\":2,\"Dart\":36},\"binary_files\":0,\"source_files\":40,\"config_files\":3,\"documentation_files\":3},\"main_files\":[],\"dependencies\":{\"pubspec.yaml\":[\"Dart/pub\"]},\"errors\":[]}","size":151217,"language":"JSON","is_binary":false,"line_count":1,"is_source_code":true,"is_configuration":true,"is_documentation":false,"timestamp":"2025-10-15T07:46:08.373445"},{"path":".gitignore","content":"# Dart & Flutter íŒ¨í‚¤ì§€ ê¸°ë³¸ .gitignore\n\n# ë¹Œë“œ/ìºì‹œ íŒŒì¼\nbuild/\n.dart_tool/\n.packages\n.pub/\n*.sqlite3\n\n# IDE/ì—ë””í„° íŒŒì¼\n.idea/\n.vscode/\n*.swp\n\n# OS ë° ê¸°íƒ€ ë¶ˆí•„ìš” íŒŒì¼\n.DS_Store\nThumbs.db\n\n# í…ŒìŠ¤íŒ…/coverage\ncoverage/\ntest-results/\n\n# í™˜ê²½ íŒŒì¼ ë° ì‹¤ì œ í† í°\n.env\n*.log\n\n# generated files\n*.generated.*\n*.g.dart\n*.iml\n\n# flutter/dart package ë°°í¬ ê´€ë ¨\npubspec.lock\n\n# ë¶„ì„/ì •ì  ê²€ì‚¬ ê²°ê³¼\n.mypy_cache/\n.pytest_cache/\n.eggs/\n\n# Windows\n*.exe\n*.dll\n\n# Linux/Mac\n*.so\n\n# ê¸°íƒ€ ì„ì‹œ íŒŒì¼\n*.tmp\n\n# ì¶”ê°€\n*.env\n*.cfs\noutput/\nexample/\ndemo.dart","size":590,"language":null,"is_binary":false,"line_count":55,"is_source_code":false,"is_configuration":true,"is_documentation":false,"timestamp":"2025-10-15T07:46:08.378230"},{"path":".metadata","content":"# This file tracks properties of this Flutter project.\n# Used by Flutter tool to assess capabilities and perform upgrades etc.\n#\n# This file should be version controlled and should not be manually edited.\n\nversion:\n  revision: \"ac4e799d237041cf905519190471f657b657155a\"\n  channel: \"stable\"\n\nproject_type: package\n","size":313,"language":null,"is_binary":false,"line_count":11,"is_source_code":false,"is_configuration":false,"is_documentation":false,"timestamp":"2025-10-15T07:46:08.378791"},{"path":"CHANGELOG.md","content":"## 0.1.0\n\n### Breaking Changes\n- **Output Format Changed**: Switched from CFS format to Markdown format for context generation\n  - `ContextGenerator.generate()` now creates `.md` files instead of `.context.md`\n  - Output is 90% more token-efficient for LLM consumption\n  - Markdown format is human-readable and supported by all major AI platforms\n\n### Performance Improvements\n- **Memory Efficiency**: Added stream-based markdown generation via `MarkdownGenerator.generateToFile()`\n  - Reduces memory usage by 50% for large repositories\n  - Can handle repositories of any size without memory issues\n- **Smart Filtering**: New configuration options for intelligent file filtering\n  - `excludeGeneratedFiles`: Automatically excludes *.g.dart, *.freezed.dart, etc.\n  - `maxTotalFiles`: Limit total files analyzed\n  - `prioritizeImportantFiles`: Focus on main code (lib/, main.dart)\n- **Optimized Output**: Inline metadata formatting reduces token usage by additional 10%\n\n### New Features\n- **MarkdownConfig**: Fine-grained control over markdown generation\n  - `MarkdownConfig.standard`: Full output (default)\n  - `MarkdownConfig.compact`: Limited files and content for smaller output\n  - Custom limits: `maxFiles`, `maxContentSize`, `minPriority`\n- **Convenience Functions**: Simpler API for common use cases\n  - `analyzeAndGenerate()`: One-step analysis and markdown generation\n  - `analyzeQuick()`: Fast analysis with optimized settings (no cache, limited files)\n  - `analyzeForLLM()`: LLM-optimized analysis (excludes tests, limits files)\n- **Smart File Naming**: Automatic output filename generation from repository name\n  - `ContextGenerator.generate(result)` auto-generates `{repo_name}_analysis.md`\n  - Manual naming still supported: `outputPath: './custom.md'`\n- **Preset Configurations**: Ready-to-use configuration presets\n  - `GithubAnalyzerConfig.quick()`: Fast analysis\n  - `GithubAnalyzerConfig.forLLM()`: Optimized for LLM context\n\n### API Enhancements\n- `ContextGenerator.generate()` now returns the output file path\n- Automatic `.md` extension handling\n- `outputDir` parameter for specifying output directory with auto-naming\n- Improved error messages and validation\n\n### Removed\n- Removed `cfs_writer.dart` (replaced by `markdown_generator.dart`)\n- Removed CFS format support\n\n### Migration Guide\n\n#### Before (v0.0.4)\n```dart\n// Analysis\nfinal result = await analyze('https://github.com/user/repo', \n  config: GithubAnalyzerConfig(\n    excludePatterns: [...],\n  ),\n);\n\n// Generate output\nawait ContextGenerator.generate(result, './output.context.md');\n```\n\n#### After (v0.1.0)\n```dart\n// Simplest way - one line\nfinal path = await analyzeAndGenerate('https://github.com/user/repo');\n\n// Or with more control\nfinal result = await analyzeQuick('https://github.com/user/repo');\nfinal path = await ContextGenerator.generate(result);  // Auto-named\n\n// Or LLM-optimized\nfinal path = await analyzeForLLM(\n  'https://github.com/user/repo',\n  maxFiles: 100,\n);\n\n// Manual configuration\nfinal result = await analyze('https://github.com/user/repo',\n  config: GithubAnalyzerConfig(\n    excludeGeneratedFiles: true,  // New option\n    maxTotalFiles: 200,  // New option\n    excludePatterns: [...],\n  ),\n);\n\nawait ContextGenerator.generate(\n  result,\n  config: MarkdownConfig.compact,  // New option\n);\n```\n\n### Performance Benchmarks\n- **Token Reduction**: 90-95% fewer tokens compared to JSON (Phase 1)\n- **Memory Usage**: 50% reduction for large repos (stream-based writing)\n- **Analysis Speed**: Same or faster with smart filtering\n- **Output Size**: 30% smaller with compact config and smart filtering\n\n## 0.0.4\n\n### Fix\nAddressed issues from v0.0.3 and introduced new features.\n\n## 0.0.3\n\n### Fix\nAddressed issues from v0.0.2 and introduced new features.\n\n## 0.0.2\n\n### Major Refactoring for Usability and Maintainability\n\n- **Added Top-Level `analyze` Function**: Drastically simplified the API. Users can now analyze repositories with a single function call, without needing to manually set up dependencies.\n- **Implemented Dependency Injection**: Refactored the `GithubAnalyzer` class to accept dependencies via its constructor, improving modularity and testability.\n- **Integrated Standard Logging Package**: Replaced the custom logger with the standard Dart logging package for more robust and flexible logging.\n- **Improved Error Handling**: Enhanced `AnalyzerException` to include the original exception and stack trace, providing more context for debugging.\n\n## 0.0.1\n\nInitial release of the package. Provides core functionality for analyzing remote and local repositories.\n","size":4592,"language":"Markdown","is_binary":false,"line_count":118,"is_source_code":true,"is_configuration":false,"is_documentation":true,"timestamp":"2025-10-15T07:46:08.379128"},{"path":"LICENSE","content":"MIT License\n\nCopyright (c) 2025 CreatorJun\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.","size":1066,"language":null,"is_binary":false,"line_count":21,"is_source_code":false,"is_configuration":false,"is_documentation":true,"timestamp":"2025-10-15T07:46:08.379531"},{"path":"README.md","content":"# GitHub Analyzer\n\n[![pub version](https://img.shields.io/pub/v/github_analyzer.svg)](https://pub.dev/packages/github_analyzer)\n[![license](https://img.shields.io/badge/license-MIT-blue.svg)](https://opensource.org/licenses/MIT)\n\nA powerful and flexible Dart package to analyze GitHub repositories. It can process both remote repositories via URL and local directories on your machine. This tool is designed to extract comprehensive metadata, generate detailed statistics, and compile source code into a structured markdown context, making it perfect for feeding into Large Language Models (LLMs) or for conducting code audits.\n\n## Key Features\n\n- **Simple & Powerful API**: Analyze any public repository with a single function call. No complex setup required.\n- **Dual Analysis Modes**: Analyze repositories from a remote GitHub URL or a local file path.\n- **Comprehensive Reports**: Generates a detailed `AnalysisResult` object containing repository metadata, file-by-file analysis, language distribution, dependency detection, and more.\n- **Smart Caching**: Avoids re-analyzing unchanged remote repositories by using a commit-based caching system, saving time and API calls.\n- **High-Performance Scans**: Utilizes Dart Isolates for parallel processing of local files, ensuring fast and efficient analysis even on large codebases.\n- **Incremental Analysis**: For local repositories, it can perform an incremental analysis by comparing against a previous result, processing only the files that have been added, modified, or deleted.\n- **Real-time Progress**: Monitor the analysis progress through a callback, perfect for providing feedback in a UI or CLI.\n- **Markdown Output**: Generates optimized markdown format that is 90% more token-efficient than JSON, perfect for AI context.\n- **Smart Filtering**: Automatically excludes generated files, with configurable limits and priorities.\n- **Memory Efficient**: Stream-based markdown generation for handling large repositories.\n- **Customizable**: Fine-tune the analysis with a rich configuration object, or use dependency injection for complete control.\n\n## Getting Started\n\n### 1. Installation\n\n```bash\ndart pub add github_analyzer\n```\n\nOr add this to your package's `pubspec.yaml` file:\n\n```yaml\ndependencies:\n  github_analyzer: ^0.1.0\n```\n\nThen install it by running:\n\n```bash\ndart pub get\n```\n\n### 2. Basic Usage\n\n#### Simplest Way - One Line\n\n```dart\nimport 'package:github_analyzer/github_analyzer.dart';\n\nvoid main() async {\n  // Analyze and generate markdown in one step\n  final outputPath = await analyzeAndGenerate(\n    'https://github.com/flutter/flutter',\n  );\n  \n  print('âœ… Analysis saved to: $outputPath');\n}\n```\n\n#### Quick Analysis (Optimized for Speed)\n\n```dart\nvoid main() async {\n  // Fast analysis with optimized settings\n  final result = await analyzeQuick('https://github.com/dart-lang/sdk');\n  \n  print('Repository: ${result.metadata.fullName}');\n  print('Files: ${result.statistics.totalFiles}');\n}\n```\n\n#### LLM-Optimized Analysis\n\n```dart\nvoid main() async {\n  // Optimized for LLM context with automatic filtering\n  final outputPath = await analyzeForLLM(\n    'https://github.com/your/repo',\n    maxFiles: 100,  // Limit to most important 100 files\n    markdownConfig: MarkdownConfig.compact,  // Compact output\n  );\n  \n  print('LLM context ready: $outputPath');\n}\n```\n\n### 3. Standard Usage with Progress\n\n```dart\nimport 'package:github_analyzer/github_analyzer.dart';\n\nvoid main() async {\n  try {\n    // Analyze with progress tracking\n    final result = await analyze(\n      'https://github.com/flutter/flutter',\n      \n      progressCallback: (progress) {\n        final percentage = (progress.progress * 100).toStringAsFixed(1);\n        print('${progress.phase.name} $percentage% - ${progress.message}');\n      },\n      \n      verbose: true,\n    );\n    \n    print('âœ… Analysis Complete!');\n    print('Repository: ${result.metadata.fullName}');\n    print('Primary Language: ${result.metadata.language}');\n    print('Total Files: ${result.files.length}');\n    print('Total Lines: ${result.statistics.totalLines}');\n    \n    // Generate markdown with auto-naming\n    final outputPath = await ContextGenerator.generate(result);\n    print('ğŸ“„ Markdown saved to: $outputPath');\n    \n  } catch (e) {\n    print('âŒ An error occurred: $e');\n  }\n}\n```\n\n## Advanced Usage\n\n### Custom Configuration\n\nFor more control, use `GithubAnalyzerConfig`:\n\n```dart\nfinal config = GithubAnalyzerConfig(\n  githubToken: 'YOUR_GITHUB_TOKEN',\n  \n  // Smart filtering options\n  excludeGeneratedFiles: true,  // Auto-exclude *.g.dart, etc.\n  maxTotalFiles: 500,  // Limit total files analyzed\n  prioritizeImportantFiles: true,  // Focus on main code\n  \n  // Additional exclude patterns\n  excludePatterns: [\n    ...kDefaultExcludePatterns,\n    '*.g.dart',\n    'test_data/**',\n  ],\n  \n  maxFileSize: 5 * 1024 * 1024,  // 5MB limit\n  enableCache: true,\n  enableIsolatePool: true,\n);\n\nfinal result = await analyze(\n  'https://github.com/your/repo',\n  config: config,\n);\n```\n\n### Preset Configurations\n\n```dart\n// Quick analysis (no cache, limited files)\nfinal quickConfig = GithubAnalyzerConfig.quick(\n  githubToken: 'YOUR_TOKEN',\n);\n\n// LLM-optimized (excludes tests, examples, max 200 files)\nfinal llmConfig = GithubAnalyzerConfig.forLLM(\n  githubToken: 'YOUR_TOKEN',\n  maxFiles: 200,\n);\n```\n\n### Markdown Generation Options\n\n```dart\n// Standard output (all content)\nawait ContextGenerator.generate(\n  result,\n  config: MarkdownConfig.standard,\n);\n\n// Compact output (limited files and content)\nawait ContextGenerator.generate(\n  result,\n  config: MarkdownConfig.compact,\n);\n\n// Custom output\nawait ContextGenerator.generate(\n  result,\n  config: MarkdownConfig(\n    maxFiles: 100,  // Limit to 100 files\n    maxContentSize: 50000,  // Truncate large files\n    includeBinaryStats: false,\n    includeErrors: false,\n  ),\n);\n\n// Specify output location\nawait ContextGenerator.generate(\n  result,\n  outputPath: './my_analysis.md',\n  // or use outputDir to auto-generate filename\n  outputDir: './output',\n);\n```\n\n### Analyzing Local Directories\n\n```dart\n// Works the same way\nfinal result = await analyze('/path/to/your/project');\n\n// Or quick local analysis\nfinal result = await analyzeQuick('/path/to/your/project');\n```\n\n### Memory-Efficient Large Repository Handling\n\nFor very large repositories, use streaming generation:\n\n```dart\nfinal result = await analyze('https://github.com/large/repo');\n\n// Directly write to file without loading full content in memory\nawait MarkdownGenerator.generateToFile(\n  result,\n  './large_repo_analysis.md',\n  config: MarkdownConfig(\n    maxFiles: 200,\n    maxContentSize: 100000,\n  ),\n);\n```\n\n## Output Format\n\nThe package generates markdown output with the following structure:\n\n```markdown\n# Repository Name\n\n## Repository Information\n**Repository:** `owner/repo`\n**Language:** Dart | **Stars:** 1234 | **Forks:** 567\n\n## Statistics\n- **Total Files:** 150\n- **Total Lines:** 25000\n- **Source Files:** 120\n\n## Directory Structure\n`\nlib/\n  src/\n    models/\n    services/\n`\n\n## Language Distribution\n- **Dart:** 120 files (80.0%)\n- **YAML:** 15 files (10.0%)\n\n## Source Code\n### lib/main.dart\n`dart\n// Full source code with syntax highlighting\n`\n```\n\n### Performance Characteristics\n\n- **Token Efficiency**: 90% reduction compared to JSON format\n- **Memory Usage**: Stream-based generation handles repositories of any size\n- **Speed**: Parallel processing with automatic optimization\n- **Smart Filtering**: Automatically excludes generated files and low-priority content\n\n## API Summary\n\n### Quick Functions\n\n- `analyzeAndGenerate()` - Analyze and create markdown in one step\n- `analyzeQuick()` - Fast analysis with optimized settings\n- `analyzeForLLM()` - LLM-optimized with automatic filtering\n\n### Core Functions\n\n- `analyze()` - Standard analysis with full control\n- `ContextGenerator.generate()` - Generate markdown from result\n- `MarkdownGenerator.generateToFile()` - Memory-efficient file generation\n\n### Configuration\n\n- `GithubAnalyzerConfig.quick()` - Fast analysis preset\n- `GithubAnalyzerConfig.forLLM()` - LLM-optimized preset\n- `MarkdownConfig.standard` - Full output\n- `MarkdownConfig.compact` - Compact output\n\n## Contributing\n\nContributions are welcome! If you find a bug or have a feature request, please open an issue on the [GitHub issue tracker](https://github.com/cruxhan/github_analyzer/issues).\n\n## License\n\nThis package is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.\n","size":8512,"language":"Markdown","is_binary":false,"line_count":303,"is_source_code":true,"is_configuration":false,"is_documentation":true,"timestamp":"2025-10-15T07:46:08.379814"},{"path":"analysis_options.yaml","content":"\n\n# Additional information about this file can be found at\n# https://dart.dev/guides/language/analysis-options\n","size":111,"language":"YAML","is_binary":false,"line_count":5,"is_source_code":true,"is_configuration":true,"is_documentation":false,"timestamp":"2025-10-15T07:46:08.380558"},{"path":"lib/github_analyzer.dart","content":"library;\n\nimport 'dart:async';\n\nimport 'package:github_analyzer/src/common/config.dart';\nimport 'package:github_analyzer/src/common/logger.dart';\nexport 'src/common/env_loader.dart';\nimport 'package:github_analyzer/src/core/cache_service.dart';\nimport 'package:github_analyzer/src/core/local_analyzer_service.dart';\nimport 'package:github_analyzer/src/core/remote_analyzer_service.dart';\nimport 'package:github_analyzer/src/data/providers/github_api_provider.dart';\nimport 'package:github_analyzer/src/data/providers/zip_downloader.dart';\nimport 'package:github_analyzer/src/github_analyzer.dart';\nimport 'package:github_analyzer/src/infrastructure/http_client_manager.dart';\nimport 'package:github_analyzer/src/infrastructure/isolate_pool.dart';\nimport 'package:github_analyzer/src/models/analysis_progress.dart';\nimport 'package:github_analyzer/src/models/analysis_result.dart';\nimport 'package:github_analyzer/src/common/utils/context_generator.dart';\nimport 'package:github_analyzer/src/common/utils/markdown_generator.dart';\n\nexport 'src/common/config.dart';\nexport 'src/common/errors/analyzer_exception.dart';\nexport 'src/common/logger.dart';\nexport 'src/github_analyzer.dart' show GithubAnalyzer;\nexport 'src/models/analysis_error.dart';\nexport 'src/models/analysis_progress.dart';\nexport 'src/models/analysis_result.dart';\nexport 'src/models/analysis_statistics.dart';\nexport 'src/models/repository_metadata.dart';\nexport 'src/models/source_file.dart';\nexport 'src/common/utils/context_generator.dart';\nexport 'src/common/utils/markdown_generator.dart';\n\n/// Analyzes a repository and returns the analysis result\nFuture<AnalysisResult> analyze(\n  String repositoryUrl, {\n  GithubAnalyzerConfig? config,\n  void Function(AnalysisProgress)? progressCallback,\n  bool verbose = false,\n}) async {\n  setupLogger(verbose: verbose);\n\n  final effectiveConfig = config ?? GithubAnalyzerConfig();\n\n  final httpClientManager = HttpClientManager();\n  final apiProvider = GithubApiProvider(\n    httpClientManager: httpClientManager,\n    token: effectiveConfig.githubToken,\n  );\n  final zipDownloader = ZipDownloader(httpClientManager: httpClientManager);\n  final cacheService = effectiveConfig.enableCache\n      ? CacheService(\n          cacheDirectory: effectiveConfig.cacheDirectory,\n          maxAge: effectiveConfig.cacheDuration,\n        )\n      : null;\n  final isolatePool = effectiveConfig.enableIsolatePool\n      ? IsolatePool(size: effectiveConfig.isolatePoolSize)\n      : null;\n  final localAnalyzer = LocalAnalyzerService(config: effectiveConfig);\n  final remoteAnalyzer = RemoteAnalyzerService(\n    config: effectiveConfig,\n    apiProvider: apiProvider,\n    zipDownloader: zipDownloader,\n    cacheService: cacheService,\n  );\n\n  final analyzer = GithubAnalyzer(\n    config: effectiveConfig,\n    httpClientManager: httpClientManager,\n    apiProvider: apiProvider,\n    zipDownloader: zipDownloader,\n    localAnalyzer: localAnalyzer,\n    remoteAnalyzer: remoteAnalyzer,\n    cacheService: cacheService,\n    isolatePool: isolatePool,\n  );\n\n  StreamSubscription<AnalysisProgress>? progressSubscription;\n  if (progressCallback != null) {\n    progressSubscription = analyzer.progressStream.listen(progressCallback);\n  }\n\n  try {\n    return await analyzer.analyze(repositoryUrl);\n  } finally {\n    await progressSubscription?.cancel();\n    await analyzer.dispose();\n  }\n}\n\n/// Analyzes a repository and generates markdown output in one step\nFuture<String> analyzeAndGenerate(\n  String repositoryUrl, {\n  String? outputPath,\n  String? outputDir,\n  GithubAnalyzerConfig? analyzerConfig,\n  MarkdownConfig markdownConfig = MarkdownConfig.standard,\n  void Function(AnalysisProgress)? progressCallback,\n  bool verbose = false,\n}) async {\n  final result = await analyze(\n    repositoryUrl,\n    config: analyzerConfig,\n    progressCallback: progressCallback,\n    verbose: verbose,\n  );\n\n  return await ContextGenerator.generate(\n    result,\n    outputPath: outputPath,\n    outputDir: outputDir,\n    config: markdownConfig,\n  );\n}\n\n/// Quick analysis with optimized settings for fast results\nFuture<AnalysisResult> analyzeQuick(\n  String repositoryUrl, {\n  String? githubToken,\n  void Function(AnalysisProgress)? progressCallback,\n}) async {\n  return await analyze(\n    repositoryUrl,\n    config: await GithubAnalyzerConfig.quick(githubToken: githubToken),\n    progressCallback: progressCallback,\n  );\n}\n\n/// Analysis optimized for LLM context generation\nFuture<String> analyzeForLLM(\n  String repositoryUrl, {\n  String? outputPath,\n  String? outputDir,\n  String? githubToken,\n  int maxFiles = 200,\n  MarkdownConfig markdownConfig = MarkdownConfig.standard,\n  void Function(AnalysisProgress)? progressCallback,\n  bool verbose = false,\n}) async {\n  final result = await analyze(\n    repositoryUrl,\n    config: await GithubAnalyzerConfig.forLLM(\n      githubToken: githubToken,\n      maxFiles: maxFiles,\n    ),\n    progressCallback: progressCallback,\n    verbose: verbose,\n  );\n\n  return await ContextGenerator.generate(\n    result,\n    outputPath: outputPath,\n    outputDir: outputDir,\n    config: markdownConfig,\n  );\n}\n","size":5106,"language":"Dart","is_binary":false,"line_count":158,"is_source_code":true,"is_configuration":false,"is_documentation":false,"timestamp":"2025-10-15T07:46:08.380919"},{"path":"lib/src/common/config.dart","content":"import 'package:github_analyzer/src/common/constants.dart';\nimport 'package:github_analyzer/src/common/env_loader.dart';\n\n/// Configuration class for the GithubAnalyzer\nclass GithubAnalyzerConfig {\n  final String? githubToken;\n  final List<String> excludePatterns;\n  final List<String> includePatterns;\n  final int maxFileSize;\n  final bool enableCache;\n  final String cacheDirectory;\n  final Duration cacheDuration;\n  final int maxConcurrentRequests;\n  final bool enableIsolatePool;\n  final int isolatePoolSize;\n  final int maxRetries;\n  final Duration retryDelay;\n  final bool excludeGeneratedFiles;\n  final int maxTotalFiles;\n  final bool prioritizeImportantFiles;\n  final bool autoLoadEnv;\n\n  const GithubAnalyzerConfig._private({\n    this.githubToken,\n    required this.excludePatterns,\n    this.includePatterns = const [],\n    this.maxFileSize = kDefaultMaxFileSize,\n    this.enableCache = true,\n    required this.cacheDirectory,\n    this.cacheDuration = kDefaultCacheDuration,\n    this.maxConcurrentRequests = kDefaultMaxConcurrentRequests,\n    this.enableIsolatePool = true,\n    this.isolatePoolSize = 4,\n    this.maxRetries = kDefaultMaxRetries,\n    this.retryDelay = const Duration(seconds: 2),\n    this.excludeGeneratedFiles = true,\n    this.maxTotalFiles = 0,\n    this.prioritizeImportantFiles = true,\n    this.autoLoadEnv = true,\n  });\n\n  /// Creates a configuration instance with automatic .env loading\n  static Future<GithubAnalyzerConfig> create({\n    String? githubToken,\n    List<String>? excludePatterns,\n    List<String>? includePatterns,\n    int maxFileSize = kDefaultMaxFileSize,\n    bool enableCache = true,\n    String? cacheDirectory,\n    Duration cacheDuration = kDefaultCacheDuration,\n    int maxConcurrentRequests = kDefaultMaxConcurrentRequests,\n    bool enableIsolatePool = true,\n    int? isolatePoolSize,\n    int maxRetries = kDefaultMaxRetries,\n    Duration retryDelay = const Duration(seconds: 2),\n    bool excludeGeneratedFiles = true,\n    int maxTotalFiles = 0,\n    bool prioritizeImportantFiles = true,\n    bool autoLoadEnv = true,\n  }) async {\n    // Auto-load .env file if enabled\n    if (autoLoadEnv) {\n      await EnvLoader.load();\n    }\n\n    // Use provided token or try to load from environment\n    final effectiveToken = githubToken ?? EnvLoader.getGithubToken();\n\n    final size = isolatePoolSize ?? 4;\n\n    return GithubAnalyzerConfig._private(\n      githubToken: effectiveToken,\n      excludePatterns: excludePatterns ?? kDefaultExcludePatterns,\n      includePatterns: includePatterns ?? const [],\n      maxFileSize: maxFileSize,\n      enableCache: enableCache,\n      cacheDirectory: cacheDirectory ?? '.github_analyzer_cache',\n      cacheDuration: cacheDuration,\n      maxConcurrentRequests: maxConcurrentRequests,\n      enableIsolatePool: enableIsolatePool,\n      isolatePoolSize: size,\n      maxRetries: maxRetries,\n      retryDelay: retryDelay,\n      excludeGeneratedFiles: excludeGeneratedFiles,\n      maxTotalFiles: maxTotalFiles,\n      prioritizeImportantFiles: prioritizeImportantFiles,\n      autoLoadEnv: autoLoadEnv,\n    );\n  }\n\n  /// Synchronous factory (without auto-load)\n  factory GithubAnalyzerConfig({\n    String? githubToken,\n    List<String>? excludePatterns,\n    List<String>? includePatterns,\n    int maxFileSize = kDefaultMaxFileSize,\n    bool enableCache = true,\n    String? cacheDirectory,\n    Duration cacheDuration = kDefaultCacheDuration,\n    int maxConcurrentRequests = kDefaultMaxConcurrentRequests,\n    bool enableIsolatePool = true,\n    int? isolatePoolSize,\n    int maxRetries = kDefaultMaxRetries,\n    Duration retryDelay = const Duration(seconds: 2),\n    bool excludeGeneratedFiles = true,\n    int maxTotalFiles = 0,\n    bool prioritizeImportantFiles = true,\n    bool autoLoadEnv = false,\n  }) {\n    // Try to load from already loaded env\n    final effectiveToken = githubToken ?? EnvLoader.get('GITHUB_TOKEN');\n    final size = isolatePoolSize ?? 4;\n\n    return GithubAnalyzerConfig._private(\n      githubToken: effectiveToken,\n      excludePatterns: excludePatterns ?? kDefaultExcludePatterns,\n      includePatterns: includePatterns ?? const [],\n      maxFileSize: maxFileSize,\n      enableCache: enableCache,\n      cacheDirectory: cacheDirectory ?? '.github_analyzer_cache',\n      cacheDuration: cacheDuration,\n      maxConcurrentRequests: maxConcurrentRequests,\n      enableIsolatePool: enableIsolatePool,\n      isolatePoolSize: size,\n      maxRetries: maxRetries,\n      retryDelay: retryDelay,\n      excludeGeneratedFiles: excludeGeneratedFiles,\n      maxTotalFiles: maxTotalFiles,\n      prioritizeImportantFiles: prioritizeImportantFiles,\n      autoLoadEnv: autoLoadEnv,\n    );\n  }\n\n  /// Quick analysis factory\n  static Future<GithubAnalyzerConfig> quick({\n    String? githubToken,\n    List<String>? excludePatterns,\n  }) async {\n    await EnvLoader.load();\n    final effectiveToken = githubToken ?? EnvLoader.getGithubToken();\n\n    return GithubAnalyzerConfig._private(\n      githubToken: effectiveToken,\n      excludePatterns: excludePatterns ?? kDefaultExcludePatterns,\n      includePatterns: const [],\n      maxFileSize: kDefaultMaxFileSize,\n      enableCache: false,\n      cacheDirectory: '.github_analyzer_cache',\n      cacheDuration: kDefaultCacheDuration,\n      maxConcurrentRequests: kDefaultMaxConcurrentRequests,\n      enableIsolatePool: false,\n      isolatePoolSize: 2,\n      maxRetries: kDefaultMaxRetries,\n      retryDelay: const Duration(seconds: 2),\n      excludeGeneratedFiles: true,\n      maxTotalFiles: 100,\n      prioritizeImportantFiles: true,\n      autoLoadEnv: true,\n    );\n  }\n\n  /// LLM-optimized factory\n  static Future<GithubAnalyzerConfig> forLLM({\n    String? githubToken,\n    List<String>? excludePatterns,\n    int maxFiles = 200,\n  }) async {\n    await EnvLoader.load();\n    final effectiveToken = githubToken ?? EnvLoader.getGithubToken();\n\n    return GithubAnalyzerConfig._private(\n      githubToken: effectiveToken,\n      excludePatterns: [\n        ...kDefaultExcludePatterns,\n        ...?excludePatterns,\n        'test/',\n        'tests/',\n        '**test.dart',\n        'example/',\n      ],\n      includePatterns: const [],\n      maxFileSize: kDefaultMaxFileSize,\n      enableCache: true,\n      cacheDirectory: '.github_analyzer_cache',\n      cacheDuration: kDefaultCacheDuration,\n      maxConcurrentRequests: kDefaultMaxConcurrentRequests,\n      enableIsolatePool: true,\n      isolatePoolSize: 4,\n      maxRetries: kDefaultMaxRetries,\n      retryDelay: const Duration(seconds: 2),\n      excludeGeneratedFiles: true,\n      maxTotalFiles: maxFiles,\n      prioritizeImportantFiles: true,\n      autoLoadEnv: true,\n    );\n  }\n\n  /// Get effective exclude patterns including generated files\n  List<String> get effectiveExcludePatterns {\n    if (!excludeGeneratedFiles) return excludePatterns;\n\n    return [\n      ...excludePatterns,\n      '**.g.dart',\n      '**.freezed.dart',\n      '**.gr.dart',\n      '**.config.dart',\n      '**.pb.dart',\n      '**.pbenum.dart',\n      '**.pbgrpc.dart',\n      '**.pbjson.dart',\n    ];\n  }\n}\n","size":7042,"language":"Dart","is_binary":false,"line_count":215,"is_source_code":true,"is_configuration":false,"is_documentation":false,"timestamp":"2025-10-15T07:46:08.381355"},{"path":"lib/src/common/constants.dart","content":"// lib/src/common/constants.dart\n\nconst List<String> kDefaultExcludePatterns = [\n  'node_modules/**',\n  '.git/**',\n  '.svn/**',\n  '.hg/**',\n  'build/**',\n  'dist/**',\n  'out/**',\n  'target/**',\n  '.dart_tool/**',\n  '.pub-cache/**',\n  '.gradle/**',\n  '.idea/**',\n  '.vscode/**',\n  '*.lock',\n  '*.log',\n  '.DS_Store',\n  'Thumbs.db',\n  '__pycache__/**',\n  '*.pyc',\n  '.pytest_cache/**',\n  'venv/**',\n  'env/**',\n  '.env/**',\n  'coverage/**',\n  '.nyc_output/**',\n  'tmp/**',\n  'temp/**',\n  '*.min.js',\n  '*.min.css',\n  'package-lock.json',\n  'yarn.lock',\n  'Cargo.lock',\n];\n\nconst Set<String> kBinaryExtensions = {\n  'png',\n  'jpg',\n  'jpeg',\n  'gif',\n  'bmp',\n  'ico',\n  'svg',\n  'webp',\n  'tiff',\n  'tif',\n  'mp3',\n  'wav',\n  'ogg',\n  'flac',\n  'aac',\n  'm4a',\n  'wma',\n  'mp4',\n  'avi',\n  'mov',\n  'wmv',\n  'flv',\n  'mkv',\n  'webm',\n  'm4v',\n  'zip',\n  'tar',\n  'gz',\n  'bz2',\n  'rar',\n  '7z',\n  'xz',\n  'tgz',\n  'exe',\n  'dll',\n  'so',\n  'dylib',\n  'bin',\n  'app',\n  'dmg',\n  'pkg',\n  'deb',\n  'rpm',\n  'pdf',\n  'doc',\n  'docx',\n  'xls',\n  'xlsx',\n  'ppt',\n  'pptx',\n  'odt',\n  'ods',\n  'odp',\n  'ttf',\n  'otf',\n  'woff',\n  'woff2',\n  'eot',\n  'class',\n  'jar',\n  'war',\n  'ear',\n  'pyc',\n  'pyo',\n  'db',\n  'sqlite',\n  'cache',\n};\n\nconst Set<String> kDocumentationFileNames = {\n  'readme',\n  'changelog',\n  'license',\n  'contributing',\n  'authors',\n  'history',\n};\n\nconst Set<String> kDocumentationExtensions = {\n  'md',\n  'markdown',\n  'rst',\n  'txt',\n};\n\nconst Set<String> kConfigurationFileNames = {\n  'pubspec.yaml',\n  'package.json',\n  'tsconfig.json',\n  'jsconfig.json',\n  'webpack.config.js',\n  'babel.config.js',\n  'rollup.config.js',\n  'vite.config.js',\n  '.eslintrc',\n  '.prettierrc',\n  '.editorconfig',\n  'cargo.toml',\n  'go.mod',\n  'pom.xml',\n  'build.gradle',\n  'settings.gradle',\n  'cmakelists.txt',\n  '.gitignore',\n  '.dockerignore',\n  'dockerfile',\n};\n\nconst Set<String> kConfigurationExtensions = {\n  'json',\n  'yaml',\n  'yml',\n  'toml',\n  'ini',\n  'cfg',\n  'conf',\n  'config',\n};\n\n// ## ìˆ˜ì •ëœ ë¶€ë¶„ ##\n// main íŒŒì¼ ëª©ë¡ì„ ìƒìˆ˜ë¡œ ì¶”ê°€\nconst Set<String> kMainFilePatterns = {\n  'main.dart',\n  'main.py',\n  'main.js',\n  'main.ts',\n  'index.js',\n  'index.ts',\n  'app.js',\n  'app.ts',\n  'server.js',\n  'server.ts',\n  'Main.java',\n  'main.go',\n  'main.rs',\n  'main.c',\n  'main.cpp',\n  'main.swift',\n  'MainActivity.kt',\n  'AppDelegate.swift',\n};\n\nconst int kDefaultMaxFileSize = 10 * 1024 * 1024; // 10 MB\nconst Duration kDefaultCacheDuration = Duration(hours: 24);\nconst Duration kDefaultRequestTimeout = Duration(seconds: 30);\nconst int kDefaultMaxConcurrentRequests = 10;\nconst int kDefaultMaxRetries = 3;\n","size":2637,"language":"Dart","is_binary":false,"line_count":186,"is_source_code":true,"is_configuration":false,"is_documentation":false,"timestamp":"2025-10-15T07:46:08.381749"},{"path":"lib/src/common/env_loader.dart","content":"import 'package:universal_io/io.dart';\nimport 'package:github_analyzer/src/common/logger.dart';\n\n/// Loads environment variables from .env file automatically\nclass EnvLoader {\n  static bool _isLoaded = false;\n  static final Map<String, String> _envVariables = {};\n\n  /// Loads .env file if not already loaded\n  static Future<void> load() async {\n    if (_isLoaded) return;\n\n    try {\n      final file = File('.env');\n\n      if (!await file.exists()) {\n        logger.fine('No .env file found');\n        _isLoaded = true;\n        return;\n      }\n\n      final lines = await file.readAsLines();\n\n      for (final line in lines) {\n        final trimmed = line.trim();\n\n        // Skip comments and empty lines\n        if (trimmed.isEmpty || trimmed.startsWith('#')) continue;\n\n        // Parse KEY=VALUE\n        final separatorIndex = trimmed.indexOf('=');\n        if (separatorIndex == -1) continue;\n\n        final key = trimmed.substring(0, separatorIndex).trim();\n        final rawValue = trimmed.substring(separatorIndex + 1).trim();\n\n        // Clean the value\n        final cleanValue = _cleanValue(rawValue);\n\n        // Store in cache\n        _envVariables[key] = cleanValue;\n\n        // Set environment variable\n        try {\n          Platform.environment[key] = cleanValue;\n        } catch (e) {\n          // Platform.environment might be immutable on some platforms\n          logger.finer('Cannot modify Platform.environment: $e');\n        }\n      }\n\n      logger.fine('.env file loaded successfully');\n      _isLoaded = true;\n    } catch (e, stackTrace) {\n      logger.warning('Error loading .env file', e, stackTrace);\n      _isLoaded = true;\n    }\n  }\n\n  /// Cleans the value by removing quotes and inline comments\n  static String _cleanValue(String value) {\n    if (value.isEmpty) return value;\n\n    String result = value;\n\n    // Remove inline comments (but not if inside quotes)\n    final commentIndex = _findCommentIndex(result);\n    if (commentIndex != -1) {\n      result = result.substring(0, commentIndex).trim();\n    }\n\n    // Remove surrounding quotes (must match)\n    if (result.length >= 2) {\n      final firstChar = result[0];\n      final lastChar = result[result.length - 1];\n\n      // Check if wrapped in matching quotes\n      if ((firstChar == '\"' && lastChar == '\"') ||\n          (firstChar == \"'\" && lastChar == \"'\")) {\n        result = result.substring(1, result.length - 1);\n\n        // Unescape escaped quotes inside\n        if (firstChar == '\"') {\n          result = result.replaceAll(r'\\\"', '\"');\n        } else {\n          result = result.replaceAll(r\"\\'\", \"'\");\n        }\n      }\n    }\n\n    return result;\n  }\n\n  /// Finds the index of a comment that's not inside quotes\n  static int _findCommentIndex(String value) {\n    bool inDoubleQuotes = false;\n    bool inSingleQuotes = false;\n    bool escaped = false;\n\n    for (int i = 0; i < value.length; i++) {\n      final char = value[i];\n\n      if (escaped) {\n        escaped = false;\n        continue;\n      }\n\n      if (char == '\\\\') {\n        escaped = true;\n        continue;\n      }\n\n      if (char == '\"' && !inSingleQuotes) {\n        inDoubleQuotes = !inDoubleQuotes;\n      } else if (char == \"'\" && !inDoubleQuotes) {\n        inSingleQuotes = !inSingleQuotes;\n      } else if (char == '#' && !inDoubleQuotes && !inSingleQuotes) {\n        return i;\n      }\n    }\n\n    return -1;\n  }\n\n  /// Gets an environment variable from .env or system environment\n  static String? get(String key) {\n    // Try internal cache first\n    if (_envVariables.containsKey(key)) {\n      return _envVariables[key];\n    }\n\n    // Fallback to Platform.environment\n    return Platform.environment[key];\n  }\n\n  /// Gets GITHUB_TOKEN specifically\n  static String? getGithubToken() {\n    return get('GITHUB_TOKEN');\n  }\n\n  /// Checks if a specific key exists\n  static bool has(String key) {\n    return _envVariables.containsKey(key) ||\n        Platform.environment.containsKey(key);\n  }\n\n  /// Gets all loaded environment variables (from .env only)\n  static Map<String, String> get all => Map.unmodifiable(_envVariables);\n\n  /// Resets the loader state (for testing)\n  static void reset() {\n    _isLoaded = false;\n    _envVariables.clear();\n  }\n}\n","size":4208,"language":"Dart","is_binary":false,"line_count":156,"is_source_code":true,"is_configuration":false,"is_documentation":false,"timestamp":"2025-10-15T07:46:08.382178"},{"path":"lib/src/common/errors/analyzer_exception.dart","content":"/// Defines the types of errors that can occur during analysis.\nenum AnalyzerErrorCode {\n  invalidUrl,\n  repositoryNotFound,\n  accessDenied,\n  rateLimitExceeded,\n  networkError,\n  cacheError,\n  analysisError,\n  directoryNotFound,\n  fileReadError,\n  archiveError,\n  configurationError,\n}\n\n/// A custom exception class for handling errors within the analyzer.\n///\n/// This class standardizes error reporting by providing a consistent\n/// structure that includes a message, detailed information, an error code,\n/// and the original exception that was caught.\nclass AnalyzerException implements Exception {\n  final String message;\n  final String? details;\n  final AnalyzerErrorCode code;\n  final Object? originalException;\n  final StackTrace? stackTrace;\n\n  /// Creates an instance of [AnalyzerException].\n  AnalyzerException(\n    this.message, {\n    this.details,\n    required this.code,\n    this.originalException,\n    this.stackTrace,\n  });\n\n  @override\n  String toString() {\n    final buffer = StringBuffer('AnalyzerException [$code]: $message');\n    if (details != null) {\n      buffer.write('\\n  Details: $details');\n    }\n    if (originalException != null) {\n      buffer.write('\\n  Original Exception: ${originalException.toString()}');\n    }\n    if (stackTrace != null) {\n      buffer.write('\\n  Stack Trace:\\n$stackTrace');\n    }\n    return buffer.toString();\n  }\n}\n","size":1372,"language":"Dart","is_binary":false,"line_count":52,"is_source_code":true,"is_configuration":false,"is_documentation":false,"timestamp":"2025-10-15T07:46:08.382635"},{"path":"lib/src/common/language_info.dart","content":"import 'package:path/path.dart' as path;\n\nconst Map<String, String> kLanguageExtensions = {\n  // Programming Languages\n  'dart': 'Dart',\n  'py': 'Python',\n  'js': 'JavaScript',\n  'ts': 'TypeScript',\n  'jsx': 'JavaScript',\n  'tsx': 'TypeScript',\n  'java': 'Java',\n  'kt': 'Kotlin',\n  'kts': 'Kotlin',\n  'swift': 'Swift',\n  'c': 'C',\n  'cpp': 'C++',\n  'cc': 'C++',\n  'cxx': 'C++',\n  'h': 'C',\n  'hpp': 'C++',\n  'cs': 'C#',\n  'go': 'Go',\n  'rs': 'Rust',\n  'rb': 'Ruby',\n  'php': 'PHP',\n  'scala': 'Scala',\n  'r': 'R',\n  'lua': 'Lua',\n  'pl': 'Perl',\n  'sh': 'Shell',\n  'bash': 'Shell',\n  'zsh': 'Shell',\n  'fish': 'Shell',\n  'ps1': 'PowerShell',\n  'bat': 'Batch',\n  'cmd': 'Batch',\n\n  // Web\n  'html': 'HTML',\n  'htm': 'HTML',\n  'css': 'CSS',\n  'scss': 'SCSS',\n  'sass': 'Sass',\n  'less': 'Less',\n  'vue': 'Vue',\n  'svelte': 'Svelte',\n\n  // Data & Config\n  'json': 'JSON',\n  'yaml': 'YAML',\n  'yml': 'YAML',\n  'toml': 'TOML',\n  'xml': 'XML',\n  'ini': 'INI',\n  'conf': 'Config',\n  'config': 'Config',\n  'env': 'Environment',\n\n  // Documentation\n  'md': 'Markdown',\n  'markdown': 'Markdown',\n  'rst': 'reStructuredText',\n  'txt': 'Text',\n  'tex': 'LaTeX',\n\n  // Database\n  'sql': 'SQL',\n  'sqlite': 'SQLite',\n  'psql': 'PostgreSQL',\n\n  // Build & Package\n  'gradle': 'Gradle',\n  'maven': 'Maven',\n  'cmake': 'CMake',\n  'make': 'Makefile',\n\n  // Others\n  'asm': 'Assembly',\n  's': 'Assembly',\n  'vim': 'VimScript',\n  'el': 'Emacs Lisp',\n  'clj': 'Clojure',\n  'ex': 'Elixir',\n  'exs': 'Elixir',\n  'erl': 'Erlang',\n  'hrl': 'Erlang',\n  'hs': 'Haskell',\n  'ml': 'OCaml',\n  'nim': 'Nim',\n  'v': 'V',\n  'zig': 'Zig',\n};\n\nconst Map<String, String> kSyntaxHighlighting = {\n  'Dart': 'dart',\n  'Python': 'python',\n  'JavaScript': 'javascript',\n  'TypeScript': 'typescript',\n  'Java': 'java',\n  'Kotlin': 'kotlin',\n  'Swift': 'swift',\n  'C': 'c',\n  'C++': 'cpp',\n  'C#': 'csharp',\n  'Go': 'go',\n  'Rust': 'rust',\n  'Ruby': 'ruby',\n  'PHP': 'php',\n  'Scala': 'scala',\n  'R': 'r',\n  'Lua': 'lua',\n  'Perl': 'perl',\n  'Shell': 'bash',\n  'Bash': 'bash',\n  'Zsh': 'zsh',\n  'Fish': 'fish',\n  'PowerShell': 'powershell',\n  'Batch': 'batch',\n  'HTML': 'html',\n  'CSS': 'css',\n  'SCSS': 'scss',\n  'Sass': 'sass',\n  'Less': 'less',\n  'Vue': 'vue',\n  'Svelte': 'svelte',\n  'JSON': 'json',\n  'YAML': 'yaml',\n  'TOML': 'toml',\n  'XML': 'xml',\n  'Markdown': 'markdown',\n  'SQL': 'sql',\n  'Gradle': 'gradle',\n  'CMake': 'cmake',\n  'Makefile': 'makefile',\n  'Dockerfile': 'dockerfile',\n  'Assembly': 'asm',\n  'VimScript': 'vim',\n  'Clojure': 'clojure',\n  'Elixir': 'elixir',\n  'Erlang': 'erlang',\n  'Haskell': 'haskell',\n  'OCaml': 'ocaml',\n  'Nim': 'nim',\n  'Zig': 'zig',\n};\n\nString? detectLanguage(String filePath) {\n  final fileName = path.basename(filePath);\n  final fileNameLower = fileName.toLowerCase();\n\n  if (fileNameLower == 'dockerfile' ||\n      fileNameLower.startsWith('dockerfile.')) {\n    return 'Dockerfile';\n  }\n  if (fileNameLower == 'makefile' || fileNameLower.startsWith('makefile.')) {\n    return 'Makefile';\n  }\n  if (fileNameLower == 'gemfile') {\n    return 'Ruby';\n  }\n  if (fileNameLower == 'rakefile') {\n    return 'Ruby';\n  }\n  if (fileNameLower == '.bashrc' || fileNameLower == '.zshrc') {\n    return 'Shell';\n  }\n\n  final extension = path.extension(filePath).toLowerCase();\n  if (extension.isEmpty) {\n    return null;\n  }\n\n  final ext = extension.substring(1);\n  return kLanguageExtensions[ext];\n}\n\nString? getSyntaxHighlighting(String filePath) {\n  final language = detectLanguage(filePath);\n  if (language == null) return null;\n  return kSyntaxHighlighting[language];\n}\n","size":3570,"language":"Dart","is_binary":false,"line_count":182,"is_source_code":true,"is_configuration":false,"is_documentation":false,"timestamp":"2025-10-15T07:46:08.382989"},{"path":"lib/src/common/logger.dart","content":"import 'package:logging/logging.dart';\n\n/// The root logger for the application.\nfinal logger = Logger('GithubAnalyzer');\n\n/// Sets up the logger for the application.\n///\n/// Configures the logging level and sets up a listener to print log records\n/// to the console with a specific format.\n///\n/// If [verbose] is true, the log level is set to [Level.ALL], otherwise it\n/// defaults to [Level.INFO].\nvoid setupLogger({bool verbose = false}) {\n  Logger.root.level = verbose ? Level.ALL : Level.INFO;\n  Logger.root.onRecord.listen((record) {\n    print(\n      '${record.time} [${record.level.name}] ${record.loggerName}: ${record.message}',\n    );\n    if (record.error != null) {\n      print('  Error: ${record.error}');\n    }\n    if (record.stackTrace != null) {\n      print('  Stack Trace:\\n${record.stackTrace}');\n    }\n  });\n}\n","size":829,"language":"Dart","is_binary":false,"line_count":27,"is_source_code":true,"is_configuration":false,"is_documentation":false,"timestamp":"2025-10-15T07:46:08.383411"},{"path":"lib/src/common/utils/context_generator.dart","content":"import 'package:path/path.dart' as path;\nimport 'package:github_analyzer/src/models/analysis_result.dart';\nimport 'package:github_analyzer/src/common/utils/markdown_generator.dart';\nimport 'package:github_analyzer/src/infrastructure/file_system/file_system.dart';\n\n/// Generates context output files from analysis results using platform-independent file system\nclass ContextGenerator {\n  static final IFileSystem _fs = getFileSystem();\n\n  /// Generates a markdown file from the analysis result with automatic naming\n  static Future<String> generate(\n    AnalysisResult result, {\n    String? outputPath,\n    String? outputDir,\n    MarkdownConfig config = MarkdownConfig.standard,\n  }) async {\n    final filePath = _resolveOutputPath(result, outputPath, outputDir);\n\n    // Ensure output directory exists\n    final dirPath = path.dirname(filePath);\n    final dirExists = await _fs.directoryExists(dirPath);\n    if (!dirExists) {\n      await _fs.createDirectory(dirPath);\n    }\n\n    await MarkdownGenerator.generateToFile(result, filePath, config: config);\n\n    return filePath;\n  }\n\n  /// Generates a markdown string from the analysis result\n  static String generateString(\n    AnalysisResult result, {\n    MarkdownConfig config = MarkdownConfig.standard,\n  }) {\n    return MarkdownGenerator.generate(result, config: config);\n  }\n\n  /// Resolves the output path with smart defaults\n  static String _resolveOutputPath(\n    AnalysisResult result,\n    String? outputPath,\n    String? outputDir,\n  ) {\n    if (outputPath != null) {\n      return _ensureMarkdownExtension(outputPath);\n    }\n\n    final dir = outputDir ?? '.';\n    final fileName = _generateFileName(result);\n\n    return path.join(dir, fileName);\n  }\n\n  /// Generates a clean file name from repository metadata\n  static String _generateFileName(AnalysisResult result) {\n    var name = result.metadata.name;\n\n    // Clean the name for file system\n    name = name.replaceAll(RegExp(r'[^\\w\\-\\.]'), '_');\n    name = name.replaceAll(RegExp(r'_+'), '_');\n\n    return '${name}_analysis.md';\n  }\n\n  /// Ensures the file has .md extension\n  static String _ensureMarkdownExtension(String filePath) {\n    if (!filePath.endsWith('.md')) {\n      return '$filePath.md';\n    }\n    return filePath;\n  }\n}\n","size":2246,"language":"Dart","is_binary":false,"line_count":74,"is_source_code":true,"is_configuration":false,"is_documentation":false,"timestamp":"2025-10-15T07:46:08.383733"},{"path":"lib/src/common/utils/directory_tree_generator.dart","content":"import 'package:path/path.dart' as p;\n\n/// Generates a textual representation of a directory tree from a list of file paths.\n///\n/// This utility is useful for providing a high-level overview of the\n/// repository structure, which can be beneficial for context in LLMs.\nclass DirectoryTreeGenerator {\n  /// Generates a directory tree string from a list of file paths.\n  ///\n  /// The [paths] should be relative paths from the repository root.\n  /// The [maxDepth] parameter can be used to limit the depth of the tree.\n  static String generate(List<String> paths, {int? maxDepth}) {\n    if (paths.isEmpty) {\n      return '';\n    }\n\n    // Use a map to represent the directory structure as a tree.\n    final Map<String, dynamic> tree = {};\n\n    for (final path in paths) {\n      final parts = p.split(path);\n      Map<String, dynamic> currentNode = tree;\n      for (final part in parts) {\n        currentNode = currentNode.putIfAbsent(part, () => <String, dynamic>{});\n      }\n    }\n\n    final buffer = StringBuffer();\n    _buildTree(buffer, tree, '', true, maxDepth, 0);\n    return buffer.toString();\n  }\n\n  static void _buildTree(\n    StringBuffer buffer,\n    Map<String, dynamic> node,\n    String prefix,\n    bool isLast,\n    int? maxDepth,\n    int currentDepth,\n  ) {\n    if (maxDepth != null && currentDepth > maxDepth) {\n      return;\n    }\n\n    final sortedKeys = node.keys.toList()..sort();\n    for (int i = 0; i < sortedKeys.length; i++) {\n      final key = sortedKeys[i];\n      final childNode = node[key] as Map<String, dynamic>;\n      final isCurrentLast = i == sortedKeys.length - 1;\n\n      buffer.write(prefix);\n      buffer.write(isCurrentLast ? 'â””â”€â”€ ' : 'â”œâ”€â”€ ');\n      buffer.writeln(key);\n\n      final newPrefix = prefix + (isCurrentLast ? '    ' : 'â”‚   ');\n      if (childNode.isNotEmpty) {\n        _buildTree(\n          buffer,\n          childNode,\n          newPrefix,\n          isCurrentLast,\n          maxDepth,\n          currentDepth + 1,\n        );\n      }\n    }\n  }\n}\n","size":2005,"language":"Dart","is_binary":false,"line_count":69,"is_source_code":true,"is_configuration":false,"is_documentation":false,"timestamp":"2025-10-15T07:46:08.384095"},{"path":"lib/src/common/utils/file_utils.dart","content":"import 'package:universal_io/io.dart';\nimport 'package:path/path.dart' as path;\nimport 'package:github_analyzer/src/common/constants.dart';\nimport 'package:github_analyzer/src/models/source_file.dart';\nimport 'package:glob/glob.dart';\n\n/// Formats file size in bytes to a human-readable string\nString formatFileSize(int bytes) {\n  if (bytes < 1024) return '$bytes B';\n  if (bytes < 1024 * 1024) return '${(bytes / 1024).toStringAsFixed(2)} KB';\n  if (bytes < 1024 * 1024 * 1024) {\n    return '${(bytes / (1024 * 1024)).toStringAsFixed(2)} MB';\n  }\n  return '${(bytes / (1024 * 1024 * 1024)).toStringAsFixed(2)} GB';\n}\n\n/// Checks if a file path matches any of the given patterns\nbool matchesAnyPattern(String filePath, List<String> patterns) {\n  final normalizedPath = path.normalize(filePath).replaceAll('\\\\', '/');\n\n  for (final pattern in patterns) {\n    if (Glob(pattern).matches(normalizedPath)) {\n      return true;\n    }\n  }\n\n  return false;\n}\n\n/// Checks if a file should be excluded based on exclude patterns\nbool shouldExclude(String filePath, List<String>? excludePatterns) {\n  return matchesAnyPattern(\n    filePath,\n    excludePatterns ?? kDefaultExcludePatterns,\n  );\n}\n\n/// Checks if a file is binary based on its extension\nbool isBinaryFile(String filePath) {\n  final extension = path.extension(filePath).toLowerCase().replaceAll('.', '');\n  return kBinaryExtensions.contains(extension);\n}\n\n/// Checks if a file is a configuration file\nbool isConfigurationFile(String filePath) {\n  final fileName = path.basename(filePath).toLowerCase();\n  final extension = path.extension(fileName).replaceAll('.', '');\n\n  if (kConfigurationFileNames.contains(fileName)) {\n    return true;\n  }\n\n  return kConfigurationExtensions.contains(extension);\n}\n\n/// Checks if a file is a documentation file\nbool isDocumentationFile(String filePath) {\n  final fileNameWithExt = path.basename(filePath).toLowerCase();\n  final extension = path.extension(fileNameWithExt).replaceAll('.', '');\n  final fileNameWithoutExt = path.basenameWithoutExtension(fileNameWithExt);\n\n  if (kDocumentationFileNames.contains(fileNameWithoutExt) ||\n      kDocumentationFileNames.contains(fileNameWithExt)) {\n    return true;\n  }\n\n  return kDocumentationExtensions.contains(extension);\n}\n\n/// Identifies main entry point files from a list of source files\nList<String> identifyMainFiles(List<SourceFile> files) {\n  final mainFiles = <String>[];\n\n  for (final file in files) {\n    final fileName = path.basename(file.path);\n    if (kMainFilePatterns.contains(fileName)) {\n      mainFiles.add(file.path);\n    }\n  }\n\n  return mainFiles;\n}\n\n/// Extracts dependency information from source files\nMap<String, List<String>> extractDependencies(List<SourceFile> files) {\n  final dependencies = <String, List<String>>{};\n\n  for (final file in files) {\n    if (file.content == null) continue;\n\n    final fileName = path.basename(file.path);\n    List<String>? deps;\n\n    switch (fileName) {\n      case 'package.json':\n        deps = ['Node.js/npm'];\n        break;\n      case 'pubspec.yaml':\n        deps = ['Dart/pub'];\n        break;\n      case 'requirements.txt':\n      case 'setup.py':\n        deps = ['Python/pip'];\n        break;\n      case 'Cargo.toml':\n        deps = ['Rust/cargo'];\n        break;\n      case 'go.mod':\n        deps = ['Go modules'];\n        break;\n      case 'pom.xml':\n      case 'build.gradle':\n        deps = ['Java/Maven or Gradle'];\n        break;\n      case 'Gemfile':\n        deps = ['Ruby/bundler'];\n        break;\n      case 'composer.json':\n        deps = ['PHP/composer'];\n        break;\n    }\n\n    if (deps != null) {\n      dependencies[file.path] = deps;\n    }\n  }\n\n  return dependencies;\n}\n\n/// Reads file content if it's within the size limit\nFuture<String?> readFileContent(File file, int maxFileSize) async {\n  final stat = await file.stat();\n\n  if (stat.size > maxFileSize) {\n    return null;\n  }\n\n  try {\n    return await file.readAsString();\n  } catch (e) {\n    return null;\n  }\n}\n","size":3985,"language":"Dart","is_binary":false,"line_count":145,"is_source_code":true,"is_configuration":false,"is_documentation":false,"timestamp":"2025-10-15T07:46:08.384574"},{"path":"lib/src/common/utils/github_utils.dart","content":"Map<String, String>? parseGitHubUrl(String url) {\n  final patterns = [\n    RegExp(r'https?://github\\.com/([^/]+)/([^/]+?)(?:\\.git)?/?$'),\n    RegExp(r'git@github\\.com:([^/]+)/([^/]+?)(?:\\.git)?$'),\n    RegExp(r'([^/]+)/([^/]+)$'),\n  ];\n\n  for (final pattern in patterns) {\n    final match = pattern.firstMatch(url);\n    if (match != null) {\n      return {\n        'owner': match.group(1)!,\n        'repo': match.group(2)!.replaceAll('.git', ''),\n      };\n    }\n  }\n\n  return null;\n}\n\nString normalizeGitHubUrl(String url) {\n  final parsed = parseGitHubUrl(url);\n  if (parsed == null) {\n    throw ArgumentError('Invalid GitHub URL: $url');\n  }\n  return 'https://github.com/${parsed['owner']}/${parsed['repo']}';\n}\n\nbool isValidGitHubUrl(String url) {\n  return parseGitHubUrl(url) != null;\n}\n","size":790,"language":"Dart","is_binary":false,"line_count":32,"is_source_code":true,"is_configuration":false,"is_documentation":false,"timestamp":"2025-10-15T07:46:08.384927"},{"path":"lib/src/common/utils/markdown_generator.dart","content":"import 'package:github_analyzer/src/models/analysis_result.dart';\nimport 'package:github_analyzer/src/infrastructure/file_system/file_system.dart';\n\n/// Configuration for markdown generation\nclass MarkdownConfig {\n  final int? maxFiles;\n  final int? maxContentSize;\n  final bool includeBinaryStats;\n  final bool includeErrors;\n  final int? minPriority;\n\n  const MarkdownConfig({\n    this.maxFiles,\n    this.maxContentSize,\n    this.includeBinaryStats = true,\n    this.includeErrors = true,\n    this.minPriority,\n  });\n\n  static const standard = MarkdownConfig();\n\n  static const compact = MarkdownConfig(\n    maxFiles: 50,\n    maxContentSize: 50000,\n    includeBinaryStats: false,\n  );\n}\n\n/// Generates markdown formatted output from analysis results\nclass MarkdownGenerator {\n  static final IFileSystem _fs = getFileSystem();\n\n  /// Generates and writes markdown directly to a file asynchronously\n  static Future<void> generateToFile(\n    AnalysisResult result,\n    String outputPath, {\n    MarkdownConfig config = MarkdownConfig.standard,\n  }) async {\n    final buffer = StringBuffer();\n\n    _writeHeaderSync(buffer, result);\n    _writeMetadataSync(buffer, result);\n    _writeStatisticsSync(buffer, result, config);\n    _writeDirectoryTreeSync(buffer, result);\n    _writeLanguageDistributionSync(buffer, result);\n    _writeMainFilesSync(buffer, result);\n    _writeDependenciesSync(buffer, result);\n\n    if (config.includeErrors) {\n      _writeErrorsSync(buffer, result);\n    }\n\n    _writeSourceCodeSync(buffer, result, config);\n\n    await _fs.writeFile(outputPath, buffer.toString());\n  }\n\n  /// Generates markdown string synchronously\n  static String generate(\n    AnalysisResult result, {\n    MarkdownConfig config = MarkdownConfig.standard,\n  }) {\n    final buffer = StringBuffer();\n\n    _writeHeaderSync(buffer, result);\n    _writeMetadataSync(buffer, result);\n    _writeStatisticsSync(buffer, result, config);\n    _writeDirectoryTreeSync(buffer, result);\n    _writeLanguageDistributionSync(buffer, result);\n    _writeMainFilesSync(buffer, result);\n    _writeDependenciesSync(buffer, result);\n\n    if (config.includeErrors) {\n      _writeErrorsSync(buffer, result);\n    }\n\n    _writeSourceCodeSync(buffer, result, config);\n\n    return buffer.toString();\n  }\n\n  // Synchronous helpers for building markdown content\n\n  static void _writeHeaderSync(StringBuffer buffer, AnalysisResult result) {\n    buffer.writeln('# ${result.metadata.name}');\n    buffer.writeln();\n\n    if (result.metadata.description != null &&\n        result.metadata.description!.isNotEmpty) {\n      buffer.writeln(result.metadata.description);\n      buffer.writeln();\n    }\n  }\n\n  static void _writeMetadataSync(StringBuffer buffer, AnalysisResult result) {\n    buffer.writeln('## Repository Information');\n    buffer.writeln();\n\n    final meta = result.metadata;\n    if (meta.fullName != null) {\n      buffer.writeln('**Repository:** `${meta.fullName}`');\n    }\n\n    final info = <String>[];\n    if (meta.language != null) info.add('**Language:** ${meta.language}');\n    if (meta.stars > 0) info.add('**Stars:** ${meta.stars}');\n    if (meta.forks > 0) info.add('**Forks:** ${meta.forks}');\n\n    if (info.isNotEmpty) {\n      buffer.writeln(info.join(' | '));\n    }\n\n    buffer.writeln();\n  }\n\n  static void _writeStatisticsSync(\n    StringBuffer buffer,\n    AnalysisResult result,\n    MarkdownConfig config,\n  ) {\n    buffer.writeln('## Statistics');\n    buffer.writeln();\n\n    final stats = result.statistics;\n    buffer.writeln('- **Total Files:** ${stats.totalFiles}');\n    buffer.writeln('- **Total Lines:** ${stats.totalLines}');\n    buffer.writeln('- **Total Size:** ${_formatBytes(stats.totalSize)}');\n    buffer.writeln('- **Source Files:** ${stats.sourceFiles}');\n\n    if (config.includeBinaryStats && stats.binaryFiles > 0) {\n      buffer.writeln('- **Binary Files:** ${stats.binaryFiles}');\n    }\n\n    buffer.writeln();\n  }\n\n  static void _writeDirectoryTreeSync(\n    StringBuffer buffer,\n    AnalysisResult result,\n  ) {\n    if (result.metadata.directoryTree.isEmpty) return;\n\n    buffer.writeln('## Directory Structure');\n    buffer.writeln();\n    buffer.writeln('```');\n    buffer.writeln(result.metadata.directoryTree);\n    buffer.writeln('```');\n    buffer.writeln();\n  }\n\n  static void _writeLanguageDistributionSync(\n    StringBuffer buffer,\n    AnalysisResult result,\n  ) {\n    if (result.statistics.languageDistribution.isEmpty) return;\n\n    buffer.writeln('## Language Distribution');\n    buffer.writeln();\n\n    final sorted = result.statistics.languageDistribution.entries.toList()\n      ..sort((a, b) => b.value.compareTo(a.value));\n\n    for (final entry in sorted.take(10)) {\n      final percentage =\n          (entry.value / result.statistics.totalFiles * 100).toStringAsFixed(1);\n      buffer.writeln('- **${entry.key}:** ${entry.value} files ($percentage%)');\n    }\n\n    buffer.writeln();\n  }\n\n  static void _writeMainFilesSync(StringBuffer buffer, AnalysisResult result) {\n    if (result.mainFiles.isEmpty) return;\n\n    buffer.writeln('## Main Entry Points');\n    buffer.writeln();\n\n    for (final file in result.mainFiles) {\n      buffer.writeln('- `$file`');\n    }\n\n    buffer.writeln();\n  }\n\n  static void _writeDependenciesSync(\n    StringBuffer buffer,\n    AnalysisResult result,\n  ) {\n    if (result.dependencies.isEmpty) return;\n\n    buffer.writeln('## Dependencies');\n    buffer.writeln();\n\n    for (final entry in result.dependencies.entries) {\n      if (entry.value.isEmpty) continue;\n\n      buffer.writeln('**${entry.key}:**');\n      for (final dep in entry.value) {\n        buffer.writeln('- $dep');\n      }\n      buffer.writeln();\n    }\n  }\n\n  static void _writeErrorsSync(StringBuffer buffer, AnalysisResult result) {\n    if (result.errors.isEmpty) return;\n\n    buffer.writeln('## Analysis Errors');\n    buffer.writeln();\n\n    for (final error in result.errors) {\n      buffer.writeln('- **${error.path}:** ${error.message}');\n    }\n\n    buffer.writeln();\n  }\n\n  static void _writeSourceCodeSync(\n    StringBuffer buffer,\n    AnalysisResult result,\n    MarkdownConfig config,\n  ) {\n    buffer.writeln('## Source Code');\n    buffer.writeln();\n\n    var sourceFiles = result.files\n        .where(\n            (f) => f.isSourceCode && f.content != null && f.content!.isNotEmpty)\n        .toList();\n\n    if (config.minPriority != null) {\n      sourceFiles = sourceFiles.where((f) {\n        if (f.path.startsWith('lib/')) return true;\n        if (f.path.contains('main.dart')) return true;\n        return false;\n      }).toList();\n    }\n\n    if (config.maxFiles != null && sourceFiles.length > config.maxFiles!) {\n      sourceFiles = sourceFiles.take(config.maxFiles!).toList();\n      buffer.writeln(\n          '> **Note:** Showing ${config.maxFiles} of ${result.files.length} files');\n      buffer.writeln();\n    }\n\n    for (final file in sourceFiles) {\n      buffer.writeln('### ${file.path}');\n      buffer.writeln();\n\n      var content = file.content!;\n\n      if (config.maxContentSize != null &&\n          content.length > config.maxContentSize!) {\n        content = content.substring(0, config.maxContentSize!);\n        content += '\\n\\n// ... truncated ...';\n      }\n\n      final language = file.language ?? '';\n      buffer.writeln('```$language');\n      buffer.writeln(content);\n      buffer.writeln('```');\n      buffer.writeln();\n    }\n  }\n\n  static String _formatBytes(int bytes) {\n    if (bytes < 1024) return '$bytes B';\n    if (bytes < 1024 * 1024) return '${(bytes / 1024).toStringAsFixed(1)} KB';\n    if (bytes < 1024 * 1024 * 1024) {\n      return '${(bytes / (1024 * 1024)).toStringAsFixed(1)} MB';\n    }\n    return '${(bytes / (1024 * 1024 * 1024)).toStringAsFixed(1)} GB';\n  }\n}\n","size":7710,"language":"Dart","is_binary":false,"line_count":275,"is_source_code":true,"is_configuration":false,"is_documentation":false,"timestamp":"2025-10-15T07:46:08.385243"},{"path":"lib/src/common/utils/metadata_generator.dart","content":"import 'package:universal_io/io.dart';\nimport 'dart:convert';\nimport 'package:github_analyzer/src/models/analysis_result.dart';\n\n/// Generates metadata JSON files from analysis results\nclass MetadataGenerator {\n  /// Generates and writes metadata to a file\n  static Future<void> generate(\n    AnalysisResult result,\n    String outputPath,\n  ) async {\n    final metadata = {\n      'repository': {\n        'name': result.metadata.name,\n        'fullname': result.metadata.fullName,\n        'description': result.metadata.description,\n        'isprivate': result.metadata.isPrivate,\n        'defaultbranch': result.metadata.defaultBranch,\n        'language': result.metadata.language,\n        'languages': result.metadata.languages,\n        'stars': result.metadata.stars,\n        'forks': result.metadata.forks,\n      },\n      'statistics': {\n        'totalfiles': result.statistics.totalFiles,\n        'totallines': result.statistics.totalLines,\n        'totalsize': result.statistics.totalSize,\n        'binaryfiles': result.statistics.binaryFiles,\n        'sourcefiles': result.statistics.sourceFiles,\n        'configfiles': result.statistics.configFiles,\n        'documentationfiles': result.statistics.documentationFiles,\n        'languagedistribution': result.statistics.languageDistribution,\n      },\n      'mainfiles': result.mainFiles,\n      'dependencies': result.dependencies,\n      'errors': result.errors.map((e) => e.toJson()).toList(),\n      'generatedat': DateTime.now().toIso8601String(),\n    };\n\n    final file = File(outputPath);\n    await file.writeAsString(\n      const JsonEncoder.withIndent('  ').convert(metadata),\n    );\n  }\n\n  /// Generates metadata as a Map without writing to file\n  static Map<String, dynamic> generateMap(AnalysisResult result) {\n    return {\n      'repository': {\n        'name': result.metadata.name,\n        'fullname': result.metadata.fullName,\n        'description': result.metadata.description,\n        'isprivate': result.metadata.isPrivate,\n        'defaultbranch': result.metadata.defaultBranch,\n        'language': result.metadata.language,\n        'languages': result.metadata.languages,\n        'stars': result.metadata.stars,\n        'forks': result.metadata.forks,\n      },\n      'statistics': {\n        'totalfiles': result.statistics.totalFiles,\n        'totallines': result.statistics.totalLines,\n        'totalsize': result.statistics.totalSize,\n        'binaryfiles': result.statistics.binaryFiles,\n        'sourcefiles': result.statistics.sourceFiles,\n        'configfiles': result.statistics.configFiles,\n        'documentationfiles': result.statistics.documentationFiles,\n        'languagedistribution': result.statistics.languageDistribution,\n      },\n      'mainfiles': result.mainFiles,\n      'dependencies': result.dependencies,\n      'errors': result.errors.map((e) => e.toJson()).toList(),\n      'generatedat': DateTime.now().toIso8601String(),\n    };\n  }\n}\n","size":2930,"language":"Dart","is_binary":false,"line_count":77,"is_source_code":true,"is_configuration":false,"is_documentation":false,"timestamp":"2025-10-15T07:46:08.385706"},{"path":"lib/src/common/utils/pattern_matcher.dart","content":"/// Pattern matching utility to replace glob package for web compatibility\nclass PatternMatcher {\n  /// Check if a path matches any of the given patterns\n  static bool matchesAny(String path, List<String> patterns) {\n    for (final pattern in patterns) {\n      if (matches(path, pattern)) {\n        return true;\n      }\n    }\n    return false;\n  }\n\n  /// Check if a path matches a single pattern\n  static bool matches(String path, String pattern) {\n    final regexPattern = _convertGlobToRegex(pattern);\n    return regexPattern.hasMatch(path);\n  }\n\n  /// Convert glob pattern to RegExp\n  static RegExp _convertGlobToRegex(String pattern) {\n    var regexStr = pattern;\n\n    // Escape special regex characters except glob wildcards\n    regexStr = regexStr.replaceAllMapped(\n      RegExp(r'[.+^${}()|[\\]\\\\]'),\n      (match) => '\\\\${match.group(0)}',\n    );\n\n    // Convert glob wildcards to regex\n    regexStr = regexStr.replaceAll('**/', '(?:.*/)');\n    regexStr = regexStr.replaceAll('**', '.*');\n    regexStr = regexStr.replaceAll('*', '[^/]*');\n    regexStr = regexStr.replaceAll('?', '[^/]');\n\n    // Anchor the pattern\n    regexStr = '^$regexStr\\$';\n\n    return RegExp(regexStr);\n  }\n\n  /// Check if a path should be excluded based on patterns\n  static bool shouldExclude(String path, List<String> excludePatterns) {\n    return matchesAny(path, excludePatterns);\n  }\n\n  /// Check if a path should be included based on patterns\n  static bool shouldInclude(String path, List<String> includePatterns) {\n    if (includePatterns.isEmpty) return true;\n    return matchesAny(path, includePatterns);\n  }\n\n  /// Filter a list of paths based on include/exclude patterns\n  static List<String> filterPaths(\n    List<String> paths,\n    List<String> includePatterns,\n    List<String> excludePatterns,\n  ) {\n    return paths.where((path) {\n      if (shouldExclude(path, excludePatterns)) return false;\n      return shouldInclude(path, includePatterns);\n    }).toList();\n  }\n}\n","size":1964,"language":"Dart","is_binary":false,"line_count":64,"is_source_code":true,"is_configuration":false,"is_documentation":false,"timestamp":"2025-10-15T07:46:08.386032"},{"path":"lib/src/core/cache_service.dart","content":"import 'dart:convert';\nimport 'package:crypto/crypto.dart';\nimport 'package:universal_io/io.dart';\nimport 'package:github_analyzer/src/common/logger.dart';\nimport 'package:github_analyzer/src/common/errors/analyzer_exception.dart';\nimport 'package:github_analyzer/src/models/analysis_result.dart';\n\n/// Manages caching of analysis results to avoid redundant computations\nclass CacheService {\n  final String cacheDirectory;\n  final Duration? maxAge;\n  bool _isInitialized = false;\n\n  CacheService({\n    required this.cacheDirectory,\n    this.maxAge,\n  });\n\n  /// Initializes the cache service by creating the cache directory if it doesn't exist\n  Future<void> initialize() async {\n    if (_isInitialized) return;\n\n    final dir = Directory(cacheDirectory);\n    if (!await dir.exists()) {\n      await dir.create(recursive: true);\n      logger.info('Cache directory created: $cacheDirectory');\n    }\n    _isInitialized = true;\n  }\n\n  /// Generates a cache key from repository URL and commit hash\n  String _generateCacheKey(String repositoryUrl, String commitHash) {\n    final input = '$repositoryUrl:$commitHash';\n    return sha256.convert(utf8.encode(input)).toString();\n  }\n\n  /// Retrieves a cached AnalysisResult if available and not expired\n  Future<AnalysisResult?> get(String repositoryUrl, String commitHash) async {\n    if (!_isInitialized) {\n      throw AnalyzerException(\n        'CacheService not initialized',\n        code: AnalyzerErrorCode.cacheError,\n      );\n    }\n\n    final key = _generateCacheKey(repositoryUrl, commitHash);\n    final cacheFile = File('$cacheDirectory/$key.json');\n\n    if (!await cacheFile.exists()) {\n      logger.fine('Cache miss for $repositoryUrl (commit: $commitHash)');\n      return null;\n    }\n\n    if (maxAge != null) {\n      final stat = await cacheFile.stat();\n      final age = DateTime.now().difference(stat.modified);\n      if (age > maxAge!) {\n        logger.info('Cache expired for $repositoryUrl. Deleting.');\n        await delete(repositoryUrl, commitHash);\n        return null;\n      }\n    }\n\n    try {\n      final content = await cacheFile.readAsString();\n      final json = jsonDecode(content) as Map<String, dynamic>;\n      logger.info('Cache hit for $repositoryUrl (commit: $commitHash)');\n      return AnalysisResult.fromJson(json);\n    } catch (e, stackTrace) {\n      logger.warning(\n        'Failed to read or parse cache file for $key. Deleting. Error: $e',\n        e,\n        stackTrace,\n      );\n      await delete(repositoryUrl, commitHash);\n      return null;\n    }\n  }\n\n  /// Saves an AnalysisResult to the cache\n  Future<void> set(\n    String repositoryUrl,\n    String commitHash,\n    AnalysisResult result,\n  ) async {\n    if (!_isInitialized) {\n      throw AnalyzerException(\n        'CacheService not initialized',\n        code: AnalyzerErrorCode.cacheError,\n      );\n    }\n\n    final key = _generateCacheKey(repositoryUrl, commitHash);\n    final cacheFile = File('$cacheDirectory/$key.json');\n\n    try {\n      final json = jsonEncode(result.toJson());\n      await cacheFile.writeAsString(json);\n      logger.info('Saved cache for $repositoryUrl (commit: $commitHash)');\n    } catch (e, stackTrace) {\n      logger.severe('Failed to write cache for $key', e, stackTrace);\n      throw AnalyzerException(\n        'Failed to write to cache',\n        code: AnalyzerErrorCode.cacheError,\n        details: e.toString(),\n      );\n    }\n  }\n\n  /// Deletes a specific entry from the cache\n  Future<void> delete(String repositoryUrl, String commitHash) async {\n    final key = _generateCacheKey(repositoryUrl, commitHash);\n    final cacheFile = File('$cacheDirectory/$key.json');\n\n    if (await cacheFile.exists()) {\n      await cacheFile.delete();\n      logger.info('Deleted cache for $key');\n    }\n  }\n\n  /// Clears the entire cache directory\n  Future<void> clear() async {\n    final dir = Directory(cacheDirectory);\n    if (await dir.exists()) {\n      await for (final entity in dir.list()) {\n        await entity.delete(recursive: true);\n      }\n      logger.info('Cache directory cleared.');\n    }\n  }\n\n  /// Gets statistics about the cache\n  Future<Map<String, dynamic>> getStatistics() async {\n    final dir = Directory(cacheDirectory);\n    if (!await dir.exists()) {\n      return {'totalFiles': 0, 'totalSize': 0};\n    }\n\n    int totalFiles = 0;\n    int totalSize = 0;\n\n    await for (final entity in dir.list()) {\n      if (entity is File) {\n        totalFiles++;\n        totalSize += await entity.length();\n      }\n    }\n\n    return {\n      'totalFiles': totalFiles,\n      'totalSize': totalSize,\n    };\n  }\n}\n","size":4579,"language":"Dart","is_binary":false,"line_count":155,"is_source_code":true,"is_configuration":false,"is_documentation":false,"timestamp":"2025-10-15T07:46:08.386342"},{"path":"lib/src/core/incremental_analyzer.dart","content":"import 'package:universal_io/io.dart';\nimport 'package:github_analyzer/src/common/config.dart';\nimport 'package:github_analyzer/src/common/logger.dart';\nimport 'package:github_analyzer/src/models/analysis_result.dart';\nimport 'package:github_analyzer/src/models/source_file.dart';\nimport 'package:github_analyzer/src/models/analysis_statistics.dart';\nimport 'package:github_analyzer/src/common/utils/file_utils.dart';\nimport 'package:path/path.dart' as path;\nimport 'package:github_analyzer/src/common/language_info.dart';\n\n/// Represents the changes detected between two analysis runs\nclass FileChange {\n  final List<String> added;\n  final List<String> modified;\n  final List<String> deleted;\n\n  FileChange({\n    required this.added,\n    required this.modified,\n    required this.deleted,\n  });\n\n  /// Returns true if no changes were detected\n  bool get isEmpty => added.isEmpty && modified.isEmpty && deleted.isEmpty;\n\n  /// Total number of changed files\n  int get length => added.length + modified.length + deleted.length;\n}\n\n/// Performs incremental analysis by comparing current state with previous result\nclass IncrementalAnalyzer {\n  final GithubAnalyzerConfig config;\n\n  IncrementalAnalyzer({required this.config});\n\n  /// Analyzes a local directory based on previous analysis result\n  Future<AnalysisResult> analyze(\n    String directoryPath, {\n    required AnalysisResult previousResult,\n  }) async {\n    logger.info('Starting incremental analysis: $directoryPath');\n\n    final changes = await _detectChanges(directoryPath, previousResult);\n\n    if (changes.isEmpty) {\n      logger.info('No changes detected, returning previous result');\n      return previousResult;\n    }\n\n    logger.info('Changes detected: ${changes.length} files');\n    logger.fine(\n      'Added: ${changes.added.length}, Modified: ${changes.modified.length}, Deleted: ${changes.deleted.length}',\n    );\n\n    return await _analyzeChanges(directoryPath, previousResult, changes);\n  }\n\n  /// Analyzes only the changed files and merges with previous result\n  Future<AnalysisResult> _analyzeChanges(\n    String directoryPath,\n    AnalysisResult previousResult,\n    FileChange changes,\n  ) async {\n    final fileMap = <String, SourceFile>{\n      for (var f in previousResult.files) f.path: f,\n    };\n\n    // Process added and modified files\n    for (final changedPath in [...changes.added, ...changes.modified]) {\n      final file = File(path.join(directoryPath, changedPath));\n      if (!await file.exists()) continue;\n\n      try {\n        final analyzed = await _analyzeFile(file, changedPath);\n        if (analyzed != null) {\n          fileMap[changedPath] = analyzed;\n        }\n      } catch (e, stackTrace) {\n        logger.warning('Failed to analyze file $changedPath', e, stackTrace);\n      }\n    }\n\n    // Remove deleted files\n    for (final deletedPath in changes.deleted) {\n      fileMap.remove(deletedPath);\n    }\n\n    final allFiles = fileMap.values.toList();\n    final statistics = AnalysisStatistics.fromSourceFiles(allFiles);\n\n    final primaryLanguage = statistics.languageDistribution.isEmpty\n        ? null\n        : statistics.languageDistribution.entries\n            .reduce((a, b) => a.value > b.value ? a : b)\n            .key;\n\n    final updatedMetadata = previousResult.metadata.copyWith(\n      language: primaryLanguage,\n      languages: statistics.languageDistribution.keys.toList(),\n      fileCount: allFiles.length,\n    );\n\n    return AnalysisResult(\n      metadata: updatedMetadata,\n      files: allFiles,\n      statistics: statistics,\n      mainFiles: identifyMainFiles(allFiles),\n      dependencies: extractDependencies(allFiles),\n      errors: previousResult.errors,\n    );\n  }\n\n  /// Analyzes a single file and returns SourceFile model\n  Future<SourceFile?> _analyzeFile(File file, String relativePath) async {\n    final stat = await file.stat();\n\n    if (stat.size > config.maxFileSize) {\n      logger.finer('Skipping large file in incremental scan: $relativePath');\n      return null;\n    }\n\n    final isBinary = isBinaryFile(relativePath);\n    String? content;\n    int lineCount = 0;\n\n    if (!isBinary) {\n      try {\n        content = await file.readAsString();\n        lineCount = content.split('\\n').length;\n      } catch (e) {\n        logger.finer(\n          'Failed to read file as text in incremental scan: $relativePath, error: $e',\n        );\n        // Treat as binary if reading fails\n        return SourceFile(\n          path: relativePath,\n          content: null,\n          size: stat.size,\n          language: null,\n          isBinary: true,\n          lineCount: 0,\n          isSourceCode: false,\n          isConfiguration: isConfigurationFile(relativePath),\n          isDocumentation: isDocumentationFile(relativePath),\n          timestamp: stat.modified,\n        );\n      }\n    }\n\n    final language = detectLanguage(relativePath);\n\n    return SourceFile(\n      path: relativePath,\n      content: content,\n      size: stat.size,\n      language: language,\n      isBinary: isBinary,\n      lineCount: lineCount,\n      isSourceCode: language != null && !isBinary,\n      isConfiguration: isConfigurationFile(relativePath),\n      isDocumentation: isDocumentationFile(relativePath),\n      timestamp: stat.modified,\n    );\n  }\n\n  /// Detects changes between current directory state and previous result\n  Future<FileChange> _detectChanges(\n    String directoryPath,\n    AnalysisResult previousResult,\n  ) async {\n    final added = <String>[];\n    final modified = <String>[];\n\n    final dir = Directory(directoryPath);\n    if (!await dir.exists()) {\n      throw Exception('Directory not found: $directoryPath');\n    }\n\n    final previousFilesMap = <String, SourceFile>{\n      for (var f in previousResult.files) f.path: f,\n    };\n\n    final currentFilePaths = <String>[];\n\n    await for (final entity in dir.list(recursive: true)) {\n      if (entity is File) {\n        final relativePath = path.relative(entity.path, from: directoryPath);\n\n        if (shouldExclude(relativePath, config.excludePatterns)) {\n          continue;\n        }\n\n        currentFilePaths.add(relativePath);\n\n        final previousFile = previousFilesMap[relativePath];\n        final stat = await entity.stat();\n\n        if (previousFile == null) {\n          added.add(relativePath);\n        } else if (stat.modified.isAfter(previousFile.timestamp) ||\n            stat.size != previousFile.size) {\n          modified.add(relativePath);\n        }\n      }\n    }\n\n    final deleted = previousFilesMap.keys\n        .toSet()\n        .difference(currentFilePaths.toSet())\n        .toList();\n\n    return FileChange(\n      added: added,\n      modified: modified,\n      deleted: deleted,\n    );\n  }\n}\n","size":6684,"language":"Dart","is_binary":false,"line_count":219,"is_source_code":true,"is_configuration":false,"is_documentation":false,"timestamp":"2025-10-15T07:46:08.386738"},{"path":"lib/src/core/local_analyzer_service.dart","content":"import 'package:github_analyzer/src/common/config.dart';\nimport 'package:github_analyzer/src/common/logger.dart';\nimport 'package:github_analyzer/src/core/repository_analyzer.dart';\nimport 'package:github_analyzer/src/common/utils/directory_tree_generator.dart';\nimport 'package:github_analyzer/src/common/utils/file_utils.dart';\nimport 'package:github_analyzer/src/models/analysis_result.dart';\nimport 'package:github_analyzer/src/models/repository_metadata.dart';\nimport 'package:github_analyzer/src/models/analysis_statistics.dart';\nimport 'package:github_analyzer/src/core/incremental_analyzer.dart';\nimport 'package:path/path.dart' as path;\n\n/// Service responsible for analyzing local file directories\nclass LocalAnalyzerService {\n  final GithubAnalyzerConfig config;\n  final RepositoryAnalyzer repositoryAnalyzer;\n  final IncrementalAnalyzer incrementalAnalyzer;\n\n  LocalAnalyzerService({\n    required this.config,\n    RepositoryAnalyzer? repositoryAnalyzer,\n  })  : repositoryAnalyzer =\n            repositoryAnalyzer ?? RepositoryAnalyzer(config: config),\n        incrementalAnalyzer = IncrementalAnalyzer(config: config);\n\n  /// Analyzes a local directory\n  /// If previousResult is provided, performs incremental analysis\n  Future<AnalysisResult> analyze(\n    String directoryPath, {\n    AnalysisResult? previousResult,\n  }) async {\n    logger.info('Starting local analysis: $directoryPath');\n\n    if (previousResult != null) {\n      logger.info(\n          'Previous analysis result found, performing incremental analysis.');\n      try {\n        final result = await incrementalAnalyzer.analyze(\n          directoryPath,\n          previousResult: previousResult,\n        );\n        logger.info(\n            'Incremental analysis completed: ${result.files.length} files analyzed');\n        return result;\n      } catch (e, stackTrace) {\n        logger.warning(\n          'Incremental analysis failed. Performing full analysis instead.',\n          e,\n          stackTrace,\n        );\n      }\n    }\n\n    logger.info('Performing full analysis.');\n    final files = await repositoryAnalyzer.analyzeDirectory(directoryPath);\n    final statistics = AnalysisStatistics.fromSourceFiles(files);\n    final filePaths = files.map((f) => f.path).toList();\n    final directoryTree = DirectoryTreeGenerator.generate(filePaths);\n\n    final primaryLanguage = statistics.languageDistribution.isEmpty\n        ? null\n        : statistics.languageDistribution.entries\n            .reduce((a, b) => a.value > b.value ? a : b)\n            .key;\n\n    final metadata = RepositoryMetadata(\n      name: path.basename(directoryPath),\n      fullName: path.basename(directoryPath),\n      description: 'Local repository analysis',\n      isPrivate: false,\n      defaultBranch: null,\n      language: primaryLanguage,\n      languages: statistics.languageDistribution.keys.toList(),\n      stars: 0,\n      forks: 0,\n      fileCount: files.length,\n      commitSha: null,\n      directoryTree: directoryTree,\n    );\n\n    final mainFiles = identifyMainFiles(files);\n    final dependencies = extractDependencies(files);\n    final errors = repositoryAnalyzer.getErrors();\n\n    logger\n        .info('Full local analysis completed: ${files.length} files analyzed');\n\n    return AnalysisResult(\n      metadata: metadata,\n      files: files,\n      statistics: statistics,\n      mainFiles: mainFiles,\n      dependencies: dependencies,\n      errors: errors,\n    );\n  }\n}\n","size":3433,"language":"Dart","is_binary":false,"line_count":97,"is_source_code":true,"is_configuration":false,"is_documentation":false,"timestamp":"2025-10-15T07:46:08.387161"},{"path":"lib/src/core/remote_analyzer_service.dart","content":"import 'dart:async';\nimport 'package:archive/archive.dart';\nimport 'package:github_analyzer/src/common/config.dart';\nimport 'package:github_analyzer/src/common/logger.dart';\nimport 'package:github_analyzer/src/common/utils/directory_tree_generator.dart';\nimport 'package:github_analyzer/src/common/utils/file_utils.dart';\nimport 'package:github_analyzer/src/common/utils/github_utils.dart';\nimport 'package:github_analyzer/src/common/errors/analyzer_exception.dart';\nimport 'package:github_analyzer/src/infrastructure/interfaces/i_github_api_provider.dart';\nimport 'package:github_analyzer/src/data/providers/zip_downloader.dart';\nimport 'package:github_analyzer/src/core/cache_service.dart';\nimport 'package:github_analyzer/src/core/repository_analyzer.dart';\nimport 'package:github_analyzer/src/models/analysis_result.dart';\nimport 'package:github_analyzer/src/models/analysis_statistics.dart';\nimport 'package:github_analyzer/src/models/analysis_progress.dart';\n\n/// Service responsible for analyzing remote GitHub repositories.\n///\n/// It handles fetching repository metadata, downloading the archive,\n/// analyzing the contents, and managing the cache.\nclass RemoteAnalyzerService {\n  final GithubAnalyzerConfig config;\n  final IGithubApiProvider apiProvider;\n  final ZipDownloader zipDownloader;\n  final CacheService? cacheService;\n  final StreamController<AnalysisProgress>? progressController;\n\n  /// Creates an instance of [RemoteAnalyzerService].\n  RemoteAnalyzerService({\n    required this.config,\n    required this.apiProvider,\n    required this.zipDownloader,\n    this.cacheService,\n    this.progressController,\n  });\n\n  /// Creates a copy of this service with the given fields replaced.\n  /// This is useful for modifying the service's behavior, such as providing\n  /// a progress controller for a specific analysis run.\n  RemoteAnalyzerService copyWith({\n    GithubAnalyzerConfig? config,\n    IGithubApiProvider? apiProvider,\n    ZipDownloader? zipDownloader,\n    CacheService? cacheService,\n    StreamController<AnalysisProgress>? progressController,\n  }) {\n    return RemoteAnalyzerService(\n      config: config ?? this.config,\n      apiProvider: apiProvider ?? this.apiProvider,\n      zipDownloader: zipDownloader ?? this.zipDownloader,\n      cacheService: cacheService ?? this.cacheService,\n      progressController: progressController ?? this.progressController,\n    );\n  }\n\n  /// Analyzes a remote repository.\n  Future<AnalysisResult> analyze({\n    required String repositoryUrl,\n    String? branch,\n    bool useCache = true,\n  }) async {\n    logger.info('Starting remote analysis: $repositoryUrl');\n\n    _emitProgress(\n      AnalysisProgress(\n        phase: AnalysisPhase.initializing,\n        progress: 0.0,\n        message: 'Initializing analysis',\n        timestamp: DateTime.now(),\n      ),\n    );\n\n    final parsedUrl = parseGitHubUrl(repositoryUrl);\n    if (parsedUrl == null) {\n      throw AnalyzerException(\n        'Invalid GitHub URL: $repositoryUrl',\n        code: AnalyzerErrorCode.invalidUrl,\n        details: 'Expected format: https://github.com/owner/repo',\n      );\n    }\n\n    final owner = parsedUrl['owner']!;\n    final repo = parsedUrl['repo']!;\n\n    try {\n      _emitProgress(\n        AnalysisProgress(\n          phase: AnalysisPhase.initializing,\n          progress: 0.1,\n          message: 'Fetching repository metadata',\n          timestamp: DateTime.now(),\n        ),\n      );\n\n      final metadata = await apiProvider.getRepositoryMetadata(owner, repo);\n      final targetBranch = branch ?? metadata.defaultBranch ?? 'main';\n      final commitSha = metadata.commitSha;\n\n      if (commitSha == null) {\n        logger.warning(\n          'Could not determine commit SHA. Caching will be disabled.',\n        );\n        useCache = false;\n      }\n\n      if (useCache && cacheService != null && commitSha != null) {\n        _emitProgress(\n          AnalysisProgress(\n            phase: AnalysisPhase.initializing,\n            progress: 0.2,\n            message: 'Checking cache',\n            timestamp: DateTime.now(),\n          ),\n        );\n\n        final cached = await cacheService!.get(repositoryUrl, commitSha);\n        if (cached != null) {\n          logger.info('Using cached result');\n          _emitProgress(\n            AnalysisProgress(\n              phase: AnalysisPhase.completed,\n              progress: 1.0,\n              message: 'Loaded from cache',\n              timestamp: DateTime.now(),\n            ),\n          );\n          return cached;\n        }\n      }\n\n      logger.info('Downloading repository as ZIP (branch: $targetBranch)');\n      _emitProgress(\n        AnalysisProgress(\n          phase: AnalysisPhase.downloading,\n          progress: 0.3,\n          message: 'Downloading repository archive',\n          timestamp: DateTime.now(),\n        ),\n      );\n\n      final zipBytes = await zipDownloader.downloadRepositoryAsBytes(\n        owner: owner,\n        repo: repo,\n        branch: targetBranch,\n        token: config.githubToken,\n      );\n\n      _emitProgress(\n        AnalysisProgress(\n          phase: AnalysisPhase.extracting,\n          progress: 0.5,\n          message: 'Extracting archive',\n          timestamp: DateTime.now(),\n        ),\n      );\n\n      logger.info('Archive downloaded, analyzing from memory...');\n      final archive = ZipDecoder().decodeBytes(zipBytes);\n\n      _emitProgress(\n        AnalysisProgress(\n          phase: AnalysisPhase.analyzing,\n          progress: 0.6,\n          message: 'Analyzing files',\n          timestamp: DateTime.now(),\n        ),\n      );\n\n      final repositoryAnalyzer = RepositoryAnalyzer(\n        config: config,\n      );\n\n      final files = await repositoryAnalyzer.analyzeArchive(archive);\n      final filePaths = files.map((f) => f.path).toList();\n      final directoryTree = DirectoryTreeGenerator.generate(filePaths);\n\n      _emitProgress(\n        AnalysisProgress(\n          phase: AnalysisPhase.processing,\n          progress: 0.8,\n          message: 'Processing analysis results',\n          timestamp: DateTime.now(),\n        ),\n      );\n\n      final statistics = AnalysisStatistics.fromSourceFiles(files);\n      final mainFiles = identifyMainFiles(files);\n      final dependencies = extractDependencies(files);\n      final errors = repositoryAnalyzer.getErrors();\n\n      final result = AnalysisResult(\n        metadata: metadata.copyWith(\n          fileCount: files.length,\n          languages: statistics.languageDistribution.keys.toList(),\n          directoryTree: directoryTree,\n        ),\n        files: files,\n        statistics: statistics,\n        mainFiles: mainFiles,\n        dependencies: dependencies,\n        errors: errors,\n      );\n\n      if (useCache && cacheService != null && commitSha != null) {\n        _emitProgress(\n          AnalysisProgress(\n            phase: AnalysisPhase.caching,\n            progress: 0.9,\n            message: 'Caching results',\n            timestamp: DateTime.now(),\n          ),\n        );\n        await cacheService!.set(repositoryUrl, commitSha, result);\n      }\n\n      _emitProgress(\n        AnalysisProgress(\n          phase: AnalysisPhase.completed,\n          progress: 1.0,\n          message: 'Analysis completed',\n          timestamp: DateTime.now(),\n        ),\n      );\n\n      logger.info('Remote analysis completed: ${files.length} files analyzed');\n      return result;\n    } catch (e, stackTrace) {\n      logger.severe('Remote analysis failed.', e, stackTrace);\n\n      _emitProgress(\n        AnalysisProgress(\n          phase: AnalysisPhase.error,\n          progress: 0.0,\n          message: 'Analysis failed: ${e.toString()}',\n          timestamp: DateTime.now(),\n        ),\n      );\n\n      if (e is AnalyzerException) {\n        rethrow;\n      }\n\n      throw AnalyzerException(\n        'Remote analysis failed',\n        code: AnalyzerErrorCode.analysisError,\n        details: e.toString(),\n        originalException: e,\n        stackTrace: stackTrace,\n      );\n    }\n  }\n\n  void _emitProgress(AnalysisProgress progress) {\n    if (progressController != null && !progressController!.isClosed) {\n      progressController!.add(progress);\n    }\n  }\n}\n","size":8140,"language":"Dart","is_binary":false,"line_count":259,"is_source_code":true,"is_configuration":false,"is_documentation":false,"timestamp":"2025-10-15T07:46:08.387723"},{"path":"lib/src/core/repository_analyzer.dart","content":"import 'package:universal_io/io.dart';\nimport 'dart:convert';\nimport 'package:archive/archive.dart';\nimport 'package:github_analyzer/src/common/config.dart';\nimport 'package:github_analyzer/src/common/language_info.dart';\nimport 'package:github_analyzer/src/common/logger.dart';\nimport 'package:github_analyzer/src/common/utils/file_utils.dart';\nimport 'package:github_analyzer/src/common/errors/analyzer_exception.dart';\nimport 'package:github_analyzer/src/models/source_file.dart';\nimport 'package:github_analyzer/src/models/analysis_error.dart';\nimport 'package:github_analyzer/src/infrastructure/isolate_pool.dart';\nimport 'package:path/path.dart' as path;\n\n/// Analyzes repository files from a local directory or memory archive\nclass RepositoryAnalyzer {\n  final GithubAnalyzerConfig config;\n  final IsolatePool? isolatePool;\n  final List<AnalysisError> _errors = [];\n\n  RepositoryAnalyzer({\n    required this.config,\n    this.isolatePool,\n  });\n\n  /// Analyzes a directory recursively to extract source files\n  Future<List<SourceFile>> analyzeDirectory(String directoryPath) async {\n    logger.info('Analyzing directory: $directoryPath');\n\n    final dir = Directory(directoryPath);\n    if (!await dir.exists()) {\n      throw AnalyzerException(\n        'Directory not found: $directoryPath',\n        code: AnalyzerErrorCode.directoryNotFound,\n      );\n    }\n\n    final fileEntities = <File>[];\n    await for (final entity in dir.list(recursive: true)) {\n      if (entity is File) {\n        final relativePath = path.relative(entity.path, from: directoryPath);\n        if (shouldExclude(relativePath, config.effectiveExcludePatterns)) {\n          continue;\n        }\n        fileEntities.add(entity);\n      }\n    }\n\n    logger.info('Found ${fileEntities.length} files to analyze');\n\n    if (isolatePool != null && config.enableIsolatePool) {\n      return await _analyzeFilesInParallel(fileEntities, directoryPath);\n    } else {\n      return await _analyzeFilesSequentially(fileEntities, directoryPath);\n    }\n  }\n\n  /// Analyzes an archive in memory\n  Future<List<SourceFile>> analyzeArchive(Archive archive) async {\n    logger.info('Analyzing archive with ${archive.length} entries');\n\n    final files = <SourceFile>[];\n    String? rootPrefix;\n\n    for (final file in archive.files) {\n      if (file.isFile) {\n        rootPrefix ??= _detectRootPrefix(file.name);\n        var relativePath = file.name;\n\n        if (rootPrefix.isNotEmpty &&\n            relativePath.startsWith(rootPrefix) &&\n            relativePath.length > rootPrefix.length) {\n          relativePath = relativePath.substring(rootPrefix.length);\n        }\n\n        if (shouldExclude(relativePath, config.effectiveExcludePatterns)) {\n          continue;\n        }\n\n        try {\n          final sourceFile = await _analyzeArchiveFile(\n            file,\n            relativePath,\n            config.maxFileSize,\n          );\n          if (sourceFile != null) {\n            files.add(sourceFile);\n          }\n        } catch (e, stackTrace) {\n          logger.warning(\n              'Failed to analyze archive file: $relativePath', e, stackTrace);\n          _errors.add(\n            AnalysisError(\n              path: relativePath,\n              message: e.toString(),\n              stackTrace: stackTrace.toString(),\n              timestamp: DateTime.now(),\n            ),\n          );\n        }\n      }\n    }\n\n    logger.info('Archive analysis completed: ${files.length} files analyzed');\n    return files;\n  }\n\n  /// Analyzes files in parallel using isolate pool\n  Future<List<SourceFile>> _analyzeFilesInParallel(\n    List<File> fileEntities,\n    String basePath,\n  ) async {\n    logger.info('Analyzing files in parallel with isolate pool');\n\n    final args = fileEntities.map((entity) {\n      return {\n        'filePath': entity.path,\n        'basePath': basePath,\n        'maxFileSize': config.maxFileSize,\n      };\n    }).toList();\n\n    try {\n      final results = await isolatePool!.executeAll(\n        _analyzeFileInIsolate,\n        args,\n      );\n\n      final files = <SourceFile>[];\n      for (int i = 0; i < results.length; i++) {\n        final result = results[i];\n        if (result is Map<String, dynamic>) {\n          if (result.containsKey('error')) {\n            final relativePath = path.relative(\n              fileEntities[i].path,\n              from: basePath,\n            );\n            _errors.add(\n              AnalysisError(\n                path: relativePath,\n                message: result['error'] as String,\n                stackTrace: result['stackTrace'] as String?,\n                timestamp: DateTime.now(),\n              ),\n            );\n          } else {\n            files.add(SourceFile.fromJson(result));\n          }\n        }\n      }\n\n      return files;\n    } catch (e, stackTrace) {\n      logger.warning('Parallel analysis failed, falling back to sequential', e,\n          stackTrace);\n      return await _analyzeFilesSequentially(fileEntities, basePath);\n    }\n  }\n\n  /// Analyzes files sequentially\n  Future<List<SourceFile>> _analyzeFilesSequentially(\n    List<File> fileEntities,\n    String basePath,\n  ) async {\n    final files = <SourceFile>[];\n\n    for (final entity in fileEntities) {\n      final relativePath = path.relative(entity.path, from: basePath);\n      try {\n        final sourceFile = await _analyzeFile(\n          entity,\n          relativePath,\n          config.maxFileSize,\n        );\n        if (sourceFile != null) {\n          files.add(sourceFile);\n        }\n      } catch (e, stackTrace) {\n        logger.warning('Failed to analyze file: ${entity.path}', e, stackTrace);\n        _errors.add(\n          AnalysisError(\n            path: relativePath,\n            message: e.toString(),\n            stackTrace: stackTrace.toString(),\n            timestamp: DateTime.now(),\n          ),\n        );\n      }\n    }\n\n    return files;\n  }\n\n  /// Isolate worker function for parallel file analysis\n  static Future<dynamic> _analyzeFileInIsolate(\n      Map<String, dynamic> args) async {\n    final String filePath = args['filePath'];\n    final String basePath = args['basePath'];\n    final int maxFileSize = args['maxFileSize'];\n\n    final File file = File(filePath);\n    final relativePath = path.relative(filePath, from: basePath);\n\n    try {\n      final stat = await file.stat();\n\n      if (stat.size > maxFileSize) {\n        return null;\n      }\n\n      final isBinary = isBinaryFile(relativePath);\n      String? content;\n      int lineCount = 0;\n\n      if (!isBinary) {\n        try {\n          content = await file.readAsString();\n          lineCount = content.split('\\n').length;\n        } catch (e) {\n          return _createFileModelFromData(\n            relativePath,\n            stat.size,\n            null,\n            true,\n            0,\n            stat.modified,\n          ).toJson();\n        }\n      }\n\n      return _createFileModelFromData(\n        relativePath,\n        stat.size,\n        content,\n        isBinary,\n        lineCount,\n        stat.modified,\n      ).toJson();\n    } catch (e, stackTrace) {\n      return {\n        'path': relativePath,\n        'error': e.toString(),\n        'stackTrace': stackTrace.toString(),\n      };\n    }\n  }\n\n  /// Analyzes a single file from the file system\n  Future<SourceFile?> _analyzeFile(\n    File file,\n    String relativePath,\n    int maxFileSize,\n  ) async {\n    final stat = await file.stat();\n\n    if (stat.size > maxFileSize) {\n      logger.finer('Excluded large file: $relativePath');\n      return null;\n    }\n\n    final isBinary = isBinaryFile(relativePath);\n    String? content;\n    int lineCount = 0;\n\n    if (!isBinary) {\n      try {\n        content = await file.readAsString();\n        lineCount = content.split('\\n').length;\n      } catch (e) {\n        return _createFileModelFromData(\n          relativePath,\n          stat.size,\n          null,\n          true,\n          0,\n          stat.modified,\n        );\n      }\n    }\n\n    return _createFileModelFromData(\n      relativePath,\n      stat.size,\n      content,\n      isBinary,\n      lineCount,\n      stat.modified,\n    );\n  }\n\n  /// Analyzes a single file from an archive\n  Future<SourceFile?> _analyzeArchiveFile(\n    ArchiveFile file,\n    String relativePath,\n    int maxFileSize,\n  ) async {\n    if (file.size > maxFileSize) {\n      logger.finer('Excluded large file: $relativePath');\n      return null;\n    }\n\n    final isBinary = isBinaryFile(relativePath);\n    String? content;\n    int lineCount = 0;\n    final timestamp = DateTime.now();\n\n    if (!isBinary) {\n      try {\n        content = utf8.decode(\n          file.content as List<int>,\n          allowMalformed: true,\n        );\n        lineCount = content.split('\\n').length;\n      } catch (e) {\n        logger.finer('Failed to read archive file as text: $relativePath ($e)');\n        return _createFileModel(\n          relativePath,\n          file.size,\n          null,\n          true,\n          0,\n          timestamp,\n        );\n      }\n    }\n\n    return _createFileModel(\n      relativePath,\n      file.size,\n      content,\n      isBinary,\n      lineCount,\n      timestamp,\n    );\n  }\n\n  /// Creates a SourceFile model from analyzed data\n  static SourceFile _createFileModelFromData(\n    String relativePath,\n    int size,\n    String? content,\n    bool isBinary,\n    int lineCount,\n    DateTime timestamp,\n  ) {\n    final language = detectLanguage(relativePath);\n    final isSrc = language != null && !isBinary;\n    final isConfig = isConfigurationFile(relativePath);\n    final isDoc = isDocumentationFile(relativePath);\n\n    return SourceFile(\n      path: relativePath,\n      content: content,\n      size: size,\n      language: language,\n      isBinary: isBinary,\n      lineCount: lineCount,\n      isSourceCode: isSrc,\n      isConfiguration: isConfig,\n      isDocumentation: isDoc,\n      timestamp: timestamp,\n    );\n  }\n\n  /// Creates a SourceFile model\n  static SourceFile _createFileModel(\n    String relativePath,\n    int size,\n    String? content,\n    bool isBinary,\n    int lineCount,\n    DateTime timestamp,\n  ) {\n    return _createFileModelFromData(\n      relativePath,\n      size,\n      content,\n      isBinary,\n      lineCount,\n      timestamp,\n    );\n  }\n\n  /// Detects the root prefix in archive entries\n  String _detectRootPrefix(String path) {\n    final parts = path.split('/');\n    return parts.length > 1 ? '${parts[0]}/' : '';\n  }\n\n  /// Gets all errors that occurred during analysis\n  List<AnalysisError> getErrors() => List.unmodifiable(_errors);\n\n  /// Clears all errors\n  void clearErrors() => _errors.clear();\n}\n","size":10559,"language":"Dart","is_binary":false,"line_count":395,"is_source_code":true,"is_configuration":false,"is_documentation":false,"timestamp":"2025-10-15T07:46:08.388293"},{"path":"lib/src/data/providers/github_api_provider.dart","content":"import 'package:dio/dio.dart';\nimport 'package:github_analyzer/src/common/logger.dart';\nimport 'package:github_analyzer/src/common/errors/analyzer_exception.dart';\nimport 'package:github_analyzer/src/infrastructure/interfaces/i_http_client_manager.dart';\nimport 'package:github_analyzer/src/infrastructure/interfaces/i_github_api_provider.dart';\nimport 'package:github_analyzer/src/models/repository_metadata.dart';\n\n/// Provides access to the GitHub API for fetching repository metadata.\nclass GithubApiProvider implements IGithubApiProvider {\n  final String? token;\n  final IHttpClientManager httpClientManager;\n\n  /// Creates an instance of [GithubApiProvider].\n  GithubApiProvider({\n    this.token,\n    required this.httpClientManager,\n  });\n\n  @override\n  Future<RepositoryMetadata> getRepositoryMetadata(\n    String owner,\n    String repo,\n  ) async {\n    logger.info('Fetching repository metadata: $owner/$repo');\n\n    final uri = Uri.parse('https://api.github.com/repos/$owner/$repo');\n    final headers = {\n      'Accept': 'application/vnd.github.v3+json',\n      if (token != null) 'Authorization': 'token $token',\n    };\n\n    try {\n      final response = await httpClientManager.get(\n        uri,\n        headers: headers,\n        responseType: ResponseType.json,\n      );\n\n      final json = response.data as Map<String, dynamic>;\n\n      // The following API calls are best-effort. If they fail, we proceed\n      // with the data we have.\n\n      final languages = await _fetchLanguages(owner, repo, headers);\n      final commitSha = await _fetchCommitSha(\n        owner,\n        repo,\n        json['default_branch'] as String? ?? 'main',\n        headers,\n      );\n\n      return RepositoryMetadata(\n        name: json['name'] as String,\n        fullName: json['full_name'] as String?,\n        description: json['description'] as String?,\n        isPrivate: json['private'] as bool? ?? false,\n        defaultBranch: json['default_branch'] as String? ?? 'main',\n        language: json['language'] as String?,\n        languages: languages,\n        stars: json['stargazers_count'] as int? ?? 0,\n        forks: json['forks_count'] as int? ?? 0,\n        commitSha: commitSha,\n        fileCount: 0, // Populated later.\n        directoryTree: '', // Populated later.\n      );\n    } on DioException catch (e, stackTrace) {\n      logger.severe('Failed to fetch repository metadata.', e, stackTrace);\n      if (e.response?.statusCode == 404) {\n        throw AnalyzerException(\n          'Repository not found: $owner/$repo',\n          code: AnalyzerErrorCode.repositoryNotFound,\n          details: 'The repository does not exist or is not accessible.',\n          originalException: e,\n        );\n      } else if (e.response?.statusCode == 403) {\n        throw AnalyzerException(\n          'Access forbidden.',\n          code: AnalyzerErrorCode.accessDenied,\n          details: 'Check your token permissions or rate limits.',\n          originalException: e,\n        );\n      }\n      throw AnalyzerException(\n        'Failed to fetch repository metadata due to a network error.',\n        code: AnalyzerErrorCode.networkError,\n        details: e.message,\n        originalException: e,\n        stackTrace: stackTrace,\n      );\n    } catch (e, stackTrace) {\n      logger.severe('An unexpected error occurred.', e, stackTrace);\n      throw AnalyzerException(\n        'An unexpected error occurred while fetching repository metadata.',\n        code: AnalyzerErrorCode.analysisError,\n        originalException: e,\n        stackTrace: stackTrace,\n      );\n    }\n  }\n\n  Future<List<String>> _fetchLanguages(\n    String owner,\n    String repo,\n    Map<String, String> headers,\n  ) async {\n    try {\n      final languagesUri =\n          Uri.parse('https://api.github.com/repos/$owner/$repo/languages');\n      final languagesResponse = await httpClientManager.get(\n        languagesUri,\n        headers: headers,\n        responseType: ResponseType.json,\n      );\n      if (languagesResponse.statusCode == 200) {\n        final languagesJson = languagesResponse.data as Map<String, dynamic>;\n        return languagesJson.keys.toList();\n      }\n    } catch (e, stackTrace) {\n      logger.warning(\n        'Could not fetch repository languages. Proceeding without it.',\n        e,\n        stackTrace,\n      );\n    }\n    return [];\n  }\n\n  Future<String?> _fetchCommitSha(\n    String owner,\n    String repo,\n    String defaultBranch,\n    Map<String, String> headers,\n  ) async {\n    try {\n      final branchInfoUri = Uri.parse(\n        'https://api.github.com/repos/$owner/$repo/branches/$defaultBranch',\n      );\n      final branchResponse = await httpClientManager.get(\n        branchInfoUri,\n        headers: headers,\n        responseType: ResponseType.json,\n      );\n      if (branchResponse.statusCode == 200) {\n        final branchJson = branchResponse.data as Map<String, dynamic>;\n        return branchJson['commit']?['sha'] as String?;\n      }\n    } catch (e, stackTrace) {\n      logger.warning(\n        'Could not fetch branch information for $defaultBranch. Proceeding without commit SHA.',\n        e,\n        stackTrace,\n      );\n    }\n    return null;\n  }\n\n  @override\n  void dispose() {\n    // HttpClientManager is disposed by the GithubAnalyzer class\n  }\n}\n","size":5252,"language":"Dart","is_binary":false,"line_count":162,"is_source_code":true,"is_configuration":false,"is_documentation":false,"timestamp":"2025-10-15T07:46:08.388779"},{"path":"lib/src/data/providers/zip_downloader.dart","content":"import 'dart:typed_data';\nimport 'package:dio/dio.dart';\nimport 'package:github_analyzer/src/common/logger.dart';\nimport 'package:github_analyzer/src/common/errors/analyzer_exception.dart';\nimport 'package:github_analyzer/src/infrastructure/interfaces/i_http_client_manager.dart';\n\n/// Handles downloading of repository zip archives from GitHub\nclass ZipDownloader {\n  final IHttpClientManager httpClientManager;\n\n  ZipDownloader({required this.httpClientManager});\n\n  /// Downloads a repository as a byte array (Uint8List)\n  Future<Uint8List> downloadRepositoryAsBytes({\n    required String owner,\n    required String repo,\n    required String branch,\n    String? token,\n  }) async {\n    logger.info(\n      'Downloading repository as bytes: $owner/$repo (branch: $branch)',\n    );\n\n    final url =\n        'https://github.com/$owner/$repo/archive/refs/heads/$branch.zip';\n    final uri = Uri.parse(url);\n\n    final headers = <String, String>{\n      'Accept': 'application/zip',\n      if (token != null) 'Authorization': 'token $token',\n    };\n\n    try {\n      final response = await httpClientManager.get(\n        uri,\n        headers: headers,\n        responseType: ResponseType.bytes,\n      );\n\n      // Dio is configured to throw exceptions for non-2xx status codes\n      logger.info('Repository downloaded as bytes successfully');\n      return Uint8List.fromList(response.data as List<int>);\n    } on DioException catch (e, stackTrace) {\n      if (e.response?.statusCode == 404) {\n        throw AnalyzerException(\n          'Repository or branch not found: $owner/$repo@$branch',\n          code: AnalyzerErrorCode.repositoryNotFound,\n          details: 'The repository or specified branch does not exist.',\n          originalException: e,\n        );\n      }\n\n      logger.severe('Failed to download repository as bytes.', e, stackTrace);\n      throw AnalyzerException(\n        'Failed to download repository as bytes',\n        code: AnalyzerErrorCode.networkError,\n        details: e.toString(),\n        originalException: e,\n        stackTrace: stackTrace,\n      );\n    } catch (e, stackTrace) {\n      if (e is AnalyzerException) rethrow;\n\n      logger.severe('An unexpected error occurred.', e, stackTrace);\n      throw AnalyzerException(\n        'An unexpected error occurred while downloading repository as bytes.',\n        code: AnalyzerErrorCode.networkError,\n        details: e.toString(),\n        originalException: e,\n        stackTrace: stackTrace,\n      );\n    }\n  }\n\n  /// Downloads a repository to a temporary local zip file\n  /// Note: This method is not available on web platforms\n  Future<String> downloadRepository({\n    required String owner,\n    required String repo,\n    required String branch,\n    String? token,\n  }) async {\n    throw UnsupportedError(\n      'downloadRepository is not supported. Use downloadRepositoryAsBytes instead.',\n    );\n  }\n}\n","size":2878,"language":"Dart","is_binary":false,"line_count":88,"is_source_code":true,"is_configuration":false,"is_documentation":false,"timestamp":"2025-10-15T07:46:08.389248"},{"path":"lib/src/github_analyzer.dart","content":"import 'dart:async';\nimport 'package:github_analyzer/src/common/config.dart';\nimport 'package:github_analyzer/src/common/logger.dart';\nimport 'package:github_analyzer/src/core/cache_service.dart';\nimport 'package:github_analyzer/src/core/local_analyzer_service.dart';\nimport 'package:github_analyzer/src/core/remote_analyzer_service.dart';\nimport 'package:github_analyzer/src/core/repository_analyzer.dart';\nimport 'package:github_analyzer/src/data/providers/github_api_provider.dart';\nimport 'package:github_analyzer/src/data/providers/zip_downloader.dart';\nimport 'package:github_analyzer/src/infrastructure/http_client_manager.dart';\nimport 'package:github_analyzer/src/infrastructure/interfaces/i_github_api_provider.dart';\nimport 'package:github_analyzer/src/infrastructure/interfaces/i_http_client_manager.dart';\nimport 'package:github_analyzer/src/infrastructure/isolate_pool.dart';\nimport 'package:github_analyzer/src/models/analysis_result.dart';\nimport 'package:github_analyzer/src/models/analysis_progress.dart';\n\n/// Main class for analyzing GitHub repositories\n/// Coordinates local and remote repository analysis\nclass GithubAnalyzer {\n  final GithubAnalyzerConfig config;\n  final IHttpClientManager httpClientManager;\n  final IGithubApiProvider apiProvider;\n  final ZipDownloader zipDownloader;\n  final CacheService? cacheService;\n  final IsolatePool? isolatePool;\n  final LocalAnalyzerService localAnalyzer;\n  final RemoteAnalyzerService remoteAnalyzer;\n  final StreamController<AnalysisProgress> _progressController =\n      StreamController.broadcast();\n\n  /// Stream of analysis progress updates\n  Stream<AnalysisProgress> get progressStream => _progressController.stream;\n\n  /// Creates a GithubAnalyzer instance with all dependencies\n  GithubAnalyzer({\n    required this.config,\n    required this.httpClientManager,\n    required this.apiProvider,\n    required this.zipDownloader,\n    required this.localAnalyzer,\n    required this.remoteAnalyzer,\n    this.cacheService,\n    this.isolatePool,\n  }) {\n    // Initialize services that require it\n    cacheService?.initialize();\n    isolatePool?.initialize();\n  }\n\n  /// Creates a GithubAnalyzer with automatic .env loading and dependency injection\n  static Future<GithubAnalyzer> create({\n    GithubAnalyzerConfig? config,\n  }) async {\n    // Create config with auto .env loading if not provided\n    final effectiveConfig = config ?? await GithubAnalyzerConfig.create();\n\n    // Create HTTP client manager\n    final httpClientManager = HttpClientManager(\n      requestTimeout: Duration(seconds: 30),\n      maxConcurrentRequests: effectiveConfig.maxConcurrentRequests,\n      maxRetries: effectiveConfig.maxRetries,\n    );\n\n    // Create API provider\n    final apiProvider = GithubApiProvider(\n      httpClientManager: httpClientManager,\n      token: effectiveConfig.githubToken,\n    );\n\n    // Create ZIP downloader\n    final zipDownloader = ZipDownloader(\n      httpClientManager: httpClientManager,\n    );\n\n    // Create cache service if enabled\n    CacheService? cacheService;\n    if (effectiveConfig.enableCache) {\n      cacheService = CacheService(\n        cacheDirectory: effectiveConfig.cacheDirectory,\n        maxAge: effectiveConfig.cacheDuration,\n      );\n      await cacheService.initialize();\n    }\n\n    // Create isolate pool if enabled\n    IsolatePool? isolatePool;\n    if (effectiveConfig.enableIsolatePool) {\n      isolatePool = IsolatePool(size: effectiveConfig.isolatePoolSize);\n      await isolatePool.initialize();\n    }\n\n    // Create repository analyzer\n    final repositoryAnalyzer = RepositoryAnalyzer(\n      config: effectiveConfig,\n      isolatePool: isolatePool,\n    );\n\n    // Create local analyzer service\n    final localAnalyzer = LocalAnalyzerService(\n      config: effectiveConfig,\n      repositoryAnalyzer: repositoryAnalyzer,\n    );\n\n    // Create remote analyzer service\n    final remoteAnalyzer = RemoteAnalyzerService(\n      config: effectiveConfig,\n      apiProvider: apiProvider,\n      zipDownloader: zipDownloader,\n      cacheService: cacheService,\n    );\n\n    return GithubAnalyzer(\n      config: effectiveConfig,\n      httpClientManager: httpClientManager,\n      apiProvider: apiProvider,\n      zipDownloader: zipDownloader,\n      localAnalyzer: localAnalyzer,\n      remoteAnalyzer: remoteAnalyzer,\n      cacheService: cacheService,\n      isolatePool: isolatePool,\n    );\n  }\n\n  /// Analyzes a local directory\n  Future<AnalysisResult> analyzeLocal(String directoryPath) async {\n    logger.info('Starting local analysis: $directoryPath');\n\n    _progressController.add(\n      AnalysisProgress(\n        phase: AnalysisPhase.initializing,\n        progress: 0.0,\n        message: 'Starting local analysis',\n        timestamp: DateTime.now(),\n      ),\n    );\n\n    final result = await localAnalyzer.analyze(directoryPath);\n\n    _progressController.add(\n      AnalysisProgress(\n        phase: AnalysisPhase.completed,\n        progress: 1.0,\n        message: 'Local analysis completed',\n        timestamp: DateTime.now(),\n      ),\n    );\n\n    return result;\n  }\n\n  /// Analyzes a remote repository from URL\n  Future<AnalysisResult> analyzeRemote({\n    required String repositoryUrl,\n    String? branch,\n    bool useCache = true,\n  }) async {\n    logger.info('Starting remote analysis: $repositoryUrl');\n\n    // Pass progress controller to remote analyzer service\n    final remoteServiceWithProgress = remoteAnalyzer.copyWith(\n      progressController: _progressController,\n    );\n\n    final result = await remoteServiceWithProgress.analyze(\n      repositoryUrl: repositoryUrl,\n      branch: branch,\n      useCache: useCache,\n    );\n\n    return result;\n  }\n\n  /// Analyzes a target (auto-detects local path or remote URL)\n  Future<AnalysisResult> analyze(String target, {String? branch}) async {\n    // Basic check to differentiate between URL and local path\n    if (target.startsWith('http') || target.startsWith('git')) {\n      return await analyzeRemote(\n        repositoryUrl: target,\n        branch: branch,\n      );\n    } else {\n      return await analyzeLocal(target);\n    }\n  }\n\n  /// Clears the cache if enabled\n  Future<void> clearCache() async {\n    if (cacheService != null) {\n      await cacheService!.clear();\n      logger.info('Cache cleared');\n    }\n  }\n\n  /// Gets cache statistics\n  Future<Map<String, dynamic>?> getCacheStatistics() async {\n    return await cacheService?.getStatistics();\n  }\n\n  /// Disposes all resources\n  Future<void> dispose() async {\n    _progressController.close();\n    httpClientManager.dispose();\n    isolatePool?.dispose();\n    logger.info('GithubAnalyzer disposed');\n  }\n}\n","size":6617,"language":"Dart","is_binary":false,"line_count":207,"is_source_code":true,"is_configuration":false,"is_documentation":false,"timestamp":"2025-10-15T07:46:08.390004"},{"path":"lib/src/infrastructure/file_system/file_system.dart","content":"export 'file_system_interface.dart';\n\n// Conditional import for platform-specific factory\nimport 'file_system_interface.dart';\nimport 'file_system_stub.dart'\n    if (dart.library.io) 'file_system_io.dart'\n    if (dart.library.html) 'file_system_web.dart';\n\n/// Get platform-specific file system instance\nIFileSystem getFileSystem() => createPlatformFileSystem();\n","size":363,"language":"Dart","is_binary":false,"line_count":11,"is_source_code":true,"is_configuration":false,"is_documentation":false,"timestamp":"2025-10-15T07:46:08.390447"},{"path":"lib/src/infrastructure/file_system/file_system_interface.dart","content":"/// Abstract file system interface to enable platform-independent file operations\nabstract class IFileSystem {\n  /// Check if a directory exists\n  Future<bool> directoryExists(String path);\n\n  /// Create a directory (recursive)\n  Future<void> createDirectory(String path);\n\n  /// Write a string to a file (overwrites if exists)\n  Future<void> writeFile(String path, String contents);\n\n  /// Read a string from a file\n  Future<String> readFile(String path);\n\n  /// Check if the file exists\n  Future<bool> fileExists(String path);\n\n  /// List all files in a directory recursively\n  Future<List<String>> listFilesRecursively(String directory);\n\n  /// Delete a file or directory recursively\n  Future<void> delete(String path);\n}\n","size":725,"language":"Dart","is_binary":false,"line_count":24,"is_source_code":true,"is_configuration":false,"is_documentation":false,"timestamp":"2025-10-15T07:46:08.390943"},{"path":"lib/src/infrastructure/file_system/file_system_io.dart","content":"import 'package:universal_io/io.dart';\nimport 'file_system_interface.dart';\n\n/// Implementation of IFileSystem using universal_io for native platforms\nclass IOFileSystem implements IFileSystem {\n  @override\n  Future<bool> directoryExists(String path) async {\n    return Directory(path).exists();\n  }\n\n  @override\n  Future<void> createDirectory(String path) async {\n    final dir = Directory(path);\n    if (!await dir.exists()) {\n      await dir.create(recursive: true);\n    }\n  }\n\n  @override\n  Future<void> writeFile(String path, String contents) async {\n    final file = File(path);\n    await file.writeAsString(contents, flush: true);\n  }\n\n  @override\n  Future<String> readFile(String path) async {\n    final file = File(path);\n    return file.readAsString();\n  }\n\n  @override\n  Future<bool> fileExists(String path) async {\n    return File(path).exists();\n  }\n\n  @override\n  Future<List<String>> listFilesRecursively(String directory) async {\n    final dir = Directory(directory);\n    if (!await dir.exists()) return [];\n\n    final List<String> files = [];\n    await for (final entity in dir.list(recursive: true, followLinks: false)) {\n      if (entity is File) {\n        files.add(entity.path);\n      }\n    }\n    return files;\n  }\n\n  @override\n  Future<void> delete(String path) async {\n    final file = File(path);\n    final dir = Directory(path);\n\n    if (await file.exists()) {\n      await file.delete(recursive: true);\n    } else if (await dir.exists()) {\n      await dir.delete(recursive: true);\n    }\n  }\n}\n\n/// Factory function for IO platform\nIFileSystem createPlatformFileSystem() => IOFileSystem();\n","size":1614,"language":"Dart","is_binary":false,"line_count":65,"is_source_code":true,"is_configuration":false,"is_documentation":false,"timestamp":"2025-10-15T07:46:08.391263"},{"path":"lib/src/infrastructure/file_system/file_system_stub.dart","content":"import 'file_system_interface.dart';\n\n/// Stub implementation for unsupported platforms (fallback)\nclass StubFileSystem implements IFileSystem {\n  @override\n  Future<void> createDirectory(String path) {\n    throw UnsupportedError('File system is not supported on this platform');\n  }\n\n  @override\n  Future<void> delete(String path) {\n    throw UnsupportedError('File system is not supported on this platform');\n  }\n\n  @override\n  Future<String> readFile(String path) {\n    throw UnsupportedError('File system is not supported on this platform');\n  }\n\n  @override\n  Future<List<String>> listFilesRecursively(String directory) {\n    throw UnsupportedError('File system is not supported on this platform');\n  }\n\n  @override\n  Future<bool> directoryExists(String path) {\n    throw UnsupportedError('File system is not supported on this platform');\n  }\n\n  @override\n  Future<bool> fileExists(String path) {\n    throw UnsupportedError('File system is not supported on this platform');\n  }\n\n  @override\n  Future<void> writeFile(String path, String contents) {\n    throw UnsupportedError('File system is not supported on this platform');\n  }\n}\n\n/// Factory function for Stub platform\nIFileSystem createPlatformFileSystem() => StubFileSystem();\n","size":1236,"language":"Dart","is_binary":false,"line_count":43,"is_source_code":true,"is_configuration":false,"is_documentation":false,"timestamp":"2025-10-15T07:46:08.391643"},{"path":"lib/src/infrastructure/file_system/file_system_web.dart","content":"import 'dart:async';\nimport 'file_system_interface.dart';\n\n/// In-memory file system implementation for web platforms\nclass WebFileSystem implements IFileSystem {\n  final Map<String, String> _files = {};\n  final Map<String, Set<String>> _directories = {};\n\n  @override\n  Future<bool> directoryExists(String path) async {\n    return _directories.containsKey(path);\n  }\n\n  @override\n  Future<void> createDirectory(String path) async {\n    if (!_directories.containsKey(path)) {\n      _directories[path] = <String>{};\n    }\n  }\n\n  @override\n  Future<void> writeFile(String path, String contents) async {\n    _files[path] = contents;\n\n    final idx = path.lastIndexOf('/');\n    if (idx != -1) {\n      final dir = path.substring(0, idx);\n      _directories.putIfAbsent(dir, () => <String>{});\n      _directories[dir]!.add(path);\n    }\n  }\n\n  @override\n  Future<String> readFile(String path) async {\n    final content = _files[path];\n    if (content == null) {\n      throw Exception('File not found: $path');\n    }\n    return content;\n  }\n\n  @override\n  Future<bool> fileExists(String path) async {\n    return _files.containsKey(path);\n  }\n\n  @override\n  Future<List<String>> listFilesRecursively(String directory) async {\n    if (!_directories.containsKey(directory)) return [];\n\n    final result = <String>[];\n    void collectFiles(String dirPath) {\n      if (_directories.containsKey(dirPath)) {\n        for (final filePath in _directories[dirPath]!) {\n          result.add(filePath);\n        }\n      }\n    }\n\n    collectFiles(directory);\n    return result;\n  }\n\n  @override\n  Future<void> delete(String path) async {\n    _files.remove(path);\n\n    final idx = path.lastIndexOf('/');\n    if (idx != -1) {\n      final dir = path.substring(0, idx);\n      _directories[dir]?.remove(path);\n      if ((_directories[dir]?.isEmpty ?? false)) {\n        _directories.remove(dir);\n      }\n    }\n  }\n}\n\n/// Factory function for Web platform\nIFileSystem createPlatformFileSystem() => WebFileSystem();\n","size":1985,"language":"Dart","is_binary":false,"line_count":81,"is_source_code":true,"is_configuration":false,"is_documentation":false,"timestamp":"2025-10-15T07:46:08.392013"},{"path":"lib/src/infrastructure/http_client_manager.dart","content":"import 'dart:async';\nimport 'dart:io';\nimport 'package:dio/dio.dart';\nimport 'package:github_analyzer/src/common/logger.dart';\nimport 'package:github_analyzer/src/infrastructure/interfaces/i_http_client_manager.dart';\n\n/// An HTTP client manager using the dio package for robust networking.\n/// It handles retries, timeouts, and concurrent requests automatically.\nclass HttpClientManager implements IHttpClientManager {\n  final Dio _dio;\n\n  /// Creates an instance of [HttpClientManager].\n  HttpClientManager({\n    Duration requestTimeout = const Duration(seconds: 30),\n    int maxConcurrentRequests = 10,\n    int maxRetries = 3,\n  }) : _dio = Dio(\n          BaseOptions(\n            connectTimeout: requestTimeout,\n            receiveTimeout: requestTimeout,\n          ),\n        ) {\n    _dio.interceptors.add(\n      InterceptorsWrapper(\n        onRequest: (options, handler) {\n          logger.finer('Request: ${options.method} ${options.uri}');\n          return handler.next(options);\n        },\n        onResponse: (response, handler) {\n          logger.finer('Response: ${response.statusCode}');\n          return handler.next(response);\n        },\n        onError: (DioException e, handler) async {\n          logger.warning('Request Error: ${e.message}', e, e.stackTrace);\n          // Simple retry logic, dio has more advanced retry packages if needed.\n          if (e.type == DioExceptionType.connectionTimeout ||\n              e.type == DioExceptionType.receiveTimeout ||\n              e.type == DioExceptionType.sendTimeout) {\n            logger.info('Retrying request due to timeout...');\n            try {\n              final response = await _dio.request(\n                e.requestOptions.path,\n                options: Options(\n                  method: e.requestOptions.method,\n                  headers: e.requestOptions.headers,\n                ),\n              );\n              return handler.resolve(response);\n            } catch (err) {\n              return handler.next(e);\n            }\n          }\n          return handler.next(e);\n        },\n      ),\n    );\n    // Adjust the pool manager for concurrent requests\n    (_dio.httpClientAdapter as dynamic).createHttpClient = () {\n      final client = HttpClient();\n      client.maxConnectionsPerHost = maxConcurrentRequests;\n      return client;\n    };\n  }\n\n  @override\n  Future<Response> get(\n    Uri uri, {\n    Map<String, String>? headers,\n    ResponseType? responseType,\n  }) async {\n    try {\n      final response = await _dio.getUri(\n        uri,\n        options: Options(headers: headers, responseType: responseType),\n      );\n      return response;\n    } on DioException {\n      // Re-throw the original DioException to preserve status codes and other details.\n      rethrow;\n    }\n  }\n\n  @override\n  Future<Response> post(\n    Uri uri, {\n    Map<String, String>? headers,\n    Object? body,\n  }) async {\n    try {\n      final response = await _dio.postUri(\n        uri,\n        data: body,\n        options: Options(headers: headers),\n      );\n      return response;\n    } on DioException {\n      // Re-throw the original DioException.\n      rethrow;\n    }\n  }\n\n  @override\n  void dispose() {\n    _dio.close();\n    logger.info('HttpClientManager disposed');\n  }\n}\n","size":3242,"language":"Dart","is_binary":false,"line_count":108,"is_source_code":true,"is_configuration":false,"is_documentation":false,"timestamp":"2025-10-15T07:46:08.392346"},{"path":"lib/src/infrastructure/interfaces/i_github_api_provider.dart","content":"import 'package:github_analyzer/src/models/repository_metadata.dart';\n\nabstract class IGithubApiProvider {\n  Future<RepositoryMetadata> getRepositoryMetadata(String owner, String repo);\n  void dispose();\n}\n","size":206,"language":"Dart","is_binary":false,"line_count":7,"is_source_code":true,"is_configuration":false,"is_documentation":false,"timestamp":"2025-10-15T07:46:08.392914"},{"path":"lib/src/infrastructure/interfaces/i_http_client_manager.dart","content":"import 'package:dio/dio.dart';\n\n/// Abstract interface for an HTTP client manager.\n/// This allows for interchangeable HTTP client implementations.\nabstract class IHttpClientManager {\n  /// Performs a GET request.\n  Future<Response> get(\n    Uri uri, {\n    Map<String, String>? headers,\n    ResponseType? responseType,\n  });\n\n  /// Performs a POST request.\n  Future<Response> post(Uri uri, {Map<String, String>? headers, Object? body});\n\n  /// Disposes of the client's resources.\n  void dispose();\n}\n","size":500,"language":"Dart","is_binary":false,"line_count":19,"is_source_code":true,"is_configuration":false,"is_documentation":false,"timestamp":"2025-10-15T07:46:08.393505"},{"path":"lib/src/infrastructure/isolate_pool.dart","content":"import 'dart:isolate';\nimport 'package:github_analyzer/src/common/logger.dart';\n\n/// Manages a pool of isolates to perform tasks in parallel.\nclass IsolatePool {\n  final int size;\n  final List<_IsolateWorker> _workers = [];\n  int _currentWorkerIndex = 0;\n  bool _isInitialized = false;\n\n  /// Creates an instance of [IsolatePool].\n  IsolatePool({required this.size});\n\n  /// Initializes the isolate pool by spawning the configured number of workers.\n  Future<void> initialize() async {\n    if (_isInitialized) return;\n\n    logger.info('Initializing isolate pool with $size workers');\n\n    for (int i = 0; i < size; i++) {\n      final worker = _IsolateWorker(id: i);\n      await worker.spawn();\n      _workers.add(worker);\n    }\n\n    _isInitialized = true;\n    logger.info('Isolate pool initialized');\n  }\n\n  /// Executes a task on the next available isolate in the pool.\n  Future<R> execute<T, R>(Future<R> Function(T) task, T argument) async {\n    if (!_isInitialized) {\n      throw StateError('IsolatePool not initialized. Call initialize() first.');\n    }\n\n    final worker = _workers[_currentWorkerIndex];\n    _currentWorkerIndex = (_currentWorkerIndex + 1) % _workers.length;\n\n    return await worker.execute(task, argument);\n  }\n\n  /// Executes a list of tasks distributed across the isolate pool.\n  Future<List<R>> executeAll<T, R>(\n    Future<R> Function(T) task,\n    List<T> arguments,\n  ) async {\n    if (!_isInitialized) {\n      throw StateError('IsolatePool not initialized. Call initialize() first.');\n    }\n\n    final futures = <Future<R>>[];\n\n    for (int i = 0; i < arguments.length; i++) {\n      final worker = _workers[i % _workers.length];\n      futures.add(worker.execute(task, arguments[i]));\n    }\n\n    return await Future.wait(futures);\n  }\n\n  /// Disposes the isolate pool by terminating all worker isolates.\n  Future<void> dispose() async {\n    if (!_isInitialized) return;\n\n    logger.info('Disposing isolate pool');\n\n    for (final worker in _workers) {\n      await worker.kill();\n    }\n\n    _workers.clear();\n    _isInitialized = false;\n    logger.info('Isolate pool disposed');\n  }\n}\n\nclass _IsolateWorker {\n  final int id;\n  Isolate? _isolate;\n  SendPort? _sendPort;\n  final ReceivePort _receivePort = ReceivePort();\n\n  _IsolateWorker({required this.id});\n\n  Future<void> spawn() async {\n    logger.fine('Spawning isolate worker $id');\n\n    _isolate = await Isolate.spawn(_isolateEntryPoint, _receivePort.sendPort);\n\n    _sendPort = await _receivePort.first as SendPort;\n    logger.fine('Isolate worker $id spawned');\n  }\n\n  Future<R> execute<T, R>(Future<R> Function(T) task, T argument) async {\n    if (_sendPort == null) {\n      throw StateError('Isolate not spawned');\n    }\n\n    final responsePort = ReceivePort();\n    _sendPort!.send([task, argument, responsePort.sendPort]);\n\n    final result = await responsePort.first;\n    responsePort.close();\n\n    if (result is _IsolateError) {\n      throw Exception('Isolate error: ${result.message}\\n${result.stackTrace}');\n    }\n\n    return result as R;\n  }\n\n  Future<void> kill() async {\n    _isolate?.kill(priority: Isolate.immediate);\n    _receivePort.close();\n    logger.fine('Isolate worker $id killed');\n  }\n\n  static void _isolateEntryPoint(SendPort sendPort) {\n    final receivePort = ReceivePort();\n    sendPort.send(receivePort.sendPort);\n\n    receivePort.listen((message) async {\n      final task = message[0] as Future<dynamic> Function(dynamic);\n      final argument = message[1];\n      final responsePort = message[2] as SendPort;\n\n      try {\n        final result = await task(argument);\n        responsePort.send(result);\n      } catch (e, stackTrace) {\n        responsePort.send(\n          _IsolateError(\n            message: e.toString(),\n            stackTrace: stackTrace.toString(),\n          ),\n        );\n      }\n    });\n  }\n}\n\nclass _IsolateError {\n  final String message;\n  final String stackTrace;\n\n  _IsolateError({required this.message, required this.stackTrace});\n}\n","size":3973,"language":"Dart","is_binary":false,"line_count":148,"is_source_code":true,"is_configuration":false,"is_documentation":false,"timestamp":"2025-10-15T07:46:08.393769"},{"path":"lib/src/models/analysis_error.dart","content":"/// Represents an error that occurred during analysis.\nclass AnalysisError {\n  final String path;\n  final String message;\n  final DateTime timestamp;\n  final String? stackTrace;\n\n  /// Creates a const instance of [AnalysisError].\n  const AnalysisError({\n    required this.path,\n    required this.message,\n    required this.timestamp,\n    this.stackTrace,\n  });\n\n  /// Creates a copy of this error but with the given fields replaced with the new values.\n  AnalysisError copyWith({\n    String? path,\n    String? message,\n    DateTime? timestamp,\n    String? stackTrace,\n  }) {\n    return AnalysisError(\n      path: path ?? this.path,\n      message: message ?? this.message,\n      timestamp: timestamp ?? this.timestamp,\n      stackTrace: stackTrace ?? this.stackTrace,\n    );\n  }\n\n  /// Converts this object into a JSON-compatible map.\n  Map<String, dynamic> toJson() {\n    return {\n      'path': path,\n      'message': message,\n      'timestamp': timestamp.toIso8601String(),\n      'stack_trace': stackTrace,\n    };\n  }\n\n  /// Creates an instance of [AnalysisError] from a JSON map.\n  factory AnalysisError.fromJson(Map<String, dynamic> json) {\n    return AnalysisError(\n      path: json['path'] as String,\n      message: json['message'] as String,\n      timestamp: DateTime.parse(json['timestamp'] as String),\n      stackTrace: json['stack_trace'] as String?,\n    );\n  }\n\n  @override\n  String toString() {\n    return 'AnalysisError(path: $path, message: $message, timestamp: $timestamp)';\n  }\n}\n","size":1495,"language":"Dart","is_binary":false,"line_count":56,"is_source_code":true,"is_configuration":false,"is_documentation":false,"timestamp":"2025-10-15T07:46:08.394139"},{"path":"lib/src/models/analysis_progress.dart","content":"/// Represents the different phases of the analysis process.\nenum AnalysisPhase {\n  initializing,\n  downloading,\n  extracting,\n  analyzing,\n  processing,\n  generating,\n  caching,\n  completed,\n  error,\n}\n\n/// Represents the progress of a repository analysis.\nclass AnalysisProgress {\n  final AnalysisPhase phase;\n  final double progress;\n  final String? message;\n  final String? currentFile;\n  final int? processedFiles;\n  final int? totalFiles;\n  final DateTime timestamp;\n\n  /// Creates a const instance of [AnalysisProgress].\n  const AnalysisProgress({\n    required this.phase,\n    required this.progress,\n    this.message,\n    this.currentFile,\n    this.processedFiles,\n    this.totalFiles,\n    required this.timestamp,\n  });\n\n  /// The progress as a percentage (0.0 to 100.0).\n  double get percentage => (progress * 100).clamp(0.0, 100.0);\n\n  /// Creates a copy of this progress object but with the given fields replaced.\n  AnalysisProgress copyWith({\n    AnalysisPhase? phase,\n    double? progress,\n    String? message,\n    String? currentFile,\n    int? processedFiles,\n    int? totalFiles,\n    DateTime? timestamp,\n  }) {\n    return AnalysisProgress(\n      phase: phase ?? this.phase,\n      progress: progress ?? this.progress,\n      message: message ?? this.message,\n      currentFile: currentFile ?? this.currentFile,\n      processedFiles: processedFiles ?? this.processedFiles,\n      totalFiles: totalFiles ?? this.totalFiles,\n      timestamp: timestamp ?? this.timestamp,\n    );\n  }\n\n  /// Converts this object into a JSON-compatible map.\n  Map<String, dynamic> toJson() {\n    return {\n      'phase': phase.name,\n      'progress': progress,\n      'message': message,\n      'current_file': currentFile,\n      'processed_files': processedFiles,\n      'total_files': totalFiles,\n      'timestamp': timestamp.toIso8601String(),\n    };\n  }\n\n  /// Creates an instance of [AnalysisProgress] from a JSON map.\n  factory AnalysisProgress.fromJson(Map<String, dynamic> json) {\n    return AnalysisProgress(\n      phase: AnalysisPhase.values.firstWhere(\n        (e) => e.name == json['phase'],\n        orElse: () => AnalysisPhase.initializing,\n      ),\n      progress: (json['progress'] as num).toDouble(),\n      message: json['message'] as String?,\n      currentFile: json['current_file'] as String?,\n      processedFiles: json['processed_files'] as int?,\n      totalFiles: json['total_files'] as int?,\n      timestamp: DateTime.parse(json['timestamp'] as String),\n    );\n  }\n\n  @override\n  String toString() {\n    final buffer = StringBuffer('AnalysisProgress(phase: ${phase.name}');\n    buffer.write(', progress: ${percentage.toStringAsFixed(1)}%');\n    if (processedFiles != null && totalFiles != null) {\n      buffer.write(', files: $processedFiles/$totalFiles');\n    }\n    if (currentFile != null) {\n      buffer.write(', current: $currentFile');\n    }\n    buffer.write(')');\n    return buffer.toString();\n  }\n}\n","size":2915,"language":"Dart","is_binary":false,"line_count":102,"is_source_code":true,"is_configuration":false,"is_documentation":false,"timestamp":"2025-10-15T07:46:08.394560"},{"path":"lib/src/models/analysis_result.dart","content":"import 'package:github_analyzer/src/models/repository_metadata.dart';\nimport 'package:github_analyzer/src/models/source_file.dart';\nimport 'package:github_analyzer/src/models/analysis_statistics.dart';\nimport 'package:github_analyzer/src/models/analysis_error.dart';\n\n/// Represents the result of a repository analysis.\nclass AnalysisResult {\n  final RepositoryMetadata metadata;\n  final List<SourceFile> files;\n  final AnalysisStatistics statistics;\n  final List<String> mainFiles;\n  final Map<String, List<String>> dependencies;\n  final List<AnalysisError> errors;\n\n  /// Creates an instance of [AnalysisResult].\n  const AnalysisResult({\n    required this.metadata,\n    required this.files,\n    required this.statistics,\n    required this.mainFiles,\n    required this.dependencies,\n    this.errors = const [],\n  });\n\n  /// Creates a copy of this result but with the given fields replaced with the new values.\n  AnalysisResult copyWith({\n    RepositoryMetadata? metadata,\n    List<SourceFile>? files,\n    AnalysisStatistics? statistics,\n    List<String>? mainFiles,\n    Map<String, List<String>>? dependencies,\n    List<AnalysisError>? errors,\n  }) {\n    return AnalysisResult(\n      metadata: metadata ?? this.metadata,\n      files: files ?? this.files,\n      statistics: statistics ?? this.statistics,\n      mainFiles: mainFiles ?? this.mainFiles,\n      dependencies: dependencies ?? this.dependencies,\n      errors: errors ?? this.errors,\n    );\n  }\n\n  /// Converts this object into a JSON-compatible map.\n  Map<String, dynamic> toJson() {\n    return {\n      'metadata': metadata.toJson(),\n      'files': files.map((f) => f.toJson()).toList(),\n      'statistics': statistics.toJson(),\n      'main_files': mainFiles,\n      'dependencies': dependencies,\n      'errors': errors.map((e) => e.toJson()).toList(),\n    };\n  }\n\n  /// Creates an instance of [AnalysisResult] from a JSON map.\n  factory AnalysisResult.fromJson(Map<String, dynamic> json) {\n    return AnalysisResult(\n      metadata: RepositoryMetadata.fromJson(\n        (json['metadata'] as Map<String, dynamic>?) ?? {},\n      ),\n      files: ((json['files'] as List<dynamic>?) ?? [])\n          .whereType<Map<String, dynamic>>()\n          .map(SourceFile.fromJson)\n          .toList(),\n      statistics: AnalysisStatistics.fromJson(\n        (json['statistics'] as Map<String, dynamic>?) ?? {},\n      ),\n      mainFiles: ((json['main_files'] as List<dynamic>?) ?? [])\n          .whereType<String>()\n          .toList(),\n      dependencies: ((json['dependencies'] as Map<String, dynamic>?) ?? {}).map(\n        (k, v) => MapEntry(\n          k,\n          ((v as List<dynamic>?) ?? []).whereType<String>().toList(),\n        ),\n      ),\n      errors: ((json['errors'] as List<dynamic>?) ?? [])\n          .whereType<Map<String, dynamic>>()\n          .map(AnalysisError.fromJson)\n          .toList(),\n    );\n  }\n\n  @override\n  String toString() {\n    return 'AnalysisResult(repo: ${metadata.name}, files: ${files.length}, lines: ${statistics.totalLines}, errors: ${errors.length})';\n  }\n}\n","size":3042,"language":"Dart","is_binary":false,"line_count":90,"is_source_code":true,"is_configuration":false,"is_documentation":false,"timestamp":"2025-10-15T07:46:08.394870"},{"path":"lib/src/models/analysis_statistics.dart","content":"import 'package:github_analyzer/src/models/source_file.dart';\n\n/// Represents statistical data from a repository analysis.\nclass AnalysisStatistics {\n  final int totalFiles;\n  final int totalLines;\n  final int totalSize;\n  final Map<String, int> languageDistribution;\n  final int binaryFiles;\n  final int sourceFiles;\n  final int configFiles;\n  final int documentationFiles;\n\n  /// Creates a const instance of [AnalysisStatistics].\n  const AnalysisStatistics({\n    required this.totalFiles,\n    required this.totalLines,\n    required this.totalSize,\n    required this.languageDistribution,\n    required this.binaryFiles,\n    required this.sourceFiles,\n    required this.configFiles,\n    required this.documentationFiles,\n  });\n\n  /// Creates a copy of this statistics object but with the given fields replaced.\n  AnalysisStatistics copyWith({\n    int? totalFiles,\n    int? totalLines,\n    int? totalSize,\n    Map<String, int>? languageDistribution,\n    int? binaryFiles,\n    int? sourceFiles,\n    int? configFiles,\n    int? documentationFiles,\n  }) {\n    return AnalysisStatistics(\n      totalFiles: totalFiles ?? this.totalFiles,\n      totalLines: totalLines ?? this.totalLines,\n      totalSize: totalSize ?? this.totalSize,\n      languageDistribution: languageDistribution ?? this.languageDistribution,\n      binaryFiles: binaryFiles ?? this.binaryFiles,\n      sourceFiles: sourceFiles ?? this.sourceFiles,\n      configFiles: configFiles ?? this.configFiles,\n      documentationFiles: documentationFiles ?? this.documentationFiles,\n    );\n  }\n\n  /// Converts this object into a JSON-compatible map.\n  Map<String, dynamic> toJson() {\n    return {\n      'total_files': totalFiles,\n      'total_lines': totalLines,\n      'total_size': totalSize,\n      'language_distribution': languageDistribution,\n      'binary_files': binaryFiles,\n      'source_files': sourceFiles,\n      'config_files': configFiles,\n      'documentation_files': documentationFiles,\n    };\n  }\n\n  /// Creates an instance of [AnalysisStatistics] from a JSON map.\n  factory AnalysisStatistics.fromJson(Map<String, dynamic> json) {\n    return AnalysisStatistics(\n      totalFiles: json['total_files'] as int? ?? 0,\n      totalLines: json['total_lines'] as int? ?? 0,\n      totalSize: json['total_size'] as int? ?? 0,\n      languageDistribution:\n          (json['language_distribution'] as Map<String, dynamic>? ?? {}).map(\n            (k, v) => MapEntry(k, v as int),\n          ),\n      binaryFiles: json['binary_files'] as int? ?? 0,\n      sourceFiles: json['source_files'] as int? ?? 0,\n      configFiles: json['config_files'] as int? ?? 0,\n      documentationFiles: json['documentation_files'] as int? ?? 0,\n    );\n  }\n\n  /// Creates an instance of [AnalysisStatistics] from a list of [SourceFile] objects.\n  factory AnalysisStatistics.fromSourceFiles(List<SourceFile> files) {\n    int totalLines = 0;\n    int totalSize = 0;\n    int binaryFiles = 0;\n    int sourceFiles = 0;\n    int configFiles = 0;\n    int documentationFiles = 0;\n    final languageDistribution = <String, int>{};\n\n    for (final file in files) {\n      totalLines += file.lineCount;\n      totalSize += file.size;\n      if (file.isBinary) binaryFiles++;\n      if (file.isSourceCode) sourceFiles++;\n      if (file.isConfiguration) configFiles++;\n      if (file.isDocumentation) documentationFiles++;\n      if (file.language != null && file.language!.isNotEmpty) {\n        languageDistribution[file.language!] =\n            (languageDistribution[file.language!] ?? 0) + 1;\n      }\n    }\n\n    return AnalysisStatistics(\n      totalFiles: files.length,\n      totalLines: totalLines,\n      totalSize: totalSize,\n      languageDistribution: languageDistribution,\n      binaryFiles: binaryFiles,\n      sourceFiles: sourceFiles,\n      configFiles: configFiles,\n      documentationFiles: documentationFiles,\n    );\n  }\n\n  @override\n  String toString() {\n    return 'AnalysisStatistics(files: $totalFiles, lines: $totalLines, size: $totalSize)';\n  }\n}\n","size":3978,"language":"Dart","is_binary":false,"line_count":120,"is_source_code":true,"is_configuration":false,"is_documentation":false,"timestamp":"2025-10-15T07:46:08.395390"},{"path":"lib/src/models/repository_metadata.dart","content":"/// Represents metadata about a GitHub repository.\nclass RepositoryMetadata {\n  final String name;\n  final String? fullName;\n  final String? description;\n  final bool isPrivate;\n  final String? defaultBranch;\n  final String? language;\n  final List<String> languages;\n  final int stars;\n  final int forks;\n  final int fileCount;\n  final String? commitSha;\n  final String directoryTree;\n\n  /// Creates a const instance of [RepositoryMetadata].\n  const RepositoryMetadata({\n    required this.name,\n    this.fullName,\n    this.description,\n    required this.isPrivate,\n    this.defaultBranch,\n    this.language,\n    required this.languages,\n    required this.stars,\n    required this.forks,\n    required this.fileCount,\n    this.commitSha,\n    required this.directoryTree,\n  });\n\n  /// Creates a copy of this metadata object but with the given fields replaced.\n  RepositoryMetadata copyWith({\n    String? name,\n    String? fullName,\n    String? description,\n    bool? isPrivate,\n    String? defaultBranch,\n    String? language,\n    List<String>? languages,\n    int? stars,\n    int? forks,\n    int? fileCount,\n    String? commitSha,\n    String? directoryTree,\n  }) {\n    return RepositoryMetadata(\n      name: name ?? this.name,\n      fullName: fullName ?? this.fullName,\n      description: description ?? this.description,\n      isPrivate: isPrivate ?? this.isPrivate,\n      defaultBranch: defaultBranch ?? this.defaultBranch,\n      language: language ?? this.language,\n      languages: languages ?? this.languages,\n      stars: stars ?? this.stars,\n      forks: forks ?? this.forks,\n      fileCount: fileCount ?? this.fileCount,\n      commitSha: commitSha ?? this.commitSha,\n      directoryTree: directoryTree ?? this.directoryTree,\n    );\n  }\n\n  /// Converts this object into a JSON-compatible map.\n  Map<String, dynamic> toJson() {\n    return {\n      'name': name,\n      'full_name': fullName,\n      'description': description,\n      'is_private': isPrivate,\n      'default_branch': defaultBranch,\n      'language': language,\n      'languages': languages,\n      'stars': stars,\n      'forks': forks,\n      'file_count': fileCount,\n      'commit_sha': commitSha,\n      'directory_tree': directoryTree,\n    };\n  }\n\n  /// Creates an instance of [RepositoryMetadata] from a JSON map.\n  factory RepositoryMetadata.fromJson(Map<String, dynamic> json) {\n    return RepositoryMetadata(\n      name: json['name'] as String? ?? '',\n      fullName: json['full_name'] as String?,\n      description: json['description'] as String?,\n      isPrivate: json['is_private'] as bool? ?? false,\n      defaultBranch: json['default_branch'] as String?,\n      language: json['language'] as String?,\n      languages:\n          (json['languages'] as List<dynamic>?)\n              ?.map((e) => e as String)\n              .toList() ??\n          [],\n      stars: json['stars'] as int? ?? 0,\n      forks: json['forks'] as int? ?? 0,\n      fileCount: json['file_count'] as int? ?? 0,\n      commitSha: json['commit_sha'] as String?,\n      directoryTree: json['directory_tree'] as String? ?? '',\n    );\n  }\n\n  @override\n  String toString() {\n    return 'RepositoryMetadata(name: $name, language: $language, stars: $stars, files: $fileCount)';\n  }\n}\n","size":3214,"language":"Dart","is_binary":false,"line_count":108,"is_source_code":true,"is_configuration":false,"is_documentation":false,"timestamp":"2025-10-15T07:46:08.395773"},{"path":"lib/src/models/source_file.dart","content":"/// Represents a single source file in the repository.\nclass SourceFile {\n  final String path;\n  final String? content;\n  final int size;\n  final String? language;\n  final bool isBinary;\n  final int lineCount;\n  final bool isSourceCode;\n  final bool isConfiguration;\n  final bool isDocumentation;\n  final DateTime timestamp; // Added for incremental analysis\n\n  /// Creates a const instance of [SourceFile].\n  const SourceFile({\n    required this.path,\n    this.content,\n    required this.size,\n    this.language,\n    required this.isBinary,\n    required this.lineCount,\n    required this.isSourceCode,\n    required this.isConfiguration,\n    required this.isDocumentation,\n    required this.timestamp,\n  });\n\n  /// Creates a copy of this source file but with the given fields replaced.\n  SourceFile copyWith({\n    String? path,\n    String? content,\n    int? size,\n    String? language,\n    bool? isBinary,\n    int? lineCount,\n    bool? isSourceCode,\n    bool? isConfiguration,\n    bool? isDocumentation,\n    DateTime? timestamp,\n  }) {\n    return SourceFile(\n      path: path ?? this.path,\n      content: content ?? this.content,\n      size: size ?? this.size,\n      language: language ?? this.language,\n      isBinary: isBinary ?? this.isBinary,\n      lineCount: lineCount ?? this.lineCount,\n      isSourceCode: isSourceCode ?? this.isSourceCode,\n      isConfiguration: isConfiguration ?? this.isConfiguration,\n      isDocumentation: isDocumentation ?? this.isDocumentation,\n      timestamp: timestamp ?? this.timestamp,\n    );\n  }\n\n  /// Converts this object into a JSON-compatible map.\n  Map<String, dynamic> toJson() {\n    return {\n      'path': path,\n      'content': content,\n      'size': size,\n      'language': language,\n      'is_binary': isBinary,\n      'line_count': lineCount,\n      'is_source_code': isSourceCode,\n      'is_configuration': isConfiguration,\n      'is_documentation': isDocumentation,\n      'timestamp': timestamp.toIso8601String(),\n    };\n  }\n\n  /// Creates an instance of [SourceFile] from a JSON map.\n  factory SourceFile.fromJson(Map<String, dynamic> json) {\n    return SourceFile(\n      path: json['path'] as String,\n      content: json['content'] as String?,\n      size: json['size'] as int? ?? 0,\n      language: json['language'] as String?,\n      isBinary: json['is_binary'] as bool? ?? false,\n      lineCount: json['line_count'] as int? ?? 0,\n      isSourceCode: json['is_source_code'] as bool? ?? false,\n      isConfiguration: json['is_configuration'] as bool? ?? false,\n      isDocumentation: json['is_documentation'] as bool? ?? false,\n      timestamp: json['timestamp'] != null\n          ? DateTime.parse(json['timestamp'] as String)\n          : DateTime.fromMillisecondsSinceEpoch(0),\n    );\n  }\n\n  @override\n  String toString() {\n    return 'SourceFile(path: $path, size: $size, language: $language, lines: $lineCount)';\n  }\n}\n","size":2871,"language":"Dart","is_binary":false,"line_count":94,"is_source_code":true,"is_configuration":false,"is_documentation":false,"timestamp":"2025-10-15T07:46:08.396110"},{"path":"pubspec.yaml","content":"name: github_analyzer\ndescription: \"Analyze GitHub repositories and generate AI context for LLMs with cross-platform support\"\nversion: 0.1.0\n\nhomepage: https://github.com/cruxhan/github_analyzer\nrepository: https://github.com/cruxhan/github_analyzer\nissue_tracker: https://github.com/cruxhan/github_analyzer/issues\n\nenvironment:\n  sdk: '>=3.0.0 <4.0.0'\n\ndependencies:\n  # Archive extraction for ZIP files\n  archive: ^4.0.0\n  \n  # HTTP client for GitHub API\n  dio: ^5.4.3+1\n  \n  # Logging support\n  logging: ^1.2.0\n  \n  # Path manipulation\n  path: ^1.9.0\n  \n  # Cross-platform IO (web + native)\n  universal_io: ^2.2.2\n\ndev_dependencies:\n  # Testing\n  test: ^1.25.2\n  \n  # Mocking for tests\n  mockito: ^5.4.4\n  \n  # Code generation for mocks\n  build_runner: ^2.4.9\n  \n  # Linting\n  lints: ^3.0.0\n\nplatforms:\n  android:\n  ios:\n  linux:\n  macos:\n  web:\n  windows:\n","size":860,"language":"YAML","is_binary":false,"line_count":48,"is_source_code":true,"is_configuration":true,"is_documentation":false,"timestamp":"2025-10-15T07:46:08.396292"}],"statistics":{"total_files":45,"total_lines":4932,"total_size":293156,"language_distribution":{"JSON":1,"Markdown":2,"YAML":2,"Dart":37},"binary_files":0,"source_files":42,"config_files":4,"documentation_files":3},"main_files":[],"dependencies":{"pubspec.yaml":["Dart/pub"]},"errors":[]}